{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-platform training with sklearn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnldcE3l4y__",
        "colab_type": "text"
      },
      "source": [
        "## AI-platform training with sklearn\n",
        "\n",
        "I have created a script for training with keras in [colab](https://colab.research.google.com/drive/1mWbS2cjBSQ5X7iAyoowJviO_dZcYYFYY#scrollTo=2R7eCG89y3qW), that is mainly used for predicting logic, the training step happens in the colab server, this script is trying to train the model in the cloud.\n",
        "\n",
        "As that is based on Tensorflow, here is trying to train model with sklearn. I will just use the sample data in sklearn, the real project should be similiar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AgCPrwY5f1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "8a1db1d0-7ed8-4de0-a421-b32ee5be8b04"
      },
      "source": [
        "# first import sklearn and pandas\n",
        "!pip install --upgrade scikit-learn pandas"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 2.7MB/s \n",
            "\u001b[?25hRequirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/db/09/cab2f398e28e9f183714afde872b2ce23629f5833e467b151f18e1e08908/threadpoolctl-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.1 threadpoolctl-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVExOr6L5qCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter('ignore')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHeNquBf5-Ad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "91d36592-3b5f-4a5c-ae99-f632afda6d13"
      },
      "source": [
        "# As this is a sample script, I just make the data into a file in current sript and upload it into bucket\n",
        "iris = load_iris()\n",
        "\n",
        "x, y = iris.data, iris.target\n",
        "\n",
        "columns_name = ['a', 'b', 'c', 'd', 'label']\n",
        "\n",
        "df = pd.DataFrame(np.concatenate([x, y[:, np.newaxis]], axis=1), columns=columns_name)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     a    b    c    d  label\n",
              "0  5.1  3.5  1.4  0.2    0.0\n",
              "1  4.9  3.0  1.4  0.2    0.0\n",
              "2  4.7  3.2  1.3  0.2    0.0\n",
              "3  4.6  3.1  1.5  0.2    0.0\n",
              "4  5.0  3.6  1.4  0.2    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX8EgWxE6m5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a32ab37-8ca2-467e-d601-72378a8d0c03"
      },
      "source": [
        "# then I just save the dataframe into server side with a csv file.\n",
        "df.to_csv(os.path.join('.', 'data.csv'), index=False)\n",
        "\n",
        "print(os.listdir(\".\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'data.csv', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9YbdDQf7Y-k",
        "colab_type": "text"
      },
      "source": [
        "## Upload file into bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1eklfqU7Nlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first install with the storage api\n",
        "! pip install google-cloud-storage --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hl66hb27eGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# auth the notebook\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxO3sG-1IQuw",
        "colab_type": "text"
      },
      "source": [
        "## Upload sample file into bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR0U3aLb8Eyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "os.environ['GCLOUD_PROJECT'] = 'cloudtutorial-278306'\n",
        "\n",
        "client = storage.Client()\n",
        "\n",
        "bucket_name = 'first_bucket_lugq'\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# in fact, we don't need to create folder first, we could just define\n",
        "blob = bucket.blob('sklearn_tutorial/data.csv')\n",
        "\n",
        "# upload file into bucket\n",
        "blob.upload_from_filename('data.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAs_Laab9xaa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a985377e-325e-4796-e192-4437365e94ef"
      },
      "source": [
        "# list the file in bucket, so we do upload the file into the bucket\n",
        "for file in client.list_blobs(bucket_name, prefix='sklearn_tutorial/'):\n",
        "  print(str(file))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Blob: first_bucket_lugq, sklearn_tutorial/, 1590565285687456>\n",
            "<Blob: first_bucket_lugq, sklearn_tutorial/data.csv, 1590572504653769>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSpIbs5AGZJC",
        "colab_type": "text"
      },
      "source": [
        "## Traning logic file\n",
        "\n",
        "This is the main training logic happens here, the step is:\n",
        "* Import modules\n",
        "* Set up project environment\n",
        "* Create storage client and download files from storage\n",
        "* Load data into memory\n",
        "* Process data\n",
        "* Split data into train and validation\n",
        "* Do model training with sklearn\n",
        "* Dump our model into server\n",
        "* Upload trained model file into bucket\n",
        "\n",
        "That's it. But when I do the sample code, one question raised in my mind is why do we need to storage our data in bucket and download it from remote server? Maybe what I could imagine is that for currently implement in cloud is training with container,  each container could have their own resource like disk, memory, etc. So for easy to do is try to download the remote files into container file system, so we could use the resouce in containers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQXD7RMuDZRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2aa8fa1-393c-4590-d792-1651b29b4fa2"
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# in fact, we do need to import whole modules in the training file, so that we could do it right.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "os.environ['GCLOUD_PROJECT'] = 'cloudtutorial-278306'\n",
        "\n",
        "client = storage.Client()\n",
        "\n",
        "bucket_name = 'first_bucket_lugq'\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# in fact, we could just download the file from bucket directly without the front step.\n",
        "blob_download = bucket.blob('sklearn_tutorial/data.csv')\n",
        "blob_download.download_to_filename('data_new.csv')\n",
        "\n",
        "# load data from server\n",
        "df = pd.read_csv('data_new.csv')\n",
        "\n",
        "data = df.drop(['label'], axis=1).values\n",
        "label = df['label'].values\n",
        "\n",
        "print(\"data shape: {}, label shape: {}\".format(str(data.shape), str(label.shape)))\n",
        "\n",
        "pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "\n",
        "# split train and validation data\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(data, label, test_size=.2, random_state=1234)\n",
        "\n",
        "pipeline.fit(xtrain, ytrain)\n",
        "\n",
        "score = pipeline.score(xtest, ytest)\n",
        "\n",
        "print(\"validation score: \", score)\n",
        "\n",
        "# store the trained model into server\n",
        "import joblib\n",
        "\n",
        "model_name = \"pipeline.pkl\"\n",
        "joblib.dump(pipeline, model_name)\n",
        "\n",
        "upload_model_name = \"{}/{}\".format('sklearn_tutorial', model_name)\n",
        "\n",
        "upload_blob = bucket.blob(upload_model_name)\n",
        "upload_blob.upload_from_filename(model_name)\n",
        "print('model has been uploaded into bucket: {}'.format(bucket_name))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12qax9WeH6iu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c064cbfa-6876-4bb5-8e9f-251507fd75b8"
      },
      "source": [
        "# let's check our bucket, so great we do find the trained model in bucket.\n",
        "! gsutil ls gs://$bucket_name/sklearn*\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://first_bucket_lugq/sklearn_tutorial/\n",
            "gs://first_bucket_lugq/sklearn_tutorial/data.csv\n",
            "gs://first_bucket_lugq/sklearn_tutorial/pipeline.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRN_zz91RQVR",
        "colab_type": "text"
      },
      "source": [
        "## Make application project\n",
        "\n",
        "One easist way to create training step is to write the code into a project and wrap it into a file uploaded into cloud storage. Then for the later step we could use the project to do training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWx6RY9hQFV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e9fb8f9c-be34-4294-f909-5667c6d0b7ba"
      },
      "source": [
        "# First make the application folder and file\n",
        "print(\"Current folder file list: \", os.listdir('.'))\n",
        "\n",
        "folder_name = 'iris_tutorial'\n",
        "try:\n",
        "  os.makedirs(folder_name)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# we do need a __init__.py file as a project\n",
        "if '__init__.py' not in os.listdir(folder_name):\n",
        "  os.system('touch {}/__init__.py'.format(folder_name))\n",
        "\n",
        "# we could copy the train.py into the project folder\n",
        "import shutil\n",
        "\n",
        "train_file_name = 'train.py'\n",
        "shutil.copy(train_file_name, os.path.join(folder_name, train_file_name))\n",
        "\n",
        "print(\"project folder file list:\", os.listdir(folder_name))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current folder file list:  ['.config', 'adc.json', 'pipeline.pkl', 'train.py', 'data.csv', 'data_new.csv', '__init__.py', 'iris_tutorial', 'sample_data']\n",
            "project folder file list: ['train.py', '__init__.py']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djwsNDmYTb-Z",
        "colab_type": "text"
      },
      "source": [
        "## submit training job\n",
        "\n",
        "This is the sample submit code logic with gcould, in fact, we do could find a more detail explain of each parameters [traininig parameters](https://cloud.google.com/ai-platform/training/docs/training-scikit-learn#gcloud), and it's recommended that we should package our code into a project [package project](https://cloud.google.com/ai-platform/training/docs/packaging-trainer#using_gcloud_to_package_and_upload_your_application_recommended)\n",
        "``` shell\n",
        "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
        "    --staging-bucket $PACKAGE_STAGING_PATH \\\n",
        "    --job-dir $JOB_DIR  \\\n",
        "    --package-path $TRAINER_PACKAGE_PATH \\\n",
        "    --module-name $MAIN_TRAINER_MODULE \\\n",
        "    --region $REGION \\\n",
        "    -- \\\n",
        "    --user_first_arg=first_arg_value \\\n",
        "    --user_second_arg=second_arg_value\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpjFuIvcgrQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d62b896-6296-4e1c-905a-ce26ff2ee18c"
      },
      "source": [
        "# set project\n",
        "! gcloud config set project cloudtutorial-278306\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P_eS-t7ROaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# config of the job\n",
        "JOB_NAME = 'iris_training_04'   # where our source code zip file.\n",
        "PACKAGE_STAGING_PATH = \"gs://{}/sklearn_tutorial/package\".format(bucket_name)\n",
        "JOB_DIR = \"gs://{}/sklearn_tutorial/output\".format(bucket_name)\n",
        "TRAINING_PACKAGE_PATH = folder_name\n",
        "REGION = 'us-central1'\n",
        "MAIN_TRAINER_MODULE = \"{}.train\".format(folder_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y4SflNZU1GL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "3d21d259-d968-4039-cd24-31bc9e0a2582"
      },
      "source": [
        "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
        "--staging-bucket gs://first_bucket_lugq \\\n",
        "--job-dir $JOB_DIR \\\n",
        "--package-path $TRAINING_PACKAGE_PATH \\\n",
        "--module-name $MAIN_TRAINER_MODULE \\\n",
        "--region $REGION \\\n",
        "--runtime-version 2.1 \\\n",
        "--python-version 3.7"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [iris_training_04] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe iris_training_04\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs iris_training_04\n",
            "jobId: iris_training_04\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS-1jVHsWrXR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a72dff5-2e47-48dd-9a1e-833fbc5a50ca"
      },
      "source": [
        "! gcloud ai-platform jobs stream-logs iris_training_04"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO\t2020-05-27 09:42:26 +0000\tservice\t\tValidating job requirements...\n",
            "INFO\t2020-05-27 09:42:26 +0000\tservice\t\tJob creation request has been successfully validated.\n",
            "INFO\t2020-05-27 09:42:27 +0000\tservice\t\tJob iris_training_04 is queued.\n",
            "INFO\t2020-05-27 09:42:27 +0000\tservice\t\tWaiting for job to be provisioned.\n",
            "INFO\t2020-05-27 09:42:29 +0000\tservice\t\tWaiting for training program to start.\n",
            "INFO\t2020-05-27 09:43:09 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"chief\": [\"127.0.0.1:2222\"]} --task={\"type\": \"chief\", \"index\": 0} --job={  \"package_uris\": [\"gs://first_bucket_lugq/iris_training_04/f99e915a32b8d9eadfdf532e1210923d3eadd38a1ac1ef2dc5e03a758aa7f375/iris_tutorial-0.0.0.tar.gz\"],  \"python_module\": \"iris_tutorial.train\",  \"region\": \"us-central1\",  \"runtime_version\": \"2.1\",  \"job_dir\": \"gs://first_bucket_lugq/sklearn_tutorial/output\",  \"run_on_raw_vm\": true,  \"python_version\": \"3.7\"}\n",
            "WARNING\t2020-05-27 09:43:39 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "WARNING\t2020-05-27 09:43:39 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
            "WARNING\t2020-05-27 09:43:39 +0000\tmaster-replica-0\t\tIf using Keras pass *_constraint arguments to layers.\n",
            "INFO\t2020-05-27 09:43:44 +0000\tmaster-replica-0\t\tRunning module iris_tutorial.train.\n",
            "INFO\t2020-05-27 09:43:44 +0000\tmaster-replica-0\t\tDownloading the package: gs://first_bucket_lugq/iris_training_04/f99e915a32b8d9eadfdf532e1210923d3eadd38a1ac1ef2dc5e03a758aa7f375/iris_tutorial-0.0.0.tar.gz\n",
            "INFO\t2020-05-27 09:43:44 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://first_bucket_lugq/iris_training_04/f99e915a32b8d9eadfdf532e1210923d3eadd38a1ac1ef2dc5e03a758aa7f375/iris_tutorial-0.0.0.tar.gz iris_tutorial-0.0.0.tar.gz\n",
            "INFO\t2020-05-27 09:43:46 +0000\tmaster-replica-0\t\tInstalling the package: gs://first_bucket_lugq/iris_training_04/f99e915a32b8d9eadfdf532e1210923d3eadd38a1ac1ef2dc5e03a758aa7f375/iris_tutorial-0.0.0.tar.gz\n",
            "INFO\t2020-05-27 09:43:46 +0000\tmaster-replica-0\t\tRunning command: pip3 install --user --upgrade --force-reinstall --no-deps iris_tutorial-0.0.0.tar.gz\n",
            "INFO\t2020-05-27 09:43:47 +0000\tmaster-replica-0\t\tProcessing ./iris_tutorial-0.0.0.tar.gz\n",
            "INFO\t2020-05-27 09:43:47 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2020-05-27 09:43:47 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2020-05-27 09:43:47 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: iris-tutorial\n",
            "INFO\t2020-05-27 09:43:47 +0000\tmaster-replica-0\t\t  Building wheel for iris-tutorial (setup.py): started\n",
            "INFO\t2020-05-27 09:43:47 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2020-05-27 09:43:47 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\t  Building wheel for iris-tutorial (setup.py): finished with status 'done'\n",
            "INFO\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\t  Created wheel for iris-tutorial: filename=iris_tutorial-0.0.0-py3-none-any.whl size=2167 sha256=db0ea552bfa346251c9cd8a279eea5f174e2c9eb0643f1246cd9730e7b46875c\n",
            "INFO\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/5e/87/5a/824fe03bef40e201580c62ceca858db6636e8afb3ba0fa7d11\n",
            "INFO\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\tSuccessfully built iris-tutorial\n",
            "INFO\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\tInstalling collected packages: iris-tutorial\n",
            "INFO\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\tSuccessfully installed iris-tutorial-0.0.0\n",
            "ERROR\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\tWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
            "ERROR\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\tYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\n",
            "INFO\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\tRunning command: pip3 install --user iris_tutorial-0.0.0.tar.gz\n",
            "INFO\t2020-05-27 09:43:48 +0000\tmaster-replica-0\t\tProcessing ./iris_tutorial-0.0.0.tar.gz\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: iris-tutorial\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\t  Building wheel for iris-tutorial (setup.py): started\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\t  Building wheel for iris-tutorial (setup.py): finished with status 'done'\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\t  Created wheel for iris-tutorial: filename=iris_tutorial-0.0.0-py3-none-any.whl size=2167 sha256=308902fec965c1153fccb8b13ad1f534ec57ffbd3f2997d2bdfe0448c9accb40\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/5e/87/5a/824fe03bef40e201580c62ceca858db6636e8afb3ba0fa7d11\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tSuccessfully built iris-tutorial\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tInstalling collected packages: iris-tutorial\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\t  Attempting uninstall: iris-tutorial\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\t    Found existing installation: iris-tutorial 0.0.0\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\t    Uninstalling iris-tutorial-0.0.0:\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\t      Successfully uninstalled iris-tutorial-0.0.0\n",
            "INFO\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tSuccessfully installed iris-tutorial-0.0.0\n",
            "ERROR\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
            "ERROR\t2020-05-27 09:43:49 +0000\tmaster-replica-0\t\tYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\n",
            "INFO\t2020-05-27 09:43:50 +0000\tmaster-replica-0\t\tRunning command: python3 -m iris_tutorial.train --job-dir gs://first_bucket_lugq/sklearn_tutorial/output\n",
            "INFO\t2020-05-27 09:43:52 +0000\tmaster-replica-0\t\tdata shape: (150, 4), label shape: (150,)\n",
            "INFO\t2020-05-27 09:43:52 +0000\tmaster-replica-0\t\tvalidation score:  1.0\n",
            "INFO\t2020-05-27 09:43:52 +0000\tmaster-replica-0\t\tmodel has been uploaded into bucket: first_bucket_lugq\n",
            "INFO\t2020-05-27 09:43:52 +0000\tmaster-replica-0\t\tModule completed; cleaning up.\n",
            "INFO\t2020-05-27 09:43:52 +0000\tmaster-replica-0\t\tClean up finished.\n",
            "INFO\t2020-05-27 09:43:52 +0000\tmaster-replica-0\t\tTask completed successfully.\n",
            "INFO\t2020-05-27 09:46:14 +0000\tservice\t\tJob completed successfully.\n",
            "\n",
            "\n",
            "Command killed by keyboard interrupt\n",
            "\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVlDzKaJaU7q",
        "colab_type": "text"
      },
      "source": [
        "## Well done\n",
        "\n",
        "We have already deployed our training logic into server with training in cloud :). So great!\n",
        "\n",
        "The next step is that we have already trained our model with fix parameters without any parameters tuning, also the benefit with cloud training is distributed training with Tensorflow,  XGBoost and even sklearn. Will make it happen in later step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ausI1oZzaihT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}