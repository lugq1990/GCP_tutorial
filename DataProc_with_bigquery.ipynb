{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataProc with bigquery.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEGf8dYO_PNO",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "According official website: Dataproc is a managed Apache Spark and Apache Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data [official](https://cloud.google.com/dataproc/docs).\n",
        "\n",
        "When we need to use Spark to do data processing, then we could use DataProc as a tool to process data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leM-0VVX_who",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first auth the lab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aB2J7QgFJU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09f18b75-e3b1-42c9-ca38-086ac5017935"
      },
      "source": [
        "# install dataproc first\n",
        "! pip install google-cloud-dataproc --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▏                              | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 143kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 194kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 225kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 256kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 2.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-LzfCFytycU",
        "colab_type": "text"
      },
      "source": [
        "### Create a cluster\n",
        "\n",
        "Please follow these steps to create a cluster from [here](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-console). I just create a cluster with one node, this doesn't matter of the code, **Spark** will handle whole logic for us.\n",
        "\n",
        "We do create a **gcloud** to create a cluster, but here I just create a cluster with console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzAzsq-Sob8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f29d06f-6943-4a33-e6bc-a0fe80ec1bf5"
      },
      "source": [
        "! gcloud config set project emerald-road-282501"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c0AwrKTGQOJ",
        "colab_type": "text"
      },
      "source": [
        "#### Noted\n",
        "\n",
        "As with the `sandbox` problem, I haven't make the job run successfully, as the reason is for DataProc, it will start a cluster in compute engine, this is really expensive for `sandbox`, so the logic should do be similiar, we could sand our jars into the server and start the job in the remote server, but do keep in mind, we have to delete the cluster in the dataproc, so that we won't cost so much. In fact, this is just a solution that we could start our Spark job in cloud but with cluster created manually by us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntp8jxO7u1Dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define with basic info\n",
        "cluster_name = \"dataproc-spark\"\n",
        "region = \"us-central1\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc-Z44WRhzTF",
        "colab_type": "text"
      },
      "source": [
        "## Spark Feature engineering with Dataproc\n",
        "\n",
        "Now we could start our training or processing logic with PySpark, here I just create a sample file to use Spark to do feature extraction. After we  finish the logic, then we could upload the training file into storage for later use case like load data into **Bigquery**, then we could query the result from **bigquery** and do the later step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao2_m3EzKVqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41037038-83ce-47c0-b2e6-dd3a9784fd94"
      },
      "source": [
        "# here I write the job logic\n",
        "%%writefile training_spark.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info('init spark')\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (0, \"Hi I heard about Spark\"),\n",
        "    (1, \"I wish Java could use case classes\"),\n",
        "    (2, \"Logistic,regression,models,are,neat\")\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "# split sentence into words.\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "\n",
        "# This a UDF function created by ourselves to get length of sentence.\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "\n",
        "# with udf function to get each sentence length.\n",
        "tokenized = tokenizer.transform(sentenceDataFrame)\n",
        "token_selected = tokenized.select(\"sentence\", \"words\")\\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\")))\n",
        "\n",
        "print(\"Get final result: \")\n",
        "token_selected.show(truncate=False)\n",
        "\n",
        "logger.info('whole Spark logic finished.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training_spark.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6v-cgO0vZU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ff33c6f-c092-437e-bbd0-f87b369575a5"
      },
      "source": [
        "# we could submit our job here with just a gcloud command,\n",
        "# let's show it\n",
        "# then we could get whole output from here or we could get the info with console\n",
        "# with Dataproc `jobs` tab.\n",
        "! gcloud dataproc jobs submit pyspark training_spark.py --cluster $cluster_name --region $region"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [9c51e38e1d5b43ec809536ce89e0ef82] submitted.\n",
            "Waiting for job output...\n",
            "20/07/08 05:20:25 INFO org.spark_project.jetty.util.log: Logging initialized @4403ms\n",
            "20/07/08 05:20:25 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/08 05:20:25 INFO org.spark_project.jetty.server.Server: Started @4602ms\n",
            "20/07/08 05:20:25 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@171d6c9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/08 05:20:25 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/08 05:20:28 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.3:8032\n",
            "20/07/08 05:20:28 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.3:10200\n",
            "20/07/08 05:20:32 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\n",
            "java.lang.InterruptedException\n",
            "\tat java.lang.Object.wait(Native Method)\n",
            "\tat java.lang.Thread.join(Thread.java:1252)\n",
            "\tat java.lang.Thread.join(Thread.java:1326)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:980)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:630)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:807)\n",
            "20/07/08 05:20:33 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1594185397110_0001\n",
            "Get final result: \n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|sentence                           |words                                     |tokens|\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "\n",
            "20/07/08 05:21:03 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@171d6c9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [9c51e38e1d5b43ec809536ce89e0ef82] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/9c51e38e1d5b43ec809536ce89e0ef82/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/9c51e38e1d5b43ec809536ce89e0ef82/driveroutput\n",
            "jobUuid: 14b733f0-a325-3e84-bbcb-715f22f0ad31\n",
            "placement:\n",
            "  clusterName: dataproc-spark\n",
            "  clusterUuid: a3bfc512-1f78-443a-87cb-09bc80d6fd78\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/9c51e38e1d5b43ec809536ce89e0ef82/staging/training_spark.py\n",
            "reference:\n",
            "  jobId: 9c51e38e1d5b43ec809536ce89e0ef82\n",
            "  projectId: emerald-road-282501\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-08T05:21:07.127Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-08T05:20:18.417Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-08T05:20:18.454Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-08T05:20:18.968Z'\n",
            "yarnApplications:\n",
            "- name: training_spark.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1594185397110_0001/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijXF2ayrzA9E",
        "colab_type": "text"
      },
      "source": [
        "#### **Noted**\n",
        "As if we just use our account to create it, won't be fine so we need to create a service account with our mail, so that we could do it easier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS0WyYZeyHBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "a9d50e5c-b30b-4acd-ace3-81905f72a1a3"
      },
      "source": [
        "# one thing to notice, currently I just submit the job in terminal\n",
        "# more common way should upload our training file into bucket\n",
        "# then trigger the job with file in bucket\n",
        "\n",
        "# let's test it\n",
        "\n",
        "# first we need to create a bucket\n",
        "! gsutil mb gs://dataproc_lugq\n",
        "\n",
        "# then upload our file into bucket\n",
        "! gsutil cp training_spark.py gs://dataproc_lugq\n",
        "\n",
        "# let's check it\n",
        "! gsutil ls gs://dataproc_lugq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating gs://dataproc_lugq/...\n",
            "ServiceException: 409 Bucket dataproc_lugq already exists.\n",
            "Copying file://training_spark.py [Content-Type=text/x-python]...\n",
            "/ [1 files][  1.0 KiB/  1.0 KiB]                                                \n",
            "Operation completed over 1 objects/1.0 KiB.                                      \n",
            "gs://dataproc_lugq/sample.txt\n",
            "gs://dataproc_lugq/training_spark.py\n",
            "gs://dataproc_lugq/words.txt/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0oU3DaKypP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f3bae7f-b6f8-44ec-df22-6414d3829b56"
      },
      "source": [
        "# then let's trigger our training job with storage file\n",
        "! gcloud dataproc jobs submit pyspark gs://dataproc_lugq/training_spark.py \\\n",
        "--cluster $cluster_name \\\n",
        "--region $region"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [a081b9d1e7c54346b82ef5307e124095] submitted.\n",
            "Waiting for job output...\n",
            "20/07/08 05:21:37 INFO org.spark_project.jetty.util.log: Logging initialized @4017ms\n",
            "20/07/08 05:21:38 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/08 05:21:38 INFO org.spark_project.jetty.server.Server: Started @4211ms\n",
            "20/07/08 05:21:38 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@2b5dc039{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/08 05:21:38 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/08 05:21:40 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.3:8032\n",
            "20/07/08 05:21:41 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.3:10200\n",
            "20/07/08 05:21:43 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\n",
            "java.lang.InterruptedException\n",
            "\tat java.lang.Object.wait(Native Method)\n",
            "\tat java.lang.Thread.join(Thread.java:1252)\n",
            "\tat java.lang.Thread.join(Thread.java:1326)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:980)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:630)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:807)\n",
            "20/07/08 05:21:44 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1594185397110_0002\n",
            "Get final result: \n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|sentence                           |words                                     |tokens|\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "\n",
            "20/07/08 05:22:08 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@2b5dc039{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [a081b9d1e7c54346b82ef5307e124095] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/a081b9d1e7c54346b82ef5307e124095/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/a081b9d1e7c54346b82ef5307e124095/driveroutput\n",
            "jobUuid: e297814a-66dd-3852-af5a-a143f0220682\n",
            "placement:\n",
            "  clusterName: dataproc-spark\n",
            "  clusterUuid: a3bfc512-1f78-443a-87cb-09bc80d6fd78\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc_lugq/training_spark.py\n",
            "reference:\n",
            "  jobId: a081b9d1e7c54346b82ef5307e124095\n",
            "  projectId: emerald-road-282501\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-08T05:22:12.193Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-08T05:21:32.575Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-08T05:21:32.612Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-08T05:21:32.813Z'\n",
            "yarnApplications:\n",
            "- name: training_spark.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1594185397110_0002/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7uxKrE70Iia",
        "colab_type": "text"
      },
      "source": [
        "Alright, we have submitted our job into cluster both in local and remote **GCS**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7NsPz1OHVAN",
        "colab_type": "text"
      },
      "source": [
        "### Dataproc with GCS\n",
        "\n",
        "Next step is how could we use **Dataproc** with **Cloud storage** to read and write. \n",
        "\n",
        "Let's start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeDQrUW2HhI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8872a85-ac6a-414f-c937-d9dcc4329301"
      },
      "source": [
        "# first let's make some sample data, for most times we will use structured data,\n",
        "# let's make sample file to count the word frequency\n",
        "import os\n",
        "\n",
        "sample_text = \"\"\"\n",
        "This part of the Python Guestbook code walkthrough shows how to deploy the application to App Engine.\n",
        "This page is part of a multi-page tutorial. To start from the beginning and see instructions for setting up, go to Creating a Guestbook.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sample.txt\", 'w') as f:\n",
        "  f.write(sample_text)\n",
        "\n",
        "# let's check\n",
        "print(\"Current folder: \", os.listdir('.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current folder:  ['.config', 'sample.txt', 'training_spark.py', 'adc.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_jwi0jmILWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "28025c65-44b5-47ac-d078-44aceb7da4d2"
      },
      "source": [
        "# upload this file into bukcet: dataproc_lugq\n",
        "! gsutil cp sample.txt gs://dataproc_lugq/\n",
        "\n",
        "! gsutil ls gs://dataproc_lugq\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://sample.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  240.0 B/  240.0 B]                                                \n",
            "Operation completed over 1 objects/240.0 B.                                      \n",
            "gs://dataproc_lugq/sample.txt\n",
            "gs://dataproc_lugq/training_spark.py\n",
            "gs://dataproc_lugq/words.txt/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SlxEd7lIab5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1243b0f6-5d0c-4d1b-e1fd-64a2712d16b7"
      },
      "source": [
        "# next step is our main function for word count\n",
        "\n",
        "%%writefile words_count.py\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "input_uri = \"gs://dataproc_lugq/sample.txt\"\n",
        "\n",
        "# init spark context\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# use spark context to read file, return is a RDD\n",
        "lines = sc.textFile(input_uri)\n",
        "\n",
        "# main step for word count: flatmap to split -> map to k: v -> reduceybykey -> sort\n",
        "word_counts = lines.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1])\n",
        "\n",
        "# let's save our rdd into GCS\n",
        "output_uri = \"gs://dataproc_lugq/words.txt\"\n",
        "# one thing to notice is even we define a file, with spark to write file won't just get one file! \n",
        "# as data is distributed\n",
        "word_counts.saveAsTextFile(output_uri)\n",
        "\n",
        "# get whole result into driver memory\n",
        "print(\"Words count:\", word_counts.collect())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing words_count.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deJTadivJUC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "outputId": "f7dddbb8-7dd5-41fd-a5a1-dc4ac394ae80"
      },
      "source": [
        "# let's deploy our code with cluster and region for where to execute our code\n",
        "! gcloud dataproc jobs submit pyspark words_count.py --cluster $cluster_name \\\n",
        "--region $region"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [729d1fa544d14c4e8ed901189abc51f9] submitted.\n",
            "Waiting for job output...\n",
            "20/07/08 05:29:32 INFO org.spark_project.jetty.util.log: Logging initialized @3716ms\n",
            "20/07/08 05:29:32 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/08 05:29:32 INFO org.spark_project.jetty.server.Server: Started @3983ms\n",
            "20/07/08 05:29:32 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@2b5dc039{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/08 05:29:32 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/08 05:29:35 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.3:8032\n",
            "20/07/08 05:29:35 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.3:10200\n",
            "20/07/08 05:29:38 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\n",
            "java.lang.InterruptedException\n",
            "\tat java.lang.Object.wait(Native Method)\n",
            "\tat java.lang.Thread.join(Thread.java:1252)\n",
            "\tat java.lang.Thread.join(Thread.java:1326)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:980)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:630)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:807)\n",
            "20/07/08 05:29:39 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1594185397110_0004\n",
            "20/07/08 05:29:53 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
            "('Words count:', [(u'and', 1), (u'up,', 1), (u'from', 1), (u'for', 1), (u'Python', 1), (u'is', 1), (u'see', 1), (u'go', 1), (u'beginning', 1), (u'App', 1), (u'Guestbook', 1), (u'To', 1), (u'Guestbook.', 1), (u'code', 1), (u'Creating', 1), (u'walkthrough', 1), (u'multi-page', 1), (u'deploy', 1), (u'page', 1), (u'tutorial.', 1), (u'application', 1), (u'setting', 1), (u'instructions', 1), (u'how', 1), (u'Engine.', 1), (u'start', 1), (u'shows', 1), (u'a', 2), (u'This', 2), (u'of', 2), (u'part', 2), (u'the', 3), (u'to', 3)])\n",
            "20/07/08 05:30:08 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@2b5dc039{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [729d1fa544d14c4e8ed901189abc51f9] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/729d1fa544d14c4e8ed901189abc51f9/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/729d1fa544d14c4e8ed901189abc51f9/driveroutput\n",
            "jobUuid: 934940ec-e220-34a0-86ad-dc5325d81846\n",
            "placement:\n",
            "  clusterName: dataproc-spark\n",
            "  clusterUuid: a3bfc512-1f78-443a-87cb-09bc80d6fd78\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/729d1fa544d14c4e8ed901189abc51f9/staging/words_count.py\n",
            "reference:\n",
            "  jobId: 729d1fa544d14c4e8ed901189abc51f9\n",
            "  projectId: emerald-road-282501\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-08T05:30:12.725Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-08T05:29:27.115Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-08T05:29:27.146Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-08T05:29:27.339Z'\n",
            "yarnApplications:\n",
            "- name: words_count.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1594185397110_0004/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD0mEc42Klgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2e1935f1-3ece-49ad-cadc-8baa73791bdd"
      },
      "source": [
        "# let's check with storage\n",
        "! gsutil ls gs://dataproc_lugq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://dataproc_lugq/sample.txt\n",
            "gs://dataproc_lugq/training_spark.py\n",
            "gs://dataproc_lugq/words.txt/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPhfydguKrD5",
        "colab_type": "text"
      },
      "source": [
        "Well done, we have read and write the data from **Cloud storage**, in fact we could just face with GCS as HDFS or S3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItD0RGvwxe3Y",
        "colab_type": "text"
      },
      "source": [
        "#### Dataproc with Bigquery\n",
        "\n",
        "In fact, if we need to process relational data with big data, with Big query should be the most common way that we could interact with database.\n",
        "\n",
        "Let's created a sample dataset, and upload it into bigquery, use **Dataproc** to read it and do some feature engineering, last step is do a model training on new data, for later usecase we could store our trained model into **GCS** for reference.\n",
        "\n",
        "Let's get start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPacfjii1A-S",
        "colab_type": "text"
      },
      "source": [
        "#### Load data into bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69xyS_WXNwzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae2f56c3-7f27-485c-c046-50c874e5e9b7"
      },
      "source": [
        "# first let's create a sample dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "x, y = load_iris(return_X_y = True)\n",
        "data = np.concatenate([x, y[:, np.newaxis]], axis=1)\n",
        "\n",
        "df = pd.DataFrame(data, columns=['a', 'b', 'c', 'd', 'label'])\n",
        "\n",
        "# save our dataframe into disk\n",
        "df.to_csv('data.csv', index=False)\n",
        "\n",
        "print(\"Now what files we have: \", os.listdir('.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now what files we have:  ['.config', 'sample.txt', 'training_spark.py', 'words_count.py', 'adc.json', 'data.csv', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK_hQZXW0xyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "e5c866ce-8c2f-4402-d070-12b702b77b4f"
      },
      "source": [
        "# let's upload our data.csv into bucket\n",
        "! gsutil cp data.csv gs://dataproc_lugq/\n",
        "\n",
        "# check we done it\n",
        "! gsutil ls gs://dataproc_lugq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://data.csv [Content-Type=text/csv]...\n",
            "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
            "Operation completed over 1 objects/2.9 KiB.                                      \n",
            "gs://dataproc_lugq/data.csv\n",
            "gs://dataproc_lugq/sample.txt\n",
            "gs://dataproc_lugq/training_spark.py\n",
            "gs://dataproc_lugq/words.txt/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkcJeWaU1Dcj",
        "colab_type": "text"
      },
      "source": [
        "#### Load data into bigquery\n",
        "\n",
        "Before we do the load action with python, first we do need to create a dataset_id in the console, please just go to the **bigquery** console and create a dataset with **iris_dataset**.\n",
        "\n",
        "After the dataset has been created, let's create our table with python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_1JqUu01lzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first install bigquery module\n",
        "! pip install google-cloud-bigquery --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUsFZLnl09kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df67240c-4431-468b-8bf1-cab66acc8310"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# we need to create the dataset in console first\n",
        "project_id = \"emerald-road-282501\"\n",
        "dataset_id = \"iris_dataset\"\n",
        "bucket_name = \"dataproc_lugq\"\n",
        "\n",
        "# init bigquery client\n",
        "client = bigquery.Client(project_id)\n",
        "\n",
        "# create dataset inference\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "\n",
        "# define schema\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job_config.schema = [bigquery.SchemaField(\"a\", \"float\"),\n",
        "                     bigquery.SchemaField(\"b\", \"float\"),\n",
        "                     bigquery.SchemaField(\"c\", \"float\"),\n",
        "                     bigquery.SchemaField(\"d\", \"float\"),\n",
        "                     bigquery.SchemaField(\"label\", \"float\")]\n",
        "\n",
        "# skip the header, as I skip first row just get 149 records. not correct\n",
        "job_config.skip_leading_rows = 1\n",
        "# set to load csv\n",
        "job_config.source_format = bigquery.SourceFormat.CSV\n",
        "\n",
        "# data uri\n",
        "data_uri = \"gs://{}/{}\".format(bucket_name, \"data.csv\")\n",
        "\n",
        "# create a load job\n",
        "load_job = client.load_table_from_uri(data_uri, dataset_ref.table('iris'), job_config =job_config)\n",
        "print(\"submitted job: {}\".format(load_job.job_id))\n",
        "\n",
        "# wait result to finish\n",
        "load_job.result()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "submitted job: ad004d23-ac25-4637-a1a5-442ab5ef7cda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.cloud.bigquery.job.LoadJob at 0x7f7bfec3c940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXjMq4Bq2LAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de22b618-8371-4109-d44a-4d41f79758dc"
      },
      "source": [
        "# let's check how many data has been inserted\n",
        "# that's right!\n",
        "response = client.get_table(dataset_ref.table('iris'))\n",
        "\n",
        "print(\"there are {} records in bigquery\".format(response.num_rows))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 150 records in bigquery\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASh2Wk6A3Zf8",
        "colab_type": "text"
      },
      "source": [
        "#### Model Training with Dataproc and Bigquery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3QMWiYu287O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c44b2fd2-1c17-4390-90bc-b7daf6309418"
      },
      "source": [
        "# after we have load data in bigquery\n",
        "# then let's use dataproc to read data from bigquery\n",
        "# so that we could use the power of Spark and Bigquery\n",
        "\n",
        "%%writefile spark_train_bigquery.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# combine features into vector and get lable\n",
        "def inputs_to_vector(row):\n",
        "  return (row['label'], Vectors.dense(float(row['a']), \n",
        "                                      float(row['b']), \n",
        "                                      float(row['c']), \n",
        "                                      float(row['d']) ))\n",
        "\n",
        "# create sparksession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# read bigquery return into a DataFrame\n",
        "logger.info(\"Read data from bigquery\")\n",
        "df = spark.read.format('bigquery').option('table', 'iris_dataset.iris').load()\n",
        "\n",
        "# logger.info(\"get dataframe:\", df.show(5))\n",
        "df.createOrReplaceTempView('iris')\n",
        "\n",
        "df_new = spark.sql(\"select * from iris\")\n",
        "\n",
        "# map dataframe with vector function\n",
        "data_df = df_new.rdd.map(inputs_to_vector).toDF([\"label\", \"features\"])\n",
        "\n",
        "# split into train and test\n",
        "(train_df, test_df) = data_df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# cache dataframe\n",
        "train_df.cache()\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.1,elasticNetParam=0.8)\n",
        "\n",
        "logger.info(\"start to train model\")\n",
        "model = lr.fit(train_df)\n",
        "\n",
        "# get model prediction on test data\n",
        "pred = model.transform(test_df)\n",
        "\n",
        "\n",
        "# let's try to save our trained model into GCS\n",
        "bucket_name = \"dataproc_lugq\"\n",
        "model_folder = \"lr_model\"\n",
        "model_storage_path = \"gs://{}/{}\".format(bucket_name, model_folder)\n",
        "\n",
        "# in case the model already exist\n",
        "# I found that couldn't just save the model file directly into bucket\n",
        "# so let's just save the file into local server, then upload file with command\n",
        "# reference here: https://stackoverflow.com/questions/48684048/save-python-data-object-to-file-in-google-storage-from-a-pyspark-job-running-in\n",
        "# import os\n",
        "\n",
        "# local_path = os.getcwd()\n",
        "# model_name = 'lr_model'\n",
        "# model_path = os.path.join(local_path, model_name)\n",
        "# # with `write` function won't write the model\n",
        "# # model.write().overwrite().save(model_path)\n",
        "# model.save(model_path)\n",
        "\n",
        "# try:\n",
        "#   print(\"what we have:\", os.listdir(local_path))\n",
        "#   os.system(\"gsutil cp -r {} {}\".format(model_path, model_storage_path))\n",
        "# except Exception as e:\n",
        "#   print(\"when upload model with error:\", e)\n",
        "\n",
        "# this is try to get the model file\n",
        "# try:\n",
        "#   file_list = os.listdir(local_path)\n",
        "#   if any(['model' in x for x in file_list]):\n",
        "#     file_name = [x for x in file_list if x.endswith('model')][0]\n",
        "#     print(\"Get file:{}\".format(file_name))\n",
        "#     os.system(\"gsutil cp -r {} {}\".format(file_name, model_storage_path))\n",
        "#   else:\n",
        "#     print(\"we don't find with model file: current we have :\", file_list)\n",
        "# except Exception as e:\n",
        "#   print(\"When upload file inot bucket with error: \", e)\n",
        "\n",
        "# let's use command to upload file\n",
        "# from subprocess import call\n",
        "# print(\"Save model into bucket\")\n",
        "# call(['gsutil', 'cp', '-r', \"./*.model\", model_storage_path])\n",
        "\n",
        "print(\"get prediction:\", pred.show(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting spark_train_bigquery.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXR28d524AAE",
        "colab_type": "text"
      },
      "source": [
        "#### Noted\n",
        "\n",
        "If we need to use **Bigquery** in **Dataproc**, we need to provide the connection between two of them, that's: `--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar`, keep this in mind if you face error with couldn't find `bigquery`, do need to provide it with `jars`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpHvZbz22yck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ec7939b-b6b8-4f3f-e20c-8f01d841de0d"
      },
      "source": [
        "# let's submit our job into Dataproc\n",
        "! gcloud dataproc jobs submit pyspark spark_train_bigquery.py \\\n",
        "--cluster $cluster_name \\\n",
        "--region $region \\\n",
        "--jars gs://spark-lib/bigquery/spark-bigquery-latest.jar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [70af41685a264c34b3cc71d7b0fe2742] submitted.\n",
            "Waiting for job output...\n",
            "20/07/08 06:29:57 INFO org.spark_project.jetty.util.log: Logging initialized @8474ms\n",
            "20/07/08 06:29:57 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/08 06:29:57 INFO org.spark_project.jetty.server.Server: Started @8703ms\n",
            "20/07/08 06:29:57 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@304f32a4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/08 06:29:58 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/08 06:29:59 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.3:8032\n",
            "20/07/08 06:29:59 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.3:10200\n",
            "20/07/08 06:30:03 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\n",
            "java.lang.InterruptedException\n",
            "\tat java.lang.Object.wait(Native Method)\n",
            "\tat java.lang.Thread.join(Thread.java:1252)\n",
            "\tat java.lang.Thread.join(Thread.java:1326)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:980)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:630)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:807)\n",
            "20/07/08 06:30:04 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1594185397110_0013\n",
            "20/07/08 06:30:25 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table emerald-road-282501.iris_dataset.iris, parameters sent from Spark: requiredColumns=[a,b,c,d,label], filters=[]\n",
            "20/07/08 06:30:25 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from emerald-road-282501.iris_dataset.iris columns=[a, b, c, d, label], filter=''\n",
            "20/07/08 06:30:28 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'emerald-road-282501.iris_dataset.iris': projects/emerald-road-282501/locations/us/sessions/CAISDG1Kc2tLNDdIaU9mNRoCamQaAml3GgJuYRoCb3MaAmluGgJqcRoCanIaAmlyGgJqYxoCb2o\n",
            "20/07/08 06:30:48 INFO com.github.fommil.jni.JniLoader: successfully loaded /tmp/jniloader941347844193569487netlib-native_system-linux-x86_64.so\n",
            "20/07/08 06:30:48 INFO breeze.optimize.OWLQN: Step Size: 0.04982\n",
            "20/07/08 06:30:48 INFO breeze.optimize.OWLQN: Val and Grad Norm: 1.07312 (rel: 0.0224) 0.692148\n",
            "20/07/08 06:30:48 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:48 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.983736 (rel: 0.0833) 0.747096\n",
            "20/07/08 06:30:48 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:48 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.838749 (rel: 0.147) 1.34877\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.791761 (rel: 0.0560) 0.513261\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.785395 (rel: 0.00804) 0.343727\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.779444 (rel: 0.00758) 0.172446\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.777398 (rel: 0.00262) 0.142158\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:49 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.776780 (rel: 0.000796) 0.167191\n",
            "20/07/08 06:30:50 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:50 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.775620 (rel: 0.00149) 0.128766\n",
            "20/07/08 06:30:50 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/08 06:30:50 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.774233 (rel: 0.00179) 0.103877\n",
            "20/07/08 06:30:50 INFO breeze.optimize.OWLQN: Converged because max iterations reached\n",
            "+-----+-----------------+--------------------+--------------------+----------+\n",
            "|label|         features|       rawPrediction|         probability|prediction|\n",
            "+-----+-----------------+--------------------+--------------------+----------+\n",
            "|  0.0|[4.4,3.0,1.3,0.2]|[1.70679660826074...|[0.77314652111644...|       0.0|\n",
            "|  0.0|[4.4,3.2,1.3,0.2]|[1.90496933986708...|[0.81124213582637...|       0.0|\n",
            "|  0.0|[4.6,3.2,1.4,0.2]|[1.83677431600719...|[0.79697998705775...|       0.0|\n",
            "|  0.0|[4.6,3.4,1.4,0.3]|[1.95504920077365...|[0.81840746891634...|       0.0|\n",
            "|  0.0|[4.8,3.0,1.4,0.3]|[1.52598711800077...|[0.72955646680502...|       0.0|\n",
            "+-----+-----------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "('get prediction:', None)\n",
            "20/07/08 06:30:53 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@304f32a4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [70af41685a264c34b3cc71d7b0fe2742] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/70af41685a264c34b3cc71d7b0fe2742/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/70af41685a264c34b3cc71d7b0fe2742/driveroutput\n",
            "jobUuid: 0e0bb760-b569-3748-88c7-c6878e79deba\n",
            "placement:\n",
            "  clusterName: dataproc-spark\n",
            "  clusterUuid: a3bfc512-1f78-443a-87cb-09bc80d6fd78\n",
            "pysparkJob:\n",
            "  jarFileUris:\n",
            "  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/a3bfc512-1f78-443a-87cb-09bc80d6fd78/jobs/70af41685a264c34b3cc71d7b0fe2742/staging/spark_train_bigquery.py\n",
            "reference:\n",
            "  jobId: 70af41685a264c34b3cc71d7b0fe2742\n",
            "  projectId: emerald-road-282501\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-08T06:30:54.978Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-08T06:29:46.341Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-08T06:29:46.379Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-08T06:29:46.581Z'\n",
            "yarnApplications:\n",
            "- name: spark_train_bigquery.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1594185397110_0013/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfDdxbt25gQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "cc815212-3fdc-43e5-906b-b74c0bbb4c1e"
      },
      "source": [
        "# last let's try to check the storage to find model exists or not\n",
        "# so we do save our trained model into bucket.\n",
        "! gsutil ls gs://dataproc_lugq/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://dataproc_lugq/data.csv\n",
            "gs://dataproc_lugq/sample.txt\n",
            "gs://dataproc_lugq/training_spark.py\n",
            "gs://dataproc_lugq/words.txt/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1sb_Cup6Oi2",
        "colab_type": "text"
      },
      "source": [
        "#### Dataproc write data into Bigquery\n",
        "\n",
        "We have read data from **Bigquery**, how about we have done some processing and want to write the result into bigquery, so that we could do query with **SQL** later step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlk-drk9O8r2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "1913f291-5a9a-4ed6-b6dc-04c36de33925"
      },
      "source": [
        "! gcloud auth login"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&code_challenge=A8VJOVCdCxX8HypLWgrSgcEw26Lw6bO4FEfWDMTxJ1Q&code_challenge_method=S256&access_type=offline&response_type=code&prompt=select_account\n",
            "\n",
            "\n",
            "Enter verification code: 4/1wEhRyc-f7I3e19znB5d3KDvBXu3KhEA45sfwmydRsESNNoi9Am6-Ts\n",
            "\n",
            "You are now logged in as [gqianglu1990@gmail.com].\n",
            "Your current project is [cloudtutorial-282707].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcnZzUX7PGWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e678b9d-a91c-495c-bd1f-2488bf5921a6"
      },
      "source": [
        "! gcloud config set project \tcloudtutorial-282707"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdREAPn0Noa1",
        "colab_type": "text"
      },
      "source": [
        "First we need to create cluster named `word-count` and bigquery dataset named: `words` with console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJKKOnzrNAic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "export cluster_name=\"words-count\"\n",
        "export region='us-central1'\n",
        "export dataset_name='words'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgAtDOFwOKFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce9d71e8-8ebd-41f6-c74d-2f423259f431"
      },
      "source": [
        "# first let's make sample file and upload it into bucket\n",
        "import os\n",
        "\n",
        "sample_texts = \"This configures the Hive servers to read from and write to the correct location. You provide the Cloud SQL Proxy initialization action that Dataproc automatically\"\n",
        "with open('sample.txt', 'w') as f:\n",
        "  f.write(sample_texts)\n",
        "\n",
        "print(os.listdir('.'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'sample.txt', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc7b55BHOh6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "8dfaf0ae-fbb5-43aa-d02a-9a3aaa9428d4"
      },
      "source": [
        "# create bucket and upload file into bucket\n",
        "%%bash\n",
        "# make bucket\n",
        "gsutil mb gs://words_count\n",
        "\n",
        "# copy files\n",
        "gsutil cp sample.txt gs://words_count\n",
        "\n",
        "# check files\n",
        "gsutil ls gs://words_count"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://words_count/sample.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating gs://words_count/...\n",
            "ServiceException: 409 Bucket words_count already exists.\n",
            "Copying file://sample.txt [Content-Type=text/plain]...\n",
            "/ [0 files][    0.0 B/  162.0 B]                                                \r/ [1 files][  162.0 B/  162.0 B]                                                \r\n",
            "Operation completed over 1 objects/162.0 B.                                      \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yEO3Qs5UZKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# before we submit our spark job, first maybe we should create bigquery table first\n",
        "! pip install google-cloud-bigquery --quiet"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWLNr9odUndJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Noted with API create table must provide with credentials\n",
        "\n",
        "# from google.cloud import bigquery\n",
        "\n",
        "# project_id = \"cloudtutorial-282707\"\n",
        "\n",
        "# client = bigquery.Client(project_id)\n",
        "\n",
        "# # Here I just create table with API, with SQL also should be fine.\n",
        "# # first define table schema\n",
        "# schema = [bigquery.SchemaField('words', 'string', mode='required'), \n",
        "#           bigquery.SchemaField('num', 'int', mode='required')]\n",
        "# table_name = \"words.sample_words\" \n",
        "\n",
        "# table = bigquery.table(table_name, schema=schema)\n",
        "\n",
        "# # make request to create table\n",
        "# table = client.create_table(table)\n",
        "\n",
        "# print(\"Created table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlCZXAJtYFlq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8fc56bc-c08f-4306-852f-218f0af764ca"
      },
      "source": [
        "# so here just with command\n",
        "%%bash\n",
        "bq mk --table cloudtutorial-282707:words.sample_words words:STRING,num:INTEGER"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Table 'cloudtutorial-282707:words.sample_words' successfully created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8IbZ8RtN6Mv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1bc7309-6ae6-40c2-d1be-5b8fa88b21be"
      },
      "source": [
        "# next step is to create our main logic here\n",
        "%%writefile words_count_to_bigquery.py\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "bucket_name = \"words_count\"\n",
        "file_name = \"sample.txt\"\n",
        "file_path = \"gs://{}/{}\".format(bucket_name, file_name)\n",
        "\n",
        "spark = SparkSession.builder.master('yarn').getOrCreate()\n",
        "\n",
        "# we have to set the temprate bucket for spark, otherwise with error: requirement failed: Temporary GCS path has not been set\n",
        "spark.conf.set('temporaryGcsBucket', bucket_name)\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# we could use sc to read files as a DataFrame\n",
        "words_df = sc.textFile(file_path).flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).toDF([\"words\", \"num\"])\n",
        "\n",
        "print(\"what we have:\")\n",
        "words_df.show()\n",
        "\n",
        "# as we have made DataFrame, let's try to save the result into Bigquery\n",
        "words_df.write.format(\"bigquery\").option(\"table\", \"words.sample_words\").mode('overwrite').save()\n",
        "\n",
        "print(\"Save data finished.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting words_count_to_bigquery.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa4xF1lBa_68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37cf758e-951e-4dbb-83ea-a78161d3b598"
      },
      "source": [
        "# now let's submit our job into cluster\n",
        "%%bash\n",
        "gcloud dataproc jobs submit pyspark  words_count_to_bigquery.py \\\n",
        "--cluster word-count --region us-central1 --jars gs://spark-lib/bigquery/spark-bigquery-latest.jar"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-870970564128-vrn3qoba/google-cloud-dataproc-metainfo/f64b324b-9298-4586-9426-ff8ec2fe748f/jobs/3ce310d450e34ccb8aa2d882c0e36535/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-870970564128-vrn3qoba/google-cloud-dataproc-metainfo/f64b324b-9298-4586-9426-ff8ec2fe748f/jobs/3ce310d450e34ccb8aa2d882c0e36535/driveroutput\n",
            "jobUuid: 1d046aca-3fa6-353b-b05e-25ab59ae6f72\n",
            "placement:\n",
            "  clusterName: word-count\n",
            "  clusterUuid: f64b324b-9298-4586-9426-ff8ec2fe748f\n",
            "pysparkJob:\n",
            "  jarFileUris:\n",
            "  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-870970564128-vrn3qoba/google-cloud-dataproc-metainfo/f64b324b-9298-4586-9426-ff8ec2fe748f/jobs/3ce310d450e34ccb8aa2d882c0e36535/staging/words_count_to_bigquery.py\n",
            "reference:\n",
            "  jobId: 3ce310d450e34ccb8aa2d882c0e36535\n",
            "  projectId: cloudtutorial-282707\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-09T02:44:34.428Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-09T02:43:30.780Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-09T02:43:30.811Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-09T02:43:31.118Z'\n",
            "yarnApplications:\n",
            "- name: words_count_to_bigquery.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://word-count-m:8088/proxy/application_1594258552714_0003/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Job [3ce310d450e34ccb8aa2d882c0e36535] submitted.\n",
            "Waiting for job output...\n",
            "20/07/09 02:43:37 INFO org.spark_project.jetty.util.log: Logging initialized @4759ms\n",
            "20/07/09 02:43:37 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/09 02:43:37 INFO org.spark_project.jetty.server.Server: Started @4964ms\n",
            "20/07/09 02:43:37 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@2a07dfeb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/09 02:43:37 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/09 02:43:39 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at word-count-m/10.128.0.5:8032\n",
            "20/07/09 02:43:39 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at word-count-m/10.128.0.5:10200\n",
            "20/07/09 02:43:43 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1594258552714_0003\n",
            "20/07/09 02:44:01 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
            "what we have:\n",
            "+--------------+---+\n",
            "|         words|num|\n",
            "+--------------+---+\n",
            "|           and|  1|\n",
            "|     location.|  1|\n",
            "|          from|  1|\n",
            "|      Dataproc|  1|\n",
            "|       provide|  1|\n",
            "|initialization|  1|\n",
            "|          Hive|  1|\n",
            "|         write|  1|\n",
            "|          This|  1|\n",
            "|          read|  1|\n",
            "|         Cloud|  1|\n",
            "|        action|  1|\n",
            "|           You|  1|\n",
            "|           the|  3|\n",
            "| automatically|  1|\n",
            "|          that|  1|\n",
            "|    configures|  1|\n",
            "|            to|  2|\n",
            "|         Proxy|  1|\n",
            "|           SQL|  1|\n",
            "+--------------+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "20/07/09 02:44:23 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=words, projectId=cloudtutorial-282707, tableId=sample_words}}. jobId: JobId{project=cloudtutorial-282707, job=c43d7426-e905-419e-bb07-e383fe07179a, location=US}\n",
            "20/07/09 02:44:32 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to cloudtutorial-282707.words.sample_words. jobId: JobId{project=cloudtutorial-282707, job=c43d7426-e905-419e-bb07-e383fe07179a, location=US}\n",
            "Save data finished.\n",
            "20/07/09 02:44:33 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@2a07dfeb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [3ce310d450e34ccb8aa2d882c0e36535] finished successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_a6oylwdrnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "007902ff-b695-4a5a-f56e-56281297fb7e"
      },
      "source": [
        "# Good news, we have run our job success.\n",
        "\n",
        "# let's check result with bigquery by command should be fine\n",
        "! bq query --nouse_legacy_sql \"select * from words.sample_words\""
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Waiting on bqjob_r7334220d269721cc_0000017331795fcd_1 ... (0s) Current status: DONE   \n",
            "+----------------+-----+\n",
            "|     words      | num |\n",
            "+----------------+-----+\n",
            "| and            |   1 |\n",
            "| location.      |   1 |\n",
            "| from           |   1 |\n",
            "| Dataproc       |   1 |\n",
            "| provide        |   1 |\n",
            "| initialization |   1 |\n",
            "| Hive           |   1 |\n",
            "| write          |   1 |\n",
            "| This           |   1 |\n",
            "| read           |   1 |\n",
            "| Cloud          |   1 |\n",
            "| action         |   1 |\n",
            "| You            |   1 |\n",
            "| automatically  |   1 |\n",
            "| that           |   1 |\n",
            "| configures     |   1 |\n",
            "| Proxy          |   1 |\n",
            "| SQL            |   1 |\n",
            "| servers        |   1 |\n",
            "| correct        |   1 |\n",
            "| to             |   2 |\n",
            "| the            |   3 |\n",
            "+----------------+-----+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFOdEHOaeYnH",
        "colab_type": "text"
      },
      "source": [
        "So good we do could both read and write data with **Bigquery** by **Dataproc**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLWVkL0W9DTd",
        "colab_type": "text"
      },
      "source": [
        "###  Last words\n",
        "\n",
        "Alright, we do could train our model with data from **Bigquery** with **Dataproc**, as **Spark** is a unified framework, we could do many things with it, like **ML** and **SQL** etc. What we could do with **Spark** then whole things could be done with **Dataproc**, as it's just a cloud framework support Spark.\n",
        "\n",
        "Let's just remove our storage and dataproc cluster!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvYxx0X_9Biy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "249860a6-b0d2-4fb2-b907-825d5be4d768"
      },
      "source": [
        "# remove dataproc\n",
        "! gcloud dataproc clusters delete dataproc-spark --region $region"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cluster 'dataproc-spark' and all attached disks will be deleted.\n",
            "\n",
            "Do you want to continue (Y/n)?  y\n",
            "\n",
            "Waiting on operation [projects/cloudtutorial-282208/regions/us-central1/operations/aca94a03-5eb6-3c6e-bb1b-6b76606c6183].\n",
            "Deleted [https://dataproc.googleapis.com/v1/projects/cloudtutorial-282208/regions/us-central1/clusters/dataproc-spark].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Rk_4a63upT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "fc4e68e8-734c-4c31-f485-75051cf46f38"
      },
      "source": [
        "# remove the bigquery dataset\n",
        "! bq rm -r -d iris_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Welcome to BigQuery! This script will walk you through the \n",
            "process of initializing your .bigqueryrc configuration file.\n",
            "\n",
            "First, we need to set up your credentials if they do not \n",
            "already exist.\n",
            "\n",
            "Credential creation complete. Now we will select a default project.\n",
            "\n",
            "List of projects:\n",
            "  #        projectId           friendlyName    \n",
            " --- ---------------------- ------------------ \n",
            "  1   cloudtutorial-282208   CloudTutorial     \n",
            "  2   my-project-34336       My Project 34336  \n",
            "Found multiple projects. Please enter a selection for \n",
            "which should be the default, or leave blank to not \n",
            "set a default.\n",
            "\n",
            "Enter a selection (1 - 2): 1\n",
            "\n",
            "BigQuery configuration complete! Type \"bq\" to get started.\n",
            "\n",
            "rm: remove dataset 'cloudtutorial-282208:iris_dataset'? (y/N) y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V94ihc_8-t0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "52983f5e-703e-4f35-e539-a671baea807e"
      },
      "source": [
        "# remove whole buckets\n",
        "from google.cloud import storage\n",
        "\n",
        "client = storage.Client(project_id)\n",
        "\n",
        "buckets_list = list(client.list_buckets())\n",
        "\n",
        "# delete whole buckets\n",
        "for bucket in buckets_list:\n",
        "  print(\"Now to delete: {}\".format(bucket.name))\n",
        "  bucket.delete(force=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now to delete: dataproc-staging-us-central1-397497159726-nw76gzlj\n",
            "Now to delete: dataproc-temp-us-central1-397497159726-jbjqy65f\n",
            "Now to delete: dataproc_lugq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r26lf1N0Nj6r",
        "colab_type": "text"
      },
      "source": [
        "### Last word\n",
        "\n",
        "This tutorial is based on using **Dataproc** to do feature engineering and model training based on **Bigquery**. When we need to do big data processing, maybe we would use **Dataproc** many times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKPqe6IV3vni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}