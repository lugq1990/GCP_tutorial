{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataProc with bigquery.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEGf8dYO_PNO",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "According official website: Dataproc is a managed Apache Spark and Apache Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data [official](https://cloud.google.com/dataproc/docs).\n",
        "\n",
        "When we need to use Spark to do data processing, then we could use DataProc as a tool to process data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leM-0VVX_who",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first auth the lab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aB2J7QgFJU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5ca5782-2eaa-4b8c-83d6-e66c6b86b555"
      },
      "source": [
        "# install dataproc first\n",
        "! pip install google-cloud-dataproc --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▏                              | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 2.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-LzfCFytycU",
        "colab_type": "text"
      },
      "source": [
        "### Create a cluster\n",
        "\n",
        "Please follow these steps to create a cluster from [here](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-console). I just create a cluster with one node, this doesn't matter of the code, **Spark** will handle whole logic for us.\n",
        "\n",
        "We do create a **gcloud** to create a cluster, but here I just create a cluster with console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzAzsq-Sob8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45b5fb7f-a35f-4d62-e01b-6acf58da7975"
      },
      "source": [
        "! gcloud config set project emerald-road-282501"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c0AwrKTGQOJ",
        "colab_type": "text"
      },
      "source": [
        "## Noted\n",
        "\n",
        "As with the `sandbox` problem, I haven't make the job run successfully, as the reason is for DataProc, it will start a cluster in compute engine, this is really expensive for `sandbox`, so the logic should do be similiar, we could sand our jars into the server and start the job in the remote server, but do keep in mind, we have to delete the cluster in the dataproc, so that we won't cost so much. In fact, this is just a solution that we could start our Spark job in cloud but with cluster created manually by us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntp8jxO7u1Dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define with basic info\n",
        "cluster_name = \"dataproc-spark\"\n",
        "region = \"us-central1\"\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc-Z44WRhzTF",
        "colab_type": "text"
      },
      "source": [
        "## Spark Feature engineering with Dataproc\n",
        "\n",
        "Now we could start our training or processing logic with PySpark, here I just create a sample file to use Spark to do feature extraction. After we  finish the logic, then we could upload the training file into storage for later use case like load data into **Bigquery**, then we could query the result from **bigquery** and do the later step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao2_m3EzKVqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e4315c5-2783-4be6-835a-e3bbfda1be24"
      },
      "source": [
        "# here I write the job logic\n",
        "%%writefile training_spark.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info('init spark')\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (0, \"Hi I heard about Spark\"),\n",
        "    (1, \"I wish Java could use case classes\"),\n",
        "    (2, \"Logistic,regression,models,are,neat\")\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "# split sentence into words.\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "\n",
        "# This a UDF function created by ourselves to get length of sentence.\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "\n",
        "# with udf function to get each sentence length.\n",
        "tokenized = tokenizer.transform(sentenceDataFrame)\n",
        "token_selected = tokenized.select(\"sentence\", \"words\")\\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\")))\n",
        "\n",
        "print(\"Get final result: \")\n",
        "token_selected.show(truncate=False)\n",
        "\n",
        "logger.info('whole Spark logic finished.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training_spark.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6v-cgO0vZU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1786b6e7-f30b-44ac-df57-89b4db4d0ac4"
      },
      "source": [
        "# we could submit our job here with just a gcloud command,\n",
        "# let's show it\n",
        "# then we could get whole output from here or we could get the info with console\n",
        "# with Dataproc `jobs` tab.\n",
        "! gcloud dataproc jobs submit pyspark training_spark.py --cluster $cluster_name --region $region"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [1942c96d1eb64030b8aa290fea31cd80] submitted.\n",
            "Waiting for job output...\n",
            "20/07/03 08:22:40 INFO org.spark_project.jetty.util.log: Logging initialized @3748ms\n",
            "20/07/03 08:22:41 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/03 08:22:41 INFO org.spark_project.jetty.server.Server: Started @3930ms\n",
            "20/07/03 08:22:41 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@3f432a74{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/03 08:22:41 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/03 08:22:43 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.2:8032\n",
            "20/07/03 08:22:43 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.2:10200\n",
            "20/07/03 08:22:46 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\n",
            "java.lang.InterruptedException\n",
            "\tat java.lang.Object.wait(Native Method)\n",
            "\tat java.lang.Thread.join(Thread.java:1252)\n",
            "\tat java.lang.Thread.join(Thread.java:1326)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:980)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:630)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:807)\n",
            "20/07/03 08:22:47 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1593763852919_0003\n",
            "Get final result: \n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|sentence                           |words                                     |tokens|\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "\n",
            "20/07/03 08:23:09 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@3f432a74{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [1942c96d1eb64030b8aa290fea31cd80] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/1942c96d1eb64030b8aa290fea31cd80/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/1942c96d1eb64030b8aa290fea31cd80/driveroutput\n",
            "jobUuid: aad9ffce-d1b5-340d-b68f-0bb3bb16041d\n",
            "placement:\n",
            "  clusterName: dataproc-spark\n",
            "  clusterUuid: 6ac75383-d0b4-4976-b781-e2c39188d3ad\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/1942c96d1eb64030b8aa290fea31cd80/staging/training_spark.py\n",
            "reference:\n",
            "  jobId: 1942c96d1eb64030b8aa290fea31cd80\n",
            "  projectId: cloudtutorial-282208\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-03T08:23:12.859Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-03T08:22:35.497Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-03T08:22:35.535Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-03T08:22:35.799Z'\n",
            "yarnApplications:\n",
            "- name: training_spark.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1593763852919_0003/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijXF2ayrzA9E",
        "colab_type": "text"
      },
      "source": [
        "#### **Noted**\n",
        "As if we just use our account to create it, won't be fine so we need to create a service account with our mail, so that we could do it easier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS0WyYZeyHBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "d7dd7d1b-a9ab-4810-e674-543f328ed81b"
      },
      "source": [
        "# one thing to notice, currently I just submit the job in terminal\n",
        "# more common way should upload our training file into bucket\n",
        "# then trigger the job with file in bucket\n",
        "\n",
        "# let's test it\n",
        "\n",
        "# first we need to create a bucket\n",
        "! gsutil mb gs://dataproc_lugq\n",
        "\n",
        "# then upload our file into bucket\n",
        "! gsutil cp training_spark.py gs://dataproc_lugq\n",
        "\n",
        "# let's check it\n",
        "! gsutil ls gs://dataproc_lugq"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating gs://dataproc_lugq/...\n",
            "Copying file://training_spark.py [Content-Type=text/x-python]...\n",
            "/ [1 files][  1.0 KiB/  1.0 KiB]                                                \n",
            "Operation completed over 1 objects/1.0 KiB.                                      \n",
            "gs://dataproc_lugq/training_spark.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0oU3DaKypP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "outputId": "9e5bc3c3-9656-48ff-be3b-0360be05a2ed"
      },
      "source": [
        "# then let's trigger our training job with storage file\n",
        "! gcloud dataproc jobs submit pyspark gs://dataproc_lugq/training_spark.py \\\n",
        "--cluster $cluster_name \\\n",
        "--region $region"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [dcc583272cdc43199a7a44acca877f92] submitted.\n",
            "Waiting for job output...\n",
            "20/07/03 08:34:31 INFO org.spark_project.jetty.util.log: Logging initialized @3860ms\n",
            "20/07/03 08:34:31 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/03 08:34:31 INFO org.spark_project.jetty.server.Server: Started @4045ms\n",
            "20/07/03 08:34:31 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@7703d473{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/03 08:34:31 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/03 08:34:34 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.2:8032\n",
            "20/07/03 08:34:34 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.2:10200\n",
            "20/07/03 08:34:37 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1593763852919_0004\n",
            "Get final result: \n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|sentence                           |words                                     |tokens|\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "\n",
            "20/07/03 08:35:00 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@7703d473{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [dcc583272cdc43199a7a44acca877f92] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/dcc583272cdc43199a7a44acca877f92/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/dcc583272cdc43199a7a44acca877f92/driveroutput\n",
            "jobUuid: b166f13c-f57b-31c4-ab58-9bb4941c33c0\n",
            "placement:\n",
            "  clusterName: dataproc-spark\n",
            "  clusterUuid: 6ac75383-d0b4-4976-b781-e2c39188d3ad\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc_lugq/training_spark.py\n",
            "reference:\n",
            "  jobId: dcc583272cdc43199a7a44acca877f92\n",
            "  projectId: cloudtutorial-282208\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-03T08:35:03.599Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-03T08:34:26.225Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-03T08:34:26.271Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-03T08:34:26.532Z'\n",
            "yarnApplications:\n",
            "- name: training_spark.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1593763852919_0004/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7uxKrE70Iia",
        "colab_type": "text"
      },
      "source": [
        "Alright, we have submitted our job into cluster both in local and remote **GCS**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7NsPz1OHVAN",
        "colab_type": "text"
      },
      "source": [
        "### Dataproc with GCS\n",
        "\n",
        "Next step is how could we use **Dataproc** with **Cloud storage** to read and write. \n",
        "\n",
        "Let's start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeDQrUW2HhI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09912d7f-7793-487c-d4b1-52cb3050e713"
      },
      "source": [
        "# first let's make some sample data, for most times we will use structured data,\n",
        "# let's make sample file to count the word frequency\n",
        "import os\n",
        "\n",
        "sample_text = \"\"\"\n",
        "This part of the Python Guestbook code walkthrough shows how to deploy the application to App Engine.\n",
        "This page is part of a multi-page tutorial. To start from the beginning and see instructions for setting up, go to Creating a Guestbook.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sample.txt\", 'w') as f:\n",
        "  f.write(sample_text)\n",
        "\n",
        "# let's check\n",
        "print(\"Current folder: \", os.listdir('.'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current folder:  ['.config', 'training_spark.py', 'adc.json', 'sample.txt', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_jwi0jmILWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "0c1b684f-2c13-4ee7-e0a1-07e033134245"
      },
      "source": [
        "# upload this file into bukcet: dataproc_lugq\n",
        "! gsutil cp sample.txt gs://dataproc_lugq/\n",
        "\n",
        "! gsutil ls gs://dataproc_lugq\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://sample.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  240.0 B/  240.0 B]                                                \n",
            "Operation completed over 1 objects/240.0 B.                                      \n",
            "gs://dataproc_lugq/sample.txt\n",
            "gs://dataproc_lugq/training_spark.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SlxEd7lIab5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0958e8a6-2cbc-4810-c117-cdf572e946e0"
      },
      "source": [
        "# next step is our main function for word count\n",
        "\n",
        "%%writefile words_count.py\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "input_uri = \"gs://dataproc_lugq/sample.txt\"\n",
        "\n",
        "# init spark context\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# use spark context to read file, return is a RDD\n",
        "lines = sc.textFile(input_uri)\n",
        "\n",
        "# main step for word count: flatmap to split -> map to k: v -> reduceybykey -> sort\n",
        "word_counts = lines.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1])\n",
        "\n",
        "# let's save our rdd into GCS\n",
        "output_uri = \"gs://dataproc_lugq/words.txt\"\n",
        "word_counts.saveAsTextFile(output_uri)\n",
        "\n",
        "# get whole result into driver memory\n",
        "print(\"Words count:\", word_counts.collect())\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting words_count.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deJTadivJUC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "2f6a221e-9cc4-4e84-fb47-2de3deab604e"
      },
      "source": [
        "# let's deploy our code with cluster and region for where to execute our code\n",
        "! gcloud dataproc jobs submit pyspark words_count.py --cluster $cluster_name \\\n",
        "--region $region"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [86226d09ada643efbed7c3bb619827c0] submitted.\n",
            "Waiting for job output...\n",
            "20/07/07 02:45:57 INFO org.spark_project.jetty.util.log: Logging initialized @3892ms\n",
            "20/07/07 02:45:57 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/07 02:45:57 INFO org.spark_project.jetty.server.Server: Started @4093ms\n",
            "20/07/07 02:45:57 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@7703d473{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/07 02:45:58 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/07 02:46:00 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.2:8032\n",
            "20/07/07 02:46:00 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.2:10200\n",
            "20/07/07 02:46:04 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1594089032599_0004\n",
            "20/07/07 02:46:17 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
            "('Words count:', [(u'and', 1), (u'up,', 1), (u'from', 1), (u'for', 1), (u'Python', 1), (u'is', 1), (u'see', 1), (u'go', 1), (u'beginning', 1), (u'App', 1), (u'Guestbook', 1), (u'To', 1), (u'Guestbook.', 1), (u'code', 1), (u'Creating', 1), (u'walkthrough', 1), (u'multi-page', 1), (u'deploy', 1), (u'page', 1), (u'tutorial.', 1), (u'application', 1), (u'setting', 1), (u'instructions', 1), (u'how', 1), (u'Engine.', 1), (u'start', 1), (u'shows', 1), (u'a', 2), (u'This', 2), (u'of', 2), (u'part', 2), (u'the', 3), (u'to', 3)])\n",
            "20/07/07 02:46:33 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@7703d473{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [86226d09ada643efbed7c3bb619827c0] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/bf99669c-9611-46fd-8a8b-b0586cf386a2/jobs/86226d09ada643efbed7c3bb619827c0/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/bf99669c-9611-46fd-8a8b-b0586cf386a2/jobs/86226d09ada643efbed7c3bb619827c0/driveroutput\n",
            "jobUuid: d8d54ab5-e1da-362e-85ed-f6848b667e6b\n",
            "placement:\n",
            "  clusterName: dataproc-spark\n",
            "  clusterUuid: bf99669c-9611-46fd-8a8b-b0586cf386a2\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-919817286532-wmlkipik/google-cloud-dataproc-metainfo/bf99669c-9611-46fd-8a8b-b0586cf386a2/jobs/86226d09ada643efbed7c3bb619827c0/staging/words_count.py\n",
            "reference:\n",
            "  jobId: 86226d09ada643efbed7c3bb619827c0\n",
            "  projectId: emerald-road-282501\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-07T02:46:34.487Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-07T02:45:52.297Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-07T02:45:52.342Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-07T02:45:52.599Z'\n",
            "yarnApplications:\n",
            "- name: words_count.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1594089032599_0004/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD0mEc42Klgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "01c9af8a-4a8e-4059-a3c4-52a1bc891c3f"
      },
      "source": [
        "# let's check with storage\n",
        "! gsutil ls gs://dataproc_lugq"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://dataproc_lugq/sample.txt\n",
            "gs://dataproc_lugq/training_spark.py\n",
            "gs://dataproc_lugq/words.txt/\n",
            "Copying gs://dataproc_lugq/words.txt/_SUCCESS...\n",
            "Copying gs://dataproc_lugq/words.txt/part-00000...\n",
            "Copying gs://dataproc_lugq/words.txt/part-00001...\n",
            "/ [3 files][  474.0 B/  474.0 B]                                                \n",
            "Operation completed over 3 objects/474.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPhfydguKrD5",
        "colab_type": "text"
      },
      "source": [
        "Well done, we have read and write the data from **Cloud storage**, in fact we could just face with GCS as HDFS or S3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItD0RGvwxe3Y",
        "colab_type": "text"
      },
      "source": [
        "#### Dataproc with Bigquery\n",
        "\n",
        "In fact, if we need to process relational data with big data, with Big query should be the most common way that we could interact with database.\n",
        "\n",
        "Let's created a sample dataset, and upload it into bigquery, use **Dataproc** to read it and do some feature engineering, last step is do a model training on new data, for later usecase we could store our trained model into **GCS** for reference.\n",
        "\n",
        "Let's get start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPacfjii1A-S",
        "colab_type": "text"
      },
      "source": [
        "#### Load data into bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69xyS_WXNwzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36bfed0b-c2ec-4603-be64-ef13a86a8b8d"
      },
      "source": [
        "# first let's create a sample dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "x, y = load_iris(return_X_y = True)\n",
        "data = np.concatenate([x, y[:, np.newaxis]], axis=1)\n",
        "\n",
        "df = pd.DataFrame(data, columns=['a', 'b', 'c', 'd', 'label'])\n",
        "\n",
        "# save our dataframe into disk\n",
        "df.to_csv('data.csv', index=False)\n",
        "\n",
        "print(\"Now what files we have: \", os.listdir('.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now what files we have:  ['.config', 'adc.json', 'training_spark.py', 'data.csv', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK_hQZXW0xyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "ca74415e-fc7c-40d1-fd6f-466553f0d398"
      },
      "source": [
        "# let's upload our data.csv into bucket\n",
        "! gsutil cp data.csv gs://dataproc_lugq/\n",
        "\n",
        "# check we done it\n",
        "! gsutil ls gs://dataproc_lugq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://data.csv [Content-Type=text/csv]...\n",
            "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
            "Operation completed over 1 objects/2.9 KiB.                                      \n",
            "gs://dataproc_lugq/data.csv\n",
            "gs://dataproc_lugq/training_spark.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkcJeWaU1Dcj",
        "colab_type": "text"
      },
      "source": [
        "#### Load data into bigquery\n",
        "\n",
        "Before we do the load action with python, first we do need to create a dataset_id in the console, please just go to the **bigquery** console and create a dataset with **iris_dataset**.\n",
        "\n",
        "After the dataset has been created, let's create our table with python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_1JqUu01lzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first install bigquery module\n",
        "! pip install google-cloud-bigquery --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUsFZLnl09kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1b04e973-c531-4e26-bc6e-a1acadd87d4b"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# we need to create the dataset in console first\n",
        "project_id = \"cloudtutorial-282208\"\n",
        "dataset_id = \"iris_dataset\"\n",
        "bucket_name = \"dataproc_lugq\"\n",
        "\n",
        "# init bigquery client\n",
        "client = bigquery.Client(project_id)\n",
        "\n",
        "# create dataset inference\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "\n",
        "# define schema\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job_config.schema = [bigquery.SchemaField(\"a\", \"float\"),\n",
        "                     bigquery.SchemaField(\"b\", \"float\"),\n",
        "                     bigquery.SchemaField(\"c\", \"float\"),\n",
        "                     bigquery.SchemaField(\"d\", \"float\"),\n",
        "                     bigquery.SchemaField(\"label\", \"float\")]\n",
        "\n",
        "# skip the header, as I skip first row just get 149 records. not correct\n",
        "job_config.skip_leading_rows = 1\n",
        "# set to load csv\n",
        "job_config.source_format = bigquery.SourceFormat.CSV\n",
        "\n",
        "# data uri\n",
        "data_uri = \"gs://{}/{}\".format(bucket_name, \"data.csv\")\n",
        "\n",
        "# create a load job\n",
        "load_job = client.load_table_from_uri(data_uri, dataset_ref.table('iris'), job_config =job_config)\n",
        "print(\"submitted job: {}\".format(load_job.job_id))\n",
        "\n",
        "# wait result to finish\n",
        "load_job.result()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "submitted job: f970b412-4eb4-4581-a9f4-0df615470b17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.cloud.bigquery.job.LoadJob at 0x7f7f87f69550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXjMq4Bq2LAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eabafb25-b39b-47dc-8d84-3959b23b357a"
      },
      "source": [
        "# let's check how many data has been inserted\n",
        "# that's right!\n",
        "response = client.get_table(dataset_ref.table('iris'))\n",
        "\n",
        "print(\"there are {} records in bigquery\".format(response.num_rows))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 150 records in bigquery\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASh2Wk6A3Zf8",
        "colab_type": "text"
      },
      "source": [
        "#### Model Training with Dataproc and Bigquery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3QMWiYu287O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abbdf9dc-ef88-4aa3-cb14-5aa2f9320905"
      },
      "source": [
        "# after we have load data in bigquery\n",
        "# then let's use dataproc to read data from bigquery\n",
        "# so that we could use the power of Spark and Bigquery\n",
        "\n",
        "%%writefile spark_train_bigquery.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# combine features into vector and get lable\n",
        "def inputs_to_vector(row):\n",
        "  return (row['label'], Vectors.dense(float(row['a']), \n",
        "                                      float(row['b']), \n",
        "                                      float(row['c']), \n",
        "                                      float(row['d']) ))\n",
        "\n",
        "# create sparksession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# read bigquery return into a DataFrame\n",
        "logger.info(\"Read data from bigquery\")\n",
        "df = spark.read.format('bigquery').option('table', 'iris_dataset.iris').load()\n",
        "\n",
        "# logger.info(\"get dataframe:\", df.show(5))\n",
        "df.createOrReplaceTempView('iris')\n",
        "\n",
        "df_new = spark.sql(\"select * from iris\")\n",
        "\n",
        "# map dataframe with vector function\n",
        "data_df = df_new.rdd.map(inputs_to_vector).toDF([\"label\", \"features\"])\n",
        "\n",
        "# split into train and test\n",
        "(train_df, test_df) = data_df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# cache dataframe\n",
        "train_df.cache()\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.1,elasticNetParam=0.8)\n",
        "\n",
        "logger.info(\"start to train model\")\n",
        "model = lr.fit(train_df)\n",
        "\n",
        "# get model prediction on test data\n",
        "pred = model.transform(test_df)\n",
        "\n",
        "\n",
        "# let's try to save our trained model into GCS\n",
        "bucket_name = \"dataproc_lugq\"\n",
        "model_folder = \"lr_model\"\n",
        "model_storage_path = \"gs://{}/{}\".format(bucket_name, model_folder)\n",
        "\n",
        "# in case the model already exist\n",
        "# I found that couldn't just save the model file directly into bucket\n",
        "# so let's just save the file into local server, then upload file with command\n",
        "# reference here: https://stackoverflow.com/questions/48684048/save-python-data-object-to-file-in-google-storage-from-a-pyspark-job-running-in\n",
        "local_path = './logistic_model'\n",
        "model.write().overwrite().save(local_path)\n",
        "\n",
        "# let's use command to upload file\n",
        "from subprocess import call\n",
        "print(\"Save model into bucket\")\n",
        "call(['gsutil', 'cp', local_path, model_storage_path])\n",
        "\n",
        "print(\"get prediction:\", pred.show(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting spark_train_bigquery.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXR28d524AAE",
        "colab_type": "text"
      },
      "source": [
        "#### Noted\n",
        "\n",
        "If we need to use **Bigquery** in **Dataproc**, we need to provide the connection between two of them, that's: `--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar`, keep this in mind if you face error with couldn't find `bigquery`, do need to provide it with `jars`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpHvZbz22yck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ccfba98-14d2-4393-b96c-73b787be2d99"
      },
      "source": [
        "# let's submit our job into Dataproc\n",
        "! gcloud dataproc jobs submit pyspark spark_train_bigquery.py \\\n",
        "--cluster $cluster_name \\\n",
        "--region $region \\\n",
        "--jars gs://spark-lib/bigquery/spark-bigquery-latest.jar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [0ce69abc56734071984a541c48037548] submitted.\n",
            "Waiting for job output...\n",
            "20/07/03 09:09:32 INFO org.spark_project.jetty.util.log: Logging initialized @5119ms\n",
            "20/07/03 09:09:32 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/03 09:09:32 INFO org.spark_project.jetty.server.Server: Started @5302ms\n",
            "20/07/03 09:09:32 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@598fc0ab{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/03 09:09:32 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/03 09:09:34 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.2:8032\n",
            "20/07/03 09:09:34 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.2:10200\n",
            "20/07/03 09:09:38 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1593763852919_0009\n",
            "20/07/03 09:09:59 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table cloudtutorial-282208.iris_dataset.iris, parameters sent from Spark: requiredColumns=[a,b,c,d,label], filters=[]\n",
            "20/07/03 09:09:59 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from cloudtutorial-282208.iris_dataset.iris columns=[a, b, c, d, label], filter=''\n",
            "20/07/03 09:10:04 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'cloudtutorial-282208.iris_dataset.iris': projects/cloudtutorial-282208/locations/us/sessions/CAISDGZXbWxOc1BaNzhkbBoCamQaAml3GgJuYRoCb3MaAmluGgJqcRoCanIaAmlyGgJqYxoCb2o\n",
            "20/07/03 09:10:26 INFO com.github.fommil.jni.JniLoader: successfully loaded /tmp/jniloader5643381052663434269netlib-native_system-linux-x86_64.so\n",
            "20/07/03 09:10:26 INFO breeze.optimize.OWLQN: Step Size: 0.05172\n",
            "20/07/03 09:10:26 INFO breeze.optimize.OWLQN: Val and Grad Norm: 1.07010 (rel: 0.0212) 0.665530\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.978226 (rel: 0.0859) 0.908623\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.876949 (rel: 0.104) 1.75098\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.797431 (rel: 0.0907) 0.560358\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.790304 (rel: 0.00894) 0.422982\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.782168 (rel: 0.0103) 0.218110\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.779315 (rel: 0.00365) 0.138319\n",
            "20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.778049 (rel: 0.00162) 0.188257\n",
            "20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.776300 (rel: 0.00225) 0.116034\n",
            "20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Step Size: 1.000\n",
            "20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.774517 (rel: 0.00230) 0.158088\n",
            "20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Converged because max iterations reached\n",
            "Save model into bucket\n",
            "20/07/03 09:11:28 WARN org.apache.hadoop.hdfs.DataStreamer: Exception for BP-1449221820-10.128.0.2-1593763846948:blk_1073741866_1042\n",
            "java.io.EOFException: Unexpected EOF while trying to read response from server\n",
            "\tat org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:453)\n",
            "\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1080)\n",
            "CommandException: No URLs matched: ./logistic_model\n",
            "20/07/03 09:11:33 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
            "java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n",
            "20/07/03 09:11:34 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
            "java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n",
            "20/07/03 09:11:38 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
            "java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n",
            "20/07/03 09:11:38 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
            "java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n",
            "20/07/03 09:11:38 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
            "java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n",
            "+-----+-----------------+--------------------+--------------------+----------+\n",
            "|label|         features|       rawPrediction|         probability|prediction|\n",
            "+-----+-----------------+--------------------+--------------------+----------+\n",
            "|  0.0|[4.8,3.0,1.4,0.3]|[1.30900599110209...|[0.72779424867165...|       0.0|\n",
            "|  0.0|[4.8,3.4,1.6,0.2]|[1.74498138762048...|[0.82107222434310...|       0.0|\n",
            "|  0.0|[4.9,3.1,1.5,0.1]|[1.54020276226484...|[0.77885875812607...|       0.0|\n",
            "|  0.0|[4.9,3.6,1.4,0.1]|[2.09161961601284...|[0.87397201225540...|       0.0|\n",
            "|  0.0|[5.0,3.2,1.2,0.2]|[1.63269575000811...|[0.79721567152778...|       0.0|\n",
            "+-----+-----------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "('get prediction:', None)\n",
            "20/07/03 09:11:38 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
            "java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n",
            "20/07/03 09:11:38 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@598fc0ab{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/03 09:11:38 ERROR org.apache.spark.util.Utils: Uncaught exception in thread shutdown-hook-0\n",
            "java.lang.IllegalArgumentException: Self-suppression not permitted\n",
            "\tat java.lang.Throwable.addSuppressed(Throwable.java:1072)\n",
            "\tat java.io.BufferedWriter.close(BufferedWriter.java:266)\n",
            "\tat java.io.PrintWriter.close(PrintWriter.java:339)\n",
            "\tat org.apache.spark.scheduler.EventLoggingListener$$anonfun$stop$1.apply(EventLoggingListener.scala:242)\n",
            "\tat org.apache.spark.scheduler.EventLoggingListener$$anonfun$stop$1.apply(EventLoggingListener.scala:242)\n",
            "\tat scala.Option.foreach(Option.scala:257)\n",
            "\tat org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:242)\n",
            "\tat org.apache.spark.SparkContext$$anonfun$stop$7$$anonfun$apply$mcV$sp$5.apply(SparkContext.scala:1927)\n",
            "\tat org.apache.spark.SparkContext$$anonfun$stop$7$$anonfun$apply$mcV$sp$5.apply(SparkContext.scala:1927)\n",
            "\tat scala.Option.foreach(Option.scala:257)\n",
            "\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1927)\n",
            "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1361)\n",
            "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1926)\n",
            "\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:573)\n",
            "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n",
            "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n",
            "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n",
            "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\n",
            "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n",
            "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n",
            "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n",
            "\tat scala.util.Try$.apply(Try.scala:192)\n",
            "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
            "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:748)\n",
            "Caused by: java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n",
            "Job [0ce69abc56734071984a541c48037548] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/0ce69abc56734071984a541c48037548/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/0ce69abc56734071984a541c48037548/driveroutput\n",
            "jobUuid: affa37ad-5453-3665-945e-33f902644f76\n",
            "placement:\n",
            "  clusterName: dataproc-spark\n",
            "  clusterUuid: 6ac75383-d0b4-4976-b781-e2c39188d3ad\n",
            "pysparkJob:\n",
            "  jarFileUris:\n",
            "  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/0ce69abc56734071984a541c48037548/staging/spark_train_bigquery.py\n",
            "reference:\n",
            "  jobId: 0ce69abc56734071984a541c48037548\n",
            "  projectId: cloudtutorial-282208\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-03T09:11:42.148Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-03T09:09:24.823Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-03T09:09:24.877Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-03T09:09:25.413Z'\n",
            "yarnApplications:\n",
            "- name: spark_train_bigquery.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1593763852919_0009/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfDdxbt25gQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ce226d6e-f2f7-4df3-83fc-c625390a6c2d"
      },
      "source": [
        "# last let's try to check the storage to find model exists or not\n",
        "# so we do save our trained model into bucket.\n",
        "! gsutil ls gs://dataproc_lugq/lr_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://dataproc_lugq/lr_model/\n",
            "gs://dataproc_lugq/lr_model/data/\n",
            "gs://dataproc_lugq/lr_model/metadata/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLWVkL0W9DTd",
        "colab_type": "text"
      },
      "source": [
        "Alright, we do could train our model with data from **Bigquery** with **Dataproc**, as **Spark** is a unified framework, we could do many things with it, like **ML** and **SQL** etc. What we could do with **Spark** then whole things could be done with **Dataproc**, as it's just a cloud framework support Spark.\n",
        "\n",
        "Let's just remove our storage and dataproc cluster!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvYxx0X_9Biy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "249860a6-b0d2-4fb2-b907-825d5be4d768"
      },
      "source": [
        "# remove dataproc\n",
        "! gcloud dataproc clusters delete dataproc-spark --region $region"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cluster 'dataproc-spark' and all attached disks will be deleted.\n",
            "\n",
            "Do you want to continue (Y/n)?  y\n",
            "\n",
            "Waiting on operation [projects/cloudtutorial-282208/regions/us-central1/operations/aca94a03-5eb6-3c6e-bb1b-6b76606c6183].\n",
            "Deleted [https://dataproc.googleapis.com/v1/projects/cloudtutorial-282208/regions/us-central1/clusters/dataproc-spark].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Rk_4a63upT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "fc4e68e8-734c-4c31-f485-75051cf46f38"
      },
      "source": [
        "# remove the bigquery dataset\n",
        "! bq rm -r -d iris_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Welcome to BigQuery! This script will walk you through the \n",
            "process of initializing your .bigqueryrc configuration file.\n",
            "\n",
            "First, we need to set up your credentials if they do not \n",
            "already exist.\n",
            "\n",
            "Credential creation complete. Now we will select a default project.\n",
            "\n",
            "List of projects:\n",
            "  #        projectId           friendlyName    \n",
            " --- ---------------------- ------------------ \n",
            "  1   cloudtutorial-282208   CloudTutorial     \n",
            "  2   my-project-34336       My Project 34336  \n",
            "Found multiple projects. Please enter a selection for \n",
            "which should be the default, or leave blank to not \n",
            "set a default.\n",
            "\n",
            "Enter a selection (1 - 2): 1\n",
            "\n",
            "BigQuery configuration complete! Type \"bq\" to get started.\n",
            "\n",
            "rm: remove dataset 'cloudtutorial-282208:iris_dataset'? (y/N) y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V94ihc_8-t0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "52983f5e-703e-4f35-e539-a671baea807e"
      },
      "source": [
        "# remove whole buckets\n",
        "from google.cloud import storage\n",
        "\n",
        "client = storage.Client(project_id)\n",
        "\n",
        "buckets_list = list(client.list_buckets())\n",
        "\n",
        "# delete whole buckets\n",
        "for bucket in buckets_list:\n",
        "  print(\"Now to delete: {}\".format(bucket.name))\n",
        "  bucket.delete(force=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now to delete: dataproc-staging-us-central1-397497159726-nw76gzlj\n",
            "Now to delete: dataproc-temp-us-central1-397497159726-jbjqy65f\n",
            "Now to delete: dataproc_lugq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r26lf1N0Nj6r",
        "colab_type": "text"
      },
      "source": [
        "### Last word\n",
        "\n",
        "This tutorial is based on using **Dataproc** to do feature engineering and model training based on **Bigquery**. When we need to do big data processing, maybe we would use **Dataproc** many times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-FVIl3PM_0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}