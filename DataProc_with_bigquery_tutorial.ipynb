{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataProc_with_bigquery_tutorial.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMaESx5uJBJe/n95G0TA9+c"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CEGf8dYO_PNO","colab_type":"text"},"source":["## Introduction\n","\n","According official website: Dataproc is a managed Apache Spark and Apache Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data [official](https://cloud.google.com/dataproc/docs).\n","\n","When we need to use Spark to do data processing, then we could use DataProc as a tool to process data."]},{"cell_type":"code","metadata":{"id":"leM-0VVX_who","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593763867319,"user_tz":-480,"elapsed":18781,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# first auth the lab\n","from google.colab import auth\n","auth.authenticate_user()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"_aB2J7QgFJU6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593763879372,"user_tz":-480,"elapsed":6477,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"03bd1cc7-1d01-47af-cb4e-c71d02630fb7"},"source":["# install dataproc first\n","! pip install google-cloud-dataproc --quiet"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |█▏                              | 10kB 29.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 6.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 112kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 143kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 194kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 225kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 256kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 7.6MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9-LzfCFytycU","colab_type":"text"},"source":["### Create a cluster\n","\n","Please follow these steps to create a cluster from [here](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-console). I just create a cluster with one node, this doesn't matter of the code, **Spark** will handle whole logic for us.\n","\n","We do create a **gcloud** to create a cluster, but here I just create a cluster with console."]},{"cell_type":"code","metadata":{"id":"MzAzsq-Sob8J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593763889104,"user_tz":-480,"elapsed":3797,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"51815e2f-3d1c-4b9b-f9fa-075983d32bf4"},"source":["! gcloud config set project \tcloudtutorial-282208"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_c0AwrKTGQOJ","colab_type":"text"},"source":["## Noted\n","\n","As with the `sandbox` problem, I haven't make the job run successfully, as the reason is for DataProc, it will start a cluster in compute engine, this is really expensive for `sandbox`, so the logic should do be similiar, we could sand our jars into the server and start the job in the remote server, but do keep in mind, we have to delete the cluster in the dataproc, so that we won't cost so much. In fact, this is just a solution that we could start our Spark job in cloud but with cluster created manually by us."]},{"cell_type":"code","metadata":{"id":"ntp8jxO7u1Dl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593764052774,"user_tz":-480,"elapsed":1237,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# define with basic info\n","cluster_name = \"dataproc-spark\"\n","region = \"us-central1\"\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yc-Z44WRhzTF","colab_type":"text"},"source":["## Spark Feature engineering with Dataproc\n","\n","Now we could start our training or processing logic with PySpark, here I just create a sample file to use Spark to do feature extraction. After we  finish the logic, then we could upload the training file into storage for later use case like load data into **Bigquery**, then we could query the result from **bigquery** and do the later step."]},{"cell_type":"code","metadata":{"id":"ao2_m3EzKVqn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593764552417,"user_tz":-480,"elapsed":709,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"982b6ee5-92e8-431d-aab3-160cdc31029c"},"source":["# here I write the job logic\n","%%writefile training_spark.py\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import IntegerType\n","\n","import logging\n","\n","logger = logging.getLogger(__name__)\n","\n","logger.info('init spark')\n","spark = SparkSession.builder.getOrCreate()\n","\n","sentenceDataFrame = spark.createDataFrame([\n","    (0, \"Hi I heard about Spark\"),\n","    (1, \"I wish Java could use case classes\"),\n","    (2, \"Logistic,regression,models,are,neat\")\n","], [\"id\", \"sentence\"])\n","\n","# split sentence into words.\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","\n","# This a UDF function created by ourselves to get length of sentence.\n","countTokens = udf(lambda words: len(words), IntegerType())\n","\n","# with udf function to get each sentence length.\n","tokenized = tokenizer.transform(sentenceDataFrame)\n","token_selected = tokenized.select(\"sentence\", \"words\")\\\n","    .withColumn(\"tokens\", countTokens(col(\"words\")))\n","\n","print(\"Get final result: \")\n","token_selected.show(truncate=False)\n","\n","logger.info('whole Spark logic finished.')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Overwriting training_spark.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H6v-cgO0vZU4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593764595662,"user_tz":-480,"elapsed":43932,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"1786b6e7-f30b-44ac-df57-89b4db4d0ac4"},"source":["# we could submit our job here with just a gcloud command,\n","# let's show it\n","# then we could get whole output from here or we could get the info with console\n","# with Dataproc `jobs` tab.\n","! gcloud dataproc jobs submit pyspark training_spark.py --cluster $cluster_name --region $region"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Job [1942c96d1eb64030b8aa290fea31cd80] submitted.\n","Waiting for job output...\n","20/07/03 08:22:40 INFO org.spark_project.jetty.util.log: Logging initialized @3748ms\n","20/07/03 08:22:41 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n","20/07/03 08:22:41 INFO org.spark_project.jetty.server.Server: Started @3930ms\n","20/07/03 08:22:41 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@3f432a74{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","20/07/03 08:22:41 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n","20/07/03 08:22:43 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.2:8032\n","20/07/03 08:22:43 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.2:10200\n","20/07/03 08:22:46 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\n","java.lang.InterruptedException\n","\tat java.lang.Object.wait(Native Method)\n","\tat java.lang.Thread.join(Thread.java:1252)\n","\tat java.lang.Thread.join(Thread.java:1326)\n","\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:980)\n","\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:630)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:807)\n","20/07/03 08:22:47 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1593763852919_0003\n","Get final result: \n","+-----------------------------------+------------------------------------------+------+\n","|sentence                           |words                                     |tokens|\n","+-----------------------------------+------------------------------------------+------+\n","|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n","|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n","|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n","+-----------------------------------+------------------------------------------+------+\n","\n","20/07/03 08:23:09 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@3f432a74{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","Job [1942c96d1eb64030b8aa290fea31cd80] finished successfully.\n","done: true\n","driverControlFilesUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/1942c96d1eb64030b8aa290fea31cd80/\n","driverOutputResourceUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/1942c96d1eb64030b8aa290fea31cd80/driveroutput\n","jobUuid: aad9ffce-d1b5-340d-b68f-0bb3bb16041d\n","placement:\n","  clusterName: dataproc-spark\n","  clusterUuid: 6ac75383-d0b4-4976-b781-e2c39188d3ad\n","pysparkJob:\n","  mainPythonFileUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/1942c96d1eb64030b8aa290fea31cd80/staging/training_spark.py\n","reference:\n","  jobId: 1942c96d1eb64030b8aa290fea31cd80\n","  projectId: cloudtutorial-282208\n","status:\n","  state: DONE\n","  stateStartTime: '2020-07-03T08:23:12.859Z'\n","statusHistory:\n","- state: PENDING\n","  stateStartTime: '2020-07-03T08:22:35.497Z'\n","- state: SETUP_DONE\n","  stateStartTime: '2020-07-03T08:22:35.535Z'\n","- details: Agent reported job success\n","  state: RUNNING\n","  stateStartTime: '2020-07-03T08:22:35.799Z'\n","yarnApplications:\n","- name: training_spark.py\n","  progress: 1.0\n","  state: FINISHED\n","  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1593763852919_0003/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ijXF2ayrzA9E","colab_type":"text"},"source":["#### **Noted**\n","As if we just use our account to create it, won't be fine so we need to create a service account with our mail, so that we could do it easier"]},{"cell_type":"code","metadata":{"id":"WS0WyYZeyHBe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":103},"executionInfo":{"status":"ok","timestamp":1593765211916,"user_tz":-480,"elapsed":11172,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"1207c65f-4274-48a6-a679-e4660e243882"},"source":["# one thing to notice, currently I just submit the job in terminal\n","# more common way should upload our training file into bucket\n","# then trigger the job with file in bucket\n","\n","# let's test it\n","\n","# first we need to create a bucket\n","! gsutil mb gs://dataproc_lugq\n","\n","# then upload our file into bucket\n","! gsutil cp training_spark.py gs://dataproc_lugq\n","\n","# let's check it\n","! gsutil ls gs://dataproc_lugq"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Creating gs://dataproc_lugq/...\n","Copying file://training_spark.py [Content-Type=text/x-python]...\n","/ [1 files][  970.0 B/  970.0 B]                                                \n","Operation completed over 1 objects/970.0 B.                                      \n","gs://dataproc_lugq/training_spark.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A0oU3DaKypP3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":883},"executionInfo":{"status":"ok","timestamp":1593765306623,"user_tz":-480,"elapsed":42866,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"9e5bc3c3-9656-48ff-be3b-0360be05a2ed"},"source":["# then let's trigger our training job with storage file\n","! gcloud dataproc jobs submit pyspark gs://dataproc_lugq/training_spark.py \\\n","--cluster $cluster_name \\\n","--region $region"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Job [dcc583272cdc43199a7a44acca877f92] submitted.\n","Waiting for job output...\n","20/07/03 08:34:31 INFO org.spark_project.jetty.util.log: Logging initialized @3860ms\n","20/07/03 08:34:31 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n","20/07/03 08:34:31 INFO org.spark_project.jetty.server.Server: Started @4045ms\n","20/07/03 08:34:31 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@7703d473{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","20/07/03 08:34:31 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n","20/07/03 08:34:34 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.2:8032\n","20/07/03 08:34:34 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.2:10200\n","20/07/03 08:34:37 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1593763852919_0004\n","Get final result: \n","+-----------------------------------+------------------------------------------+------+\n","|sentence                           |words                                     |tokens|\n","+-----------------------------------+------------------------------------------+------+\n","|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n","|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n","|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n","+-----------------------------------+------------------------------------------+------+\n","\n","20/07/03 08:35:00 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@7703d473{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","Job [dcc583272cdc43199a7a44acca877f92] finished successfully.\n","done: true\n","driverControlFilesUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/dcc583272cdc43199a7a44acca877f92/\n","driverOutputResourceUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/dcc583272cdc43199a7a44acca877f92/driveroutput\n","jobUuid: b166f13c-f57b-31c4-ab58-9bb4941c33c0\n","placement:\n","  clusterName: dataproc-spark\n","  clusterUuid: 6ac75383-d0b4-4976-b781-e2c39188d3ad\n","pysparkJob:\n","  mainPythonFileUri: gs://dataproc_lugq/training_spark.py\n","reference:\n","  jobId: dcc583272cdc43199a7a44acca877f92\n","  projectId: cloudtutorial-282208\n","status:\n","  state: DONE\n","  stateStartTime: '2020-07-03T08:35:03.599Z'\n","statusHistory:\n","- state: PENDING\n","  stateStartTime: '2020-07-03T08:34:26.225Z'\n","- state: SETUP_DONE\n","  stateStartTime: '2020-07-03T08:34:26.271Z'\n","- details: Agent reported job success\n","  state: RUNNING\n","  stateStartTime: '2020-07-03T08:34:26.532Z'\n","yarnApplications:\n","- name: training_spark.py\n","  progress: 1.0\n","  state: FINISHED\n","  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1593763852919_0004/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i7uxKrE70Iia","colab_type":"text"},"source":["Alright, we have submitted our job into cluster both in local and remote **GCS**."]},{"cell_type":"markdown","metadata":{"id":"ItD0RGvwxe3Y","colab_type":"text"},"source":["#### Dataproc with Bigquery\n","\n","In fact, if we need to process relational data with big data, with Big query should be the most common way that we could interact with database.\n","\n","Let's created a sample dataset, and upload it into bigquery, use **Dataproc** to read it and do some feature engineering, last step is do a model training on new data, for later usecase we could store our trained model into **GCS** for reference.\n","\n","Let's get start."]},{"cell_type":"markdown","metadata":{"id":"YPacfjii1A-S","colab_type":"text"},"source":["#### Load data into bucket"]},{"cell_type":"code","metadata":{"id":"69xyS_WXNwzV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593765470175,"user_tz":-480,"elapsed":695,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"36bfed0b-c2ec-4603-be64-ef13a86a8b8d"},"source":["# first let's create a sample dataset\n","import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.datasets import load_iris\n","\n","x, y = load_iris(return_X_y = True)\n","data = np.concatenate([x, y[:, np.newaxis]], axis=1)\n","\n","df = pd.DataFrame(data, columns=['a', 'b', 'c', 'd', 'label'])\n","\n","# save our dataframe into disk\n","df.to_csv('data.csv', index=False)\n","\n","print(\"Now what files we have: \", os.listdir('.'))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Now what files we have:  ['.config', 'adc.json', 'training_spark.py', 'data.csv', 'sample_data']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yK_hQZXW0xyH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":103},"executionInfo":{"status":"ok","timestamp":1593765525354,"user_tz":-480,"elapsed":7684,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"ca74415e-fc7c-40d1-fd6f-466553f0d398"},"source":["# let's upload our data.csv into bucket\n","! gsutil cp data.csv gs://dataproc_lugq/\n","\n","# check we done it\n","! gsutil ls gs://dataproc_lugq"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Copying file://data.csv [Content-Type=text/csv]...\n","/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n","Operation completed over 1 objects/2.9 KiB.                                      \n","gs://dataproc_lugq/data.csv\n","gs://dataproc_lugq/training_spark.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EkcJeWaU1Dcj","colab_type":"text"},"source":["#### Load data into bigquery\n","\n","Before we do the load action with python, first we do need to create a dataset_id in the console, please just go to the **bigquery** console and create a dataset with **iris_dataset**.\n","\n","After the dataset has been created, let's create our table with python."]},{"cell_type":"code","metadata":{"id":"u_1JqUu01lzI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593765707162,"user_tz":-480,"elapsed":5516,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# first install bigquery module\n","! pip install google-cloud-bigquery --quiet"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUsFZLnl09kY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593765925897,"user_tz":-480,"elapsed":5577,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"1b04e973-c531-4e26-bc6e-a1acadd87d4b"},"source":["from google.cloud import bigquery\n","\n","# we need to create the dataset in console first\n","project_id = \"cloudtutorial-282208\"\n","dataset_id = \"iris_dataset\"\n","bucket_name = \"dataproc_lugq\"\n","\n","# init bigquery client\n","client = bigquery.Client(project_id)\n","\n","# create dataset inference\n","dataset_ref = client.dataset(dataset_id)\n","\n","# define schema\n","job_config = bigquery.LoadJobConfig()\n","job_config.schema = [bigquery.SchemaField(\"a\", \"float\"),\n","                     bigquery.SchemaField(\"b\", \"float\"),\n","                     bigquery.SchemaField(\"c\", \"float\"),\n","                     bigquery.SchemaField(\"d\", \"float\"),\n","                     bigquery.SchemaField(\"label\", \"float\")]\n","\n","# skip the header, as I skip first row just get 149 records. not correct\n","job_config.skip_leading_rows = 1\n","# set to load csv\n","job_config.source_format = bigquery.SourceFormat.CSV\n","\n","# data uri\n","data_uri = \"gs://{}/{}\".format(bucket_name, \"data.csv\")\n","\n","# create a load job\n","load_job = client.load_table_from_uri(data_uri, dataset_ref.table('iris'), job_config =job_config)\n","print(\"submitted job: {}\".format(load_job.job_id))\n","\n","# wait result to finish\n","load_job.result()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["submitted job: f970b412-4eb4-4581-a9f4-0df615470b17\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<google.cloud.bigquery.job.LoadJob at 0x7f7f87f69550>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"hXjMq4Bq2LAb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593766024092,"user_tz":-480,"elapsed":1226,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"eabafb25-b39b-47dc-8d84-3959b23b357a"},"source":["# let's check how many data has been inserted\n","# that's right!\n","response = client.get_table(dataset_ref.table('iris'))\n","\n","print(\"there are {} records in bigquery\".format(response.num_rows))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["there are 150 records in bigquery\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ASh2Wk6A3Zf8","colab_type":"text"},"source":["#### Model Training with Dataproc and Bigquery"]},{"cell_type":"code","metadata":{"id":"a3QMWiYu287O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593767360565,"user_tz":-480,"elapsed":1847,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"abbdf9dc-ef88-4aa3-cb14-5aa2f9320905"},"source":["# after we have load data in bigquery\n","# then let's use dataproc to read data from bigquery\n","# so that we could use the power of Spark and Bigquery\n","\n","%%writefile spark_train_bigquery.py\n","\n","from pyspark.sql import SparkSession\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.classification import LogisticRegression\n","import logging\n","\n","logger = logging.getLogger(__name__)\n","\n","# combine features into vector and get lable\n","def inputs_to_vector(row):\n","  return (row['label'], Vectors.dense(float(row['a']), \n","                                      float(row['b']), \n","                                      float(row['c']), \n","                                      float(row['d']) ))\n","\n","# create sparksession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# read bigquery return into a DataFrame\n","logger.info(\"Read data from bigquery\")\n","df = spark.read.format('bigquery').option('table', 'iris_dataset.iris').load()\n","\n","# logger.info(\"get dataframe:\", df.show(5))\n","df.createOrReplaceTempView('iris')\n","\n","df_new = spark.sql(\"select * from iris\")\n","\n","# map dataframe with vector function\n","data_df = df_new.rdd.map(inputs_to_vector).toDF([\"label\", \"features\"])\n","\n","# split into train and test\n","(train_df, test_df) = data_df.randomSplit([0.7, 0.3])\n","\n","# cache dataframe\n","train_df.cache()\n","\n","lr = LogisticRegression(maxIter=10, regParam=0.1,elasticNetParam=0.8)\n","\n","logger.info(\"start to train model\")\n","model = lr.fit(train_df)\n","\n","# get model prediction on test data\n","pred = model.transform(test_df)\n","\n","\n","# let's try to save our trained model into GCS\n","bucket_name = \"dataproc_lugq\"\n","model_folder = \"lr_model\"\n","model_storage_path = \"gs://{}/{}\".format(bucket_name, model_folder)\n","\n","# in case the model already exist\n","# I found that couldn't just save the model file directly into bucket\n","# so let's just save the file into local server, then upload file with command\n","# reference here: https://stackoverflow.com/questions/48684048/save-python-data-object-to-file-in-google-storage-from-a-pyspark-job-running-in\n","local_path = './logistic_model'\n","model.write().overwrite().save(local_path)\n","\n","# let's use command to upload file\n","from subprocess import call\n","print(\"Save model into bucket\")\n","call(['gsutil', 'cp', local_path, model_storage_path])\n","\n","print(\"get prediction:\", pred.show(5))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Overwriting spark_train_bigquery.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VXR28d524AAE","colab_type":"text"},"source":["#### Noted\n","\n","If we need to use **Bigquery** in **Dataproc**, we need to provide the connection between two of them, that's: `--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar`, keep this in mind if you face error with couldn't find `bigquery`, do need to provide it with `jars`."]},{"cell_type":"code","metadata":{"id":"wpHvZbz22yck","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593767505285,"user_tz":-480,"elapsed":143662,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"5ccfba98-14d2-4393-b96c-73b787be2d99"},"source":["# let's submit our job into Dataproc\n","! gcloud dataproc jobs submit pyspark spark_train_bigquery.py \\\n","--cluster $cluster_name \\\n","--region $region \\\n","--jars gs://spark-lib/bigquery/spark-bigquery-latest.jar"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Job [0ce69abc56734071984a541c48037548] submitted.\n","Waiting for job output...\n","20/07/03 09:09:32 INFO org.spark_project.jetty.util.log: Logging initialized @5119ms\n","20/07/03 09:09:32 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n","20/07/03 09:09:32 INFO org.spark_project.jetty.server.Server: Started @5302ms\n","20/07/03 09:09:32 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@598fc0ab{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","20/07/03 09:09:32 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n","20/07/03 09:09:34 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-spark-m/10.128.0.2:8032\n","20/07/03 09:09:34 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dataproc-spark-m/10.128.0.2:10200\n","20/07/03 09:09:38 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1593763852919_0009\n","20/07/03 09:09:59 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table cloudtutorial-282208.iris_dataset.iris, parameters sent from Spark: requiredColumns=[a,b,c,d,label], filters=[]\n","20/07/03 09:09:59 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from cloudtutorial-282208.iris_dataset.iris columns=[a, b, c, d, label], filter=''\n","20/07/03 09:10:04 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'cloudtutorial-282208.iris_dataset.iris': projects/cloudtutorial-282208/locations/us/sessions/CAISDGZXbWxOc1BaNzhkbBoCamQaAml3GgJuYRoCb3MaAmluGgJqcRoCanIaAmlyGgJqYxoCb2o\n","20/07/03 09:10:26 INFO com.github.fommil.jni.JniLoader: successfully loaded /tmp/jniloader5643381052663434269netlib-native_system-linux-x86_64.so\n","20/07/03 09:10:26 INFO breeze.optimize.OWLQN: Step Size: 0.05172\n","20/07/03 09:10:26 INFO breeze.optimize.OWLQN: Val and Grad Norm: 1.07010 (rel: 0.0212) 0.665530\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.978226 (rel: 0.0859) 0.908623\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.876949 (rel: 0.104) 1.75098\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.797431 (rel: 0.0907) 0.560358\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.790304 (rel: 0.00894) 0.422982\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.782168 (rel: 0.0103) 0.218110\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:27 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.779315 (rel: 0.00365) 0.138319\n","20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.778049 (rel: 0.00162) 0.188257\n","20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.776300 (rel: 0.00225) 0.116034\n","20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.774517 (rel: 0.00230) 0.158088\n","20/07/03 09:10:28 INFO breeze.optimize.OWLQN: Converged because max iterations reached\n","Save model into bucket\n","20/07/03 09:11:28 WARN org.apache.hadoop.hdfs.DataStreamer: Exception for BP-1449221820-10.128.0.2-1593763846948:blk_1073741866_1042\n","java.io.EOFException: Unexpected EOF while trying to read response from server\n","\tat org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:453)\n","\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)\n","\tat org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1080)\n","CommandException: No URLs matched: ./logistic_model\n","20/07/03 09:11:33 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n","java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n","\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n","\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n","\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n","20/07/03 09:11:34 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n","java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n","\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n","\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n","\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n","20/07/03 09:11:38 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n","java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n","\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n","\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n","\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n","20/07/03 09:11:38 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n","java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n","\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n","\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n","\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n","20/07/03 09:11:38 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n","java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n","\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n","\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n","\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n","+-----+-----------------+--------------------+--------------------+----------+\n","|label|         features|       rawPrediction|         probability|prediction|\n","+-----+-----------------+--------------------+--------------------+----------+\n","|  0.0|[4.8,3.0,1.4,0.3]|[1.30900599110209...|[0.72779424867165...|       0.0|\n","|  0.0|[4.8,3.4,1.6,0.2]|[1.74498138762048...|[0.82107222434310...|       0.0|\n","|  0.0|[4.9,3.1,1.5,0.1]|[1.54020276226484...|[0.77885875812607...|       0.0|\n","|  0.0|[4.9,3.6,1.4,0.1]|[2.09161961601284...|[0.87397201225540...|       0.0|\n","|  0.0|[5.0,3.2,1.2,0.2]|[1.63269575000811...|[0.79721567152778...|       0.0|\n","+-----+-----------------+--------------------+--------------------+----------+\n","only showing top 5 rows\n","\n","('get prediction:', None)\n","20/07/03 09:11:38 ERROR org.apache.spark.scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n","java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n","\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n","\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n","\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n","20/07/03 09:11:38 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@598fc0ab{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","20/07/03 09:11:38 ERROR org.apache.spark.util.Utils: Uncaught exception in thread shutdown-hook-0\n","java.lang.IllegalArgumentException: Self-suppression not permitted\n","\tat java.lang.Throwable.addSuppressed(Throwable.java:1072)\n","\tat java.io.BufferedWriter.close(BufferedWriter.java:266)\n","\tat java.io.PrintWriter.close(PrintWriter.java:339)\n","\tat org.apache.spark.scheduler.EventLoggingListener$$anonfun$stop$1.apply(EventLoggingListener.scala:242)\n","\tat org.apache.spark.scheduler.EventLoggingListener$$anonfun$stop$1.apply(EventLoggingListener.scala:242)\n","\tat scala.Option.foreach(Option.scala:257)\n","\tat org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:242)\n","\tat org.apache.spark.SparkContext$$anonfun$stop$7$$anonfun$apply$mcV$sp$5.apply(SparkContext.scala:1927)\n","\tat org.apache.spark.SparkContext$$anonfun$stop$7$$anonfun$apply$mcV$sp$5.apply(SparkContext.scala:1927)\n","\tat scala.Option.foreach(Option.scala:257)\n","\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1927)\n","\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1361)\n","\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1926)\n","\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:573)\n","\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n","\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n","\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n","\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\n","\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n","\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n","\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n","\tat scala.util.Try$.apply(Try.scala:192)\n","\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n","\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","Caused by: java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.128.0.2:9866,DS-8642717c-dec6-4737-a45a-418e2344beac,DISK]] are bad. Aborting...\n","\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538)\n","\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472)\n","\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)\n","Job [0ce69abc56734071984a541c48037548] finished successfully.\n","done: true\n","driverControlFilesUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/0ce69abc56734071984a541c48037548/\n","driverOutputResourceUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/0ce69abc56734071984a541c48037548/driveroutput\n","jobUuid: affa37ad-5453-3665-945e-33f902644f76\n","placement:\n","  clusterName: dataproc-spark\n","  clusterUuid: 6ac75383-d0b4-4976-b781-e2c39188d3ad\n","pysparkJob:\n","  jarFileUris:\n","  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n","  mainPythonFileUri: gs://dataproc-staging-us-central1-397497159726-nw76gzlj/google-cloud-dataproc-metainfo/6ac75383-d0b4-4976-b781-e2c39188d3ad/jobs/0ce69abc56734071984a541c48037548/staging/spark_train_bigquery.py\n","reference:\n","  jobId: 0ce69abc56734071984a541c48037548\n","  projectId: cloudtutorial-282208\n","status:\n","  state: DONE\n","  stateStartTime: '2020-07-03T09:11:42.148Z'\n","statusHistory:\n","- state: PENDING\n","  stateStartTime: '2020-07-03T09:09:24.823Z'\n","- state: SETUP_DONE\n","  stateStartTime: '2020-07-03T09:09:24.877Z'\n","- details: Agent reported job success\n","  state: RUNNING\n","  stateStartTime: '2020-07-03T09:09:25.413Z'\n","yarnApplications:\n","- name: spark_train_bigquery.py\n","  progress: 1.0\n","  state: FINISHED\n","  trackingUrl: http://dataproc-spark-m:8088/proxy/application_1593763852919_0009/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nfDdxbt25gQN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593767569517,"user_tz":-480,"elapsed":5149,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"ce226d6e-f2f7-4df3-83fc-c625390a6c2d"},"source":["# last let's try to check the storage to find model exists or not\n","# so we do save our trained model into bucket.\n","! gsutil ls gs://dataproc_lugq/lr_model"],"execution_count":39,"outputs":[{"output_type":"stream","text":["gs://dataproc_lugq/lr_model/\n","gs://dataproc_lugq/lr_model/data/\n","gs://dataproc_lugq/lr_model/metadata/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PLWVkL0W9DTd","colab_type":"text"},"source":["Alright, we do could train our model with data from **Bigquery** with **Dataproc**, as **Spark** is a unified framework, we could do many things with it, like **ML** and **SQL** etc. What we could do with **Spark** then whole things could be done with **Dataproc**, as it's just a cloud framework support Spark.\n","\n","Let's just remove our storage and dataproc cluster!"]},{"cell_type":"code","metadata":{"id":"xvYxx0X_9Biy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":120},"executionInfo":{"status":"ok","timestamp":1593767994675,"user_tz":-480,"elapsed":61858,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"249860a6-b0d2-4fb2-b907-825d5be4d768"},"source":["# remove dataproc\n","! gcloud dataproc clusters delete dataproc-spark --region $region"],"execution_count":42,"outputs":[{"output_type":"stream","text":["The cluster 'dataproc-spark' and all attached disks will be deleted.\n","\n","Do you want to continue (Y/n)?  y\n","\n","Waiting on operation [projects/cloudtutorial-282208/regions/us-central1/operations/aca94a03-5eb6-3c6e-bb1b-6b76606c6183].\n","Deleted [https://dataproc.googleapis.com/v1/projects/cloudtutorial-282208/regions/us-central1/clusters/dataproc-spark].\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G5Rk_4a63upT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":414},"executionInfo":{"status":"ok","timestamp":1593768099079,"user_tz":-480,"elapsed":24147,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"fc4e68e8-734c-4c31-f485-75051cf46f38"},"source":["! bq rm -r -d iris_dataset"],"execution_count":43,"outputs":[{"output_type":"stream","text":["\n","Welcome to BigQuery! This script will walk you through the \n","process of initializing your .bigqueryrc configuration file.\n","\n","First, we need to set up your credentials if they do not \n","already exist.\n","\n","Credential creation complete. Now we will select a default project.\n","\n","List of projects:\n","  #        projectId           friendlyName    \n"," --- ---------------------- ------------------ \n","  1   cloudtutorial-282208   CloudTutorial     \n","  2   my-project-34336       My Project 34336  \n","Found multiple projects. Please enter a selection for \n","which should be the default, or leave blank to not \n","set a default.\n","\n","Enter a selection (1 - 2): 1\n","\n","BigQuery configuration complete! Type \"bq\" to get started.\n","\n","rm: remove dataset 'cloudtutorial-282208:iris_dataset'? (y/N) y\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V94ihc_8-t0r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593771965458,"user_tz":-480,"elapsed":10548,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"52983f5e-703e-4f35-e539-a671baea807e"},"source":["# remove whole buckets\n","from google.cloud import storage\n","\n","client = storage.Client(project_id)\n","\n","buckets_list = list(client.list_buckets())\n","\n","# delete whole buckets\n","for bucket in buckets_list:\n","  print(\"Now to delete: {}\".format(bucket.name))\n","  bucket.delete(force=True)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Now to delete: dataproc-staging-us-central1-397497159726-nw76gzlj\n","Now to delete: dataproc-temp-us-central1-397497159726-jbjqy65f\n","Now to delete: dataproc_lugq\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r26lf1N0Nj6r","colab_type":"text"},"source":["### Last word\n","\n","This tutorial is based on using **Dataproc** to do feature engineering and model training based on **Bigquery**. When we need to do big data processing, maybe we would use **Dataproc** many times."]},{"cell_type":"code","metadata":{"id":"H-FVIl3PM_0x","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}