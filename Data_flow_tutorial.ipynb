{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data Flow tutorial.ipynb","provenance":[],"authorship_tag":"ABX9TyOnd37D9lHPoDESiVQ9AiRi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uz4KqUusDLWE","colab_type":"text"},"source":["## DataFlow with Python\n","\n","GCP data flow is based on apache-beam open source project to process data with unified data processing tool, just like Spark, we could do batch processing and streaming processing withg data, like Spark batch-processing and structured-streaming functionility.\n","\n","So if we do need to use it in real project, we need to learn apache-beam definitely, so the transformation logic is really like RDD transformation.\n","\n","Here is to use Python language for tutorial. "]},{"cell_type":"code","metadata":{"id":"aG9qwQ_FEOME","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":460},"outputId":"2c99f99f-006f-49f7-ec14-c1dceebf9f9b","executionInfo":{"status":"ok","timestamp":1591057030912,"user_tz":-480,"elapsed":45431,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# First we need to install with apache-beam\n","! pip install apache-beam[gcp] --quiet"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 3.5MB 4.6MB/s \n","\u001b[K     |████████████████████████████████| 225kB 33.6MB/s \n","\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n","\u001b[K     |████████████████████████████████| 1.2MB 41.2MB/s \n","\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n","\u001b[K     |████████████████████████████████| 81kB 7.1MB/s \n","\u001b[K     |████████████████████████████████| 63.2MB 55kB/s \n","\u001b[K     |████████████████████████████████| 184kB 47.2MB/s \n","\u001b[K     |████████████████████████████████| 92kB 9.4MB/s \n","\u001b[K     |████████████████████████████████| 215kB 44.4MB/s \n","\u001b[K     |████████████████████████████████| 153kB 48.3MB/s \n","\u001b[K     |████████████████████████████████| 174kB 42.2MB/s \n","\u001b[K     |████████████████████████████████| 440kB 44.3MB/s \n","\u001b[K     |████████████████████████████████| 92kB 9.8MB/s \n","\u001b[K     |████████████████████████████████| 122kB 49.8MB/s \n","\u001b[K     |████████████████████████████████| 235kB 41.7MB/s \n","\u001b[K     |████████████████████████████████| 112kB 39.7MB/s \n","\u001b[?25h  Building wheel for httplib2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-api-python-client 1.7.12 has requirement httplib2<1dev,>=0.17.0, but you'll have httplib2 0.12.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: chainer 6.5.0 has requirement typing-extensions<=3.6.6, but you'll have typing-extensions 3.7.4.2 which is incompatible.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yc6YU6pCElvx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":566},"outputId":"539018cc-0f79-4b25-989c-9b18bfdf79ac","executionInfo":{"status":"ok","timestamp":1591057046033,"user_tz":-480,"elapsed":6646,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# in fact, if we finished the installment, we could do the test with python command to do the test\n","# for apache-beam\n","! python -m apache_beam.examples.wordcount --output outputs"],"execution_count":2,"outputs":[{"output_type":"stream","text":["INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","WARNING:apache_beam.internal.gcp.auth:Unable to find default credentials to use: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.\n","Connecting anonymously.\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f4113604840> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f4113604950> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f41136049d8> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f4113604a60> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f4113604ae8> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f4113604bf8> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f4113604c80> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f4113604d08> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f4113604d90> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f4113605048> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f4113604f28> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f41136050d0> ====================\n","INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n","INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f411302a198> for environment urn: \"beam:env:embedded_python:v1\"\n","\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_read/Read/_SDFBoundedSourceWrapper/Impulse_5)+(read/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(read/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_PCollection_PCollection_1_split/Read)+(read/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_split_7))+(ref_AppliedPTransform_pair_with_one_8))+(group/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Impulse_19)+(ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2706>)_20))+(ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Map(decode)_22))+(ref_AppliedPTransform_write/Write/WriteImpl/InitializeWrite_23))+(ref_PCollection_PCollection_12/Write))+(ref_PCollection_PCollection_13/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((group/Read)+(ref_AppliedPTransform_count_13))+(ref_AppliedPTransform_format_14))+(ref_AppliedPTransform_write/Write/WriteImpl/WriteBundles_24))+(ref_AppliedPTransform_write/Write/WriteImpl/Pair_25))+(ref_AppliedPTransform_write/Write/WriteImpl/WindowInto(WindowIntoFn)_26))+(write/Write/WriteImpl/GroupByKey/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_write/Write/WriteImpl/Extract_31))+(ref_PCollection_PCollection_20/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_12/Read)+(ref_AppliedPTransform_write/Write/WriteImpl/PreFinalize_32))+(ref_PCollection_PCollection_21/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_12/Read)+(ref_AppliedPTransform_write/Write/WriteImpl/FinalizeWrite_33)\n","INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n","INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n","INFO:root:number of empty lines: 1663\n","INFO:root:average word length: 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bh_ZKCIOE5OX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"c12f7191-8fa1-4552-bc79-852b5bf83a57","executionInfo":{"status":"ok","timestamp":1591057052304,"user_tz":-480,"elapsed":947,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# this file will be outputed into a file started with outputs**\n","import os\n","\n","os.listdir('.')"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config',\n"," 'outputs-00000-of-00001',\n"," 'CloudTutorial-e8d69ceedc4d.json',\n"," 'sample_data']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"y_W7zTnbE_H7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"21942528-420b-4f1d-acc6-fd55ad161637","executionInfo":{"status":"ok","timestamp":1591057055154,"user_tz":-480,"elapsed":883,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# let's get some samples for show case\n","file_name = [x for x in os.listdir('.') if x.startswith('outputs')][0]\n","with open(file_name) as f:\n","  data = f.readlines()\n","\n","# as we could see that result is a file with each word count.\n","data[:10]"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['KING: 243\\n',\n"," 'LEAR: 236\\n',\n"," 'DRAMATIS: 1\\n',\n"," 'PERSONAE: 1\\n',\n"," 'king: 65\\n',\n"," 'of: 447\\n',\n"," 'Britain: 2\\n',\n"," 'OF: 15\\n',\n"," 'FRANCE: 10\\n',\n"," 'DUKE: 3\\n']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"eUc2xO_PFQCH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e856e484-97b6-402a-db5e-52eee53a813b","executionInfo":{"status":"ok","timestamp":1591057058147,"user_tz":-480,"elapsed":962,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["print(\"How many words:\", len(data))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["How many words: 4784\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RbdREavlFrc3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e9291eda-0e37-45a8-ecf0-edf46db02f65","executionInfo":{"status":"ok","timestamp":1591057061138,"user_tz":-480,"elapsed":938,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["%%writefile wordcount.py\n","\n","# this is main logic for word count with beam.\n","from __future__ import absolute_import\n","\n","import argparse\n","import logging\n","import re\n","\n","from past.builtins import unicode\n","\n","import apache_beam as beam\n","from apache_beam.io import ReadFromText\n","from apache_beam.io import WriteToText\n","from apache_beam.metrics import Metrics\n","from apache_beam.metrics.metric import MetricsFilter\n","from apache_beam.options.pipeline_options import PipelineOptions\n","from apache_beam.options.pipeline_options import SetupOptions\n","\n","\n","class WordExtractingDoFn(beam.DoFn):\n","  \"\"\"Parse each line of input text into words.\"\"\"\n","  def __init__(self):\n","    # TODO(BEAM-6158): Revert the workaround once we can pickle super() on py3.\n","    # super(WordExtractingDoFn, self).__init__()\n","    beam.DoFn.__init__(self)\n","    self.words_counter = Metrics.counter(self.__class__, 'words')\n","    self.word_lengths_counter = Metrics.counter(self.__class__, 'word_lengths')\n","    self.word_lengths_dist = Metrics.distribution(\n","        self.__class__, 'word_len_dist')\n","    self.empty_line_counter = Metrics.counter(self.__class__, 'empty_lines')\n","\n","  def process(self, element):\n","    \"\"\"Returns an iterator over the words of this element.\n","\n","    The element is a line of text.  If the line is blank, note that, too.\n","\n","    Args:\n","      element: the element being processed\n","\n","    Returns:\n","      The processed element.\n","    \"\"\"\n","    text_line = element.strip()\n","    if not text_line:\n","      self.empty_line_counter.inc(1)\n","    words = re.findall(r'[\\w\\']+', text_line, re.UNICODE)\n","    for w in words:\n","      self.words_counter.inc()\n","      self.word_lengths_counter.inc(len(w))\n","      self.word_lengths_dist.update(len(w))\n","    return words\n","\n","\n","def run(argv=None, save_main_session=True):\n","  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n","  parser = argparse.ArgumentParser()\n","  parser.add_argument(\n","      '--input',\n","      dest='input',\n","      default='gs://dataflow-samples/shakespeare/kinglear.txt',\n","      help='Input file to process.')\n","  parser.add_argument(\n","      '--output',\n","      dest='output',\n","      required=True,\n","      help='Output file to write results to.')\n","  known_args, pipeline_args = parser.parse_known_args(argv)\n","\n","  # We use the save_main_session option because one or more DoFn's in this\n","  # workflow rely on global context (e.g., a module imported at module level).\n","  pipeline_options = PipelineOptions(pipeline_args)\n","  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n","  p = beam.Pipeline(options=pipeline_options)\n","\n","  # Read the text file[pattern] into a PCollection.\n","  lines = p | 'read' >> ReadFromText(known_args.input)\n","\n","  # Count the occurrences of each word.\n","  def count_ones(word_ones):\n","    (word, ones) = word_ones\n","    return (word, sum(ones))\n","\n","  # Here is where to could define our processing logic.\n","  counts = (\n","      lines\n","      | 'split' >>\n","      (beam.ParDo(WordExtractingDoFn()).with_output_types(unicode))\n","      | 'lowercase' >> beam.Map(lambda x: str(x).lower())\n","      | 'pair_with_one' >> beam.Map(lambda x: (x, 1))\n","      | 'group' >> beam.GroupByKey()\n","      | 'count' >> beam.Map(count_ones))\n","\n","  # Format the counts into a PCollection of strings.\n","  def format_result(word_count):\n","    (word, count) = word_count\n","    return '%s: %d' % (word, count)\n","\n","  output = counts | 'format' >> beam.Map(format_result)\n","\n","  # Write the output using a \"Write\" transform that has side effects.\n","  # pylint: disable=expression-not-assigned\n","  output | 'write' >> WriteToText(known_args.output)\n","\n","  result = p.run()\n","  result.wait_until_finish()\n","\n","  # Do not query metrics when creating a template which doesn't run\n","  if (not hasattr(result, 'has_job')  # direct runner\n","      or result.has_job):  # not just a template creation\n","    empty_lines_filter = MetricsFilter().with_name('empty_lines')\n","    query_result = result.metrics().query(empty_lines_filter)\n","    if query_result['counters']:\n","      empty_lines_counter = query_result['counters'][0]\n","      logging.info('number of empty lines: %d', empty_lines_counter.result)\n","\n","    word_lengths_filter = MetricsFilter().with_name('word_len_dist')\n","    query_result = result.metrics().query(word_lengths_filter)\n","    if query_result['distributions']:\n","      word_lengths_dist = query_result['distributions'][0]\n","      logging.info('average word length: %d', word_lengths_dist.result.mean)\n","\n","if __name__ == '__main__':\n","  logging.getLogger().setLevel(logging.INFO)\n","  run()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Writing wordcount.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sGOhLcHLF69u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":566},"outputId":"0a488842-9e96-49be-bda5-b09c735c46fa","executionInfo":{"status":"ok","timestamp":1591057069775,"user_tz":-480,"elapsed":5901,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# so that we could start our training file. Here I change each words into small case. \n","# we could try again the logic locally.\n","! python wordcount.py --output  outputs_new\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","WARNING:apache_beam.internal.gcp.auth:Unable to find default credentials to use: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.\n","Connecting anonymously.\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f300f945a60> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f300f945b70> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f300f945bf8> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f300f945c80> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f300f945d08> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f300f945e18> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f300f945ea0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f300f945f28> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f300f946048> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f300f946268> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f300f9461e0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f300f9462f0> ====================\n","INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n","INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f300f365198> for environment urn: \"beam:env:embedded_python:v1\"\n","\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_read/Read/_SDFBoundedSourceWrapper/Impulse_5)+(read/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(read/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_PCollection_PCollection_1_split/Read)+(read/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_split_7))+(ref_AppliedPTransform_lowercase_8))+(ref_AppliedPTransform_pair_with_one_9))+(group/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Impulse_20)+(ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2706>)_21))+(ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Map(decode)_23))+(ref_AppliedPTransform_write/Write/WriteImpl/InitializeWrite_24))+(ref_PCollection_PCollection_13/Write))+(ref_PCollection_PCollection_14/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((group/Read)+(ref_AppliedPTransform_count_14))+(ref_AppliedPTransform_format_15))+(ref_AppliedPTransform_write/Write/WriteImpl/WriteBundles_25))+(ref_AppliedPTransform_write/Write/WriteImpl/Pair_26))+(ref_AppliedPTransform_write/Write/WriteImpl/WindowInto(WindowIntoFn)_27))+(write/Write/WriteImpl/GroupByKey/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_write/Write/WriteImpl/Extract_32))+(ref_PCollection_PCollection_21/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_13/Read)+(ref_AppliedPTransform_write/Write/WriteImpl/PreFinalize_33))+(ref_PCollection_PCollection_22/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_13/Read)+(ref_AppliedPTransform_write/Write/WriteImpl/FinalizeWrite_34)\n","INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n","INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n","INFO:root:number of empty lines: 1663\n","INFO:root:average word length: 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hby3C1tfGTKb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"72ba5037-3bb8-4ff6-d80c-5bc4ee2433bb","executionInfo":{"status":"ok","timestamp":1591057074549,"user_tz":-480,"elapsed":606,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# let's get some samples for show case\n","file_name = [x for x in os.listdir('.') if x.startswith('outputs_new')][0]\n","with open(file_name) as f:\n","  data_new = f.readlines()\n","\n","# as we could see that result is a file with each word count.\n","# we do find that for each word is lower case.\n","data_new[:10]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['king: 311\\n',\n"," 'lear: 253\\n',\n"," 'dramatis: 1\\n',\n"," 'personae: 1\\n',\n"," 'of: 483\\n',\n"," 'britain: 2\\n',\n"," 'france: 32\\n',\n"," 'duke: 26\\n',\n"," 'burgundy: 20\\n',\n"," 'cornwall: 75\\n']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"hzK2ITK0HaXU","colab_type":"text"},"source":["## Upload local logic with GCP dataflow\n","\n","As we have already tested with logic that we created, then we could do the logic in dataflow, but I have to say that for cloud run with so smaller file, do take more time than local, but if we have much data will be fine. \n","\n","As for dataflow is serverless in cloud, we could use how much resource as we need."]},{"cell_type":"code","metadata":{"id":"ehgMXH24HThg","colab_type":"code","colab":{}},"source":["# configuration of the project, but we also need to create a bucket first.\n","PROJECT_ID = \"cloudtutorial-279003\"\n","BUCKET = \"dataflow_tutorial_bucket\"\n","REGION = 'us-central1'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNauRMymIcYN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"a40eb563-1aef-462f-e56f-67731a95e1b3","executionInfo":{"status":"ok","timestamp":1591057092988,"user_tz":-480,"elapsed":740,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# after we have already created the bucket, but dataflow will use data in bucket, so\n","# let's make a file in the bucket.\n","text = \"\"\"\n","Prepare input for prediction\n","To receive valid and useful predictions, you must preprocess input for prediction in the same way that training data was preprocessed. In a production system, you may want to create a preprocessing pipeline that can be used identically at training time and prediction time.\n","\n","For this exercise, use the training package's data-loading code to select a random sample from the evaluation data. This data is in the form that was used to evaluate accuracy after each epoch of training, so it can be used to send test predictions without further preprocessing.\n","\n","Open the Python interpreter (python) from your current working directory in order to run the next several snippets of code:\n","\"\"\"\n","\n","file_name = 'sample.txt'\n","\n","with open(file_name, 'w') as f:\n","  f.write(text)\n","\n","print(\"Current folder file list:\", os.listdir('.'))\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Current folder file list: ['.config', 'outputs-00000-of-00001', 'wordcount.py', 'outputs_new-00000-of-00001', 'sample.txt', 'CloudTutorial-e8d69ceedc4d.json', 'sample_data']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EfdJX5lyKesk","colab_type":"code","colab":{}},"source":["# before we do anything with client, we have to set the credentials\n","credentials_file = [x for x in os.listdir('.') if x.endswith('json')][0]\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_file"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"STIsLQ6bIr6X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cb40786d-4147-4444-ed39-93c6d3c1d9bc","executionInfo":{"status":"ok","timestamp":1591057737590,"user_tz":-480,"elapsed":890,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# then let's upload the file into our bucket\n","from google.cloud import storage\n","\n","client = storage.Client(project=PROJECT_ID)\n","\n","bucket = client.bucket(BUCKET)\n","\n","# where to store our file\n","des_file = 'data_flow_inputs/{}'.format(file_name)\n","\n","# let's upload\n","blob = bucket.blob(des_file)\n","try:\n","  blob.upload_from_filename(file_name, des_file)\n","  print('We have already uploaded in the file into bucket')\n","except Exception as e:\n","  raise Exception(\"When upload file with error. \")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["We have already uploaded in the file into bucket\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HmdeXwO7JupV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"643e8303-f5e1-47fa-d08d-4f21c4caa585","executionInfo":{"status":"ok","timestamp":1591058125164,"user_tz":-480,"elapsed":371090,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# as we already have the data file in bucket, so that we could start our \n","# processing logic in cloud\n","! python -m apache_beam.examples.wordcount \\\n","--region $REGION \\\n","--input gs://$BUCKET/data_flow_inputs/sample.txt \\\n","--output gs://$BUCKET/data_flow_output/outputs \\\n","--runner DataflowRunner \\\n","--project $PROJECT_ID \\\n","--temp_location gs://$BUCKET/tmp"],"execution_count":24,"outputs":[{"output_type":"stream","text":["INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n","INFO:oauth2client.client:Refreshing access_token\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.6 interpreter.\n","INFO:root:Using Python SDK docker image: apache/beam_python3.6_sdk:2.21.0. If the image is not available at local, we will try to pull from hub.docker.com\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://dataflow_tutorial_bucket/tmp\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://dataflow_tutorial_bucket/tmp/beamapp-root-0602002917-401233.1591057757.402644/pipeline.pb...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://dataflow_tutorial_bucket/tmp/beamapp-root-0602002917-401233.1591057757.402644/pipeline.pb in 0 seconds.\n","INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpgw6h1w8g', 'apache-beam==2.21.0', '--no-deps', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n","INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpgw6h1w8g', 'apache-beam==2.21.0', '--no-deps', '--only-binary', ':all:', '--python-version', '36', '--implementation', 'cp', '--abi', 'cp36m', '--platform', 'manylinux1_x86_64']\n","INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.21.0-cp36-cp36m-manylinux1_x86_64.whl\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://dataflow_tutorial_bucket/tmp/beamapp-root-0602002917-401233.1591057757.402644/pickled_main_session...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://dataflow_tutorial_bucket/tmp/beamapp-root-0602002917-401233.1591057757.402644/pickled_main_session in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://dataflow_tutorial_bucket/tmp/beamapp-root-0602002917-401233.1591057757.402644/dataflow_python_sdk.tar...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://dataflow_tutorial_bucket/tmp/beamapp-root-0602002917-401233.1591057757.402644/dataflow_python_sdk.tar in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://dataflow_tutorial_bucket/tmp/beamapp-root-0602002917-401233.1591057757.402644/apache_beam-2.21.0-cp36-cp36m-manylinux1_x86_64.whl...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://dataflow_tutorial_bucket/tmp/beamapp-root-0602002917-401233.1591057757.402644/apache_beam-2.21.0-cp36-cp36m-manylinux1_x86_64.whl in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n"," createTime: '2020-06-02T00:29:33.048470Z'\n"," currentStateTime: '1970-01-01T00:00:00Z'\n"," id: '2020-06-01_17_29_31-9732832422105468746'\n"," location: 'us-central1'\n"," name: 'beamapp-root-0602002917-401233'\n"," projectId: 'cloudtutorial-279003'\n"," stageStates: []\n"," startTime: '2020-06-02T00:29:33.048470Z'\n"," steps: []\n"," tempFiles: []\n"," type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-06-01_17_29_31-9732832422105468746]\n","INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-06-01_17_29_31-9732832422105468746?project=cloudtutorial-279003\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-06-01_17_29_31-9732832422105468746 is in state JOB_STATE_PENDING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:31.830Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-06-01_17_29_31-9732832422105468746. The number of workers will be between 1 and 1000.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:31.830Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-06-01_17_29_31-9732832422105468746.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.013Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-a.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.675Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.704Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step write/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.735Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step group: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.771Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.804Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.891Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.942Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.972Z: JOB_MESSAGE_DETAILED: Fusing consumer split into read/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:36.993Z: JOB_MESSAGE_DETAILED: Fusing consumer pair_with_one into split\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.023Z: JOB_MESSAGE_DETAILED: Fusing consumer group/Reify into pair_with_one\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.054Z: JOB_MESSAGE_DETAILED: Fusing consumer group/Write into group/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.082Z: JOB_MESSAGE_DETAILED: Fusing consumer group/GroupByWindow into group/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.106Z: JOB_MESSAGE_DETAILED: Fusing consumer count into group/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.133Z: JOB_MESSAGE_DETAILED: Fusing consumer format into count\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.162Z: JOB_MESSAGE_DETAILED: Fusing consumer write/Write/WriteImpl/WriteBundles/WriteBundles into format\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.189Z: JOB_MESSAGE_DETAILED: Fusing consumer write/Write/WriteImpl/Pair into write/Write/WriteImpl/WriteBundles/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.217Z: JOB_MESSAGE_DETAILED: Fusing consumer write/Write/WriteImpl/WindowInto(WindowIntoFn) into write/Write/WriteImpl/Pair\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.247Z: JOB_MESSAGE_DETAILED: Fusing consumer write/Write/WriteImpl/GroupByKey/Reify into write/Write/WriteImpl/WindowInto(WindowIntoFn)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.276Z: JOB_MESSAGE_DETAILED: Fusing consumer write/Write/WriteImpl/GroupByKey/Write into write/Write/WriteImpl/GroupByKey/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.309Z: JOB_MESSAGE_DETAILED: Fusing consumer write/Write/WriteImpl/GroupByKey/GroupByWindow into write/Write/WriteImpl/GroupByKey/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.343Z: JOB_MESSAGE_DETAILED: Fusing consumer write/Write/WriteImpl/Extract into write/Write/WriteImpl/GroupByKey/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.379Z: JOB_MESSAGE_DETAILED: Fusing consumer write/Write/WriteImpl/InitializeWrite into write/Write/WriteImpl/DoOnce/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.405Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.439Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.463Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.485Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.623Z: JOB_MESSAGE_DEBUG: Executing wait step start26\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.675Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/DoOnce/Read+write/Write/WriteImpl/InitializeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.706Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/GroupByKey/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.715Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.743Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-a...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.744Z: JOB_MESSAGE_BASIC: Executing operation group/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.804Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/GroupByKey/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.804Z: JOB_MESSAGE_BASIC: Finished operation group/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.854Z: JOB_MESSAGE_DEBUG: Value \"group/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.881Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/GroupByKey/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:29:37.916Z: JOB_MESSAGE_BASIC: Executing operation read/Read+split+pair_with_one+group/Reify+group/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-06-01_17_29_31-9732832422105468746 is in state JOB_STATE_RUNNING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:30:01.709Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:31:44.600Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:31:44.639Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.192Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/DoOnce/Read+write/Write/WriteImpl/InitializeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.331Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/DoOnce/Read.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.365Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/InitializeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.442Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.480Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.518Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.523Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.550Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.580Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.593Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.638Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:42.690Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:55.618Z: JOB_MESSAGE_BASIC: Finished operation read/Read+split+pair_with_one+group/Reify+group/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:55.693Z: JOB_MESSAGE_BASIC: Executing operation group/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:55.750Z: JOB_MESSAGE_BASIC: Finished operation group/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:32:55.819Z: JOB_MESSAGE_BASIC: Executing operation group/Read+group/GroupByWindow+count+format+write/Write/WriteImpl/WriteBundles/WriteBundles+write/Write/WriteImpl/Pair+write/Write/WriteImpl/WindowInto(WindowIntoFn)+write/Write/WriteImpl/GroupByKey/Reify+write/Write/WriteImpl/GroupByKey/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:10.395Z: JOB_MESSAGE_BASIC: Finished operation group/Read+group/GroupByWindow+count+format+write/Write/WriteImpl/WriteBundles/WriteBundles+write/Write/WriteImpl/Pair+write/Write/WriteImpl/WindowInto(WindowIntoFn)+write/Write/WriteImpl/GroupByKey/Reify+write/Write/WriteImpl/GroupByKey/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:10.466Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/GroupByKey/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:10.522Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/GroupByKey/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:10.647Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/GroupByKey/Read+write/Write/WriteImpl/GroupByKey/GroupByWindow+write/Write/WriteImpl/Extract\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.318Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/GroupByKey/Read+write/Write/WriteImpl/GroupByKey/GroupByWindow+write/Write/WriteImpl/Extract\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.395Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/Extract.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.458Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.503Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.524Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.579Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.582Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.647Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:21.714Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/PreFinalize/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:26.332Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/PreFinalize/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:26.399Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/PreFinalize.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:26.479Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:26.533Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:26.591Z: JOB_MESSAGE_DEBUG: Value \"write/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:26.666Z: JOB_MESSAGE_BASIC: Executing operation write/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:31.100Z: JOB_MESSAGE_BASIC: Finished operation write/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:31.177Z: JOB_MESSAGE_DEBUG: Executing success step success24\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:31.321Z: JOB_MESSAGE_DETAILED: Cleaning up.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:31.382Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:33:31.416Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:35:09.507Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:35:09.549Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2020-06-02T00:35:09.580Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-06-01_17_29_31-9732832422105468746 is in state JOB_STATE_DONE\n","INFO:root:number of empty lines: 3\n","INFO:root:average word length: 5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XUz06h_waNz6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"058b3a4c-e154-4b8e-cbdc-d5c111ffa2fd","executionInfo":{"status":"ok","timestamp":1591058703043,"user_tz":-480,"elapsed":2060,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# after we have already finished the job, so that we could check that we do have output\n","# the result into our bucket\n","output_folder = 'data_flow_output'\n","\n","def list_bucket_files(prefix):\n","  blobs = client.list_blobs(BUCKET, prefix=prefix)\n","\n","  print('Get file list:')\n","  for blob in blobs:\n","    print(blob.name, end='\\t')\n","  print()\n","\n","# so that we could find we do get files in bucket.\n","list_bucket_files(output_folder)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Get file list:\n","data_flow_output/outputs-00000-of-00003\tdata_flow_output/outputs-00001-of-00003\tdata_flow_output/outputs-00002-of-00003\t\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GHqUVsa3gAUF","colab_type":"code","colab":{}},"source":["# if we want to use command line to do some query, we have to auth the lab first\n","from google.colab import auth\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hR1pJRiKle1O","colab_type":"text"},"source":["## Check result\n","\n","As we have already finished the whole step, let's check our reuslt"]},{"cell_type":"code","metadata":{"id":"vdMUYRs2afGn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"13e5bf67-0ea9-48db-84c7-13e6b6a1e5b2","executionInfo":{"status":"ok","timestamp":1591059149960,"user_tz":-480,"elapsed":4777,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# let get some contents of these files.\n","! gsutil cat gs://$BUCKET/data_flow_output/outputs*"],"execution_count":35,"outputs":[{"output_type":"stream","text":["preprocessed: 1\n","without: 1\n","predictions: 2\n","test: 1\n","may: 1\n","form: 1\n","order: 1\n","each: 1\n","data: 4\n","This: 1\n","so: 1\n","system: 1\n","used: 3\n","further: 1\n","way: 1\n","pipeline: 1\n","receive: 1\n","For: 1\n","python: 1\n","exercise: 1\n","of: 2\n","the: 6\n","To: 1\n","run: 1\n","next: 1\n","working: 1\n","epoch: 1\n","is: 1\n","same: 1\n","was: 2\n","be: 2\n","training: 4\n","time: 2\n","select: 1\n","input: 2\n","that: 3\n","use: 1\n","directory: 1\n","after: 1\n","preprocessing: 2\n","at: 1\n","to: 5\n","package's: 1\n","snippets: 1\n","can: 2\n","In: 1\n","loading: 1\n","it: 1\n","identically: 1\n","this: 1\n","evaluation: 1\n","your: 1\n","want: 1\n","accuracy: 1\n","code: 2\n","interpreter: 1\n","from: 2\n","production: 1\n","prediction: 3\n","valid: 1\n","Open: 1\n","current: 1\n","several: 1\n","in: 3\n","sample: 1\n","Prepare: 1\n","preprocess: 1\n","create: 1\n","and: 2\n","must: 1\n","evaluate: 1\n","send: 1\n","a: 3\n","random: 1\n","useful: 1\n","for: 2\n","you: 2\n","Python: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zgWoFqiufyu0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}