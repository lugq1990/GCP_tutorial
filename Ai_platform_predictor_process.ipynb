{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI-Platform predictor process.ipynb","provenance":[],"authorship_tag":"ABX9TyPo3O6SYYxSGQPuCBZK7xFC"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"EacmlubNM-JG","colab_type":"code","outputId":"97785281-a415-436f-e2aa-2a097d410f1c","executionInfo":{"status":"ok","timestamp":1590554172782,"user_tz":-480,"elapsed":5620,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":365}},"source":["!pip install tensorflow==1.12"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.8.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.8)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.18.4)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.10.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.34.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.2)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.9.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.29.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.3.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (46.3.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.2.2)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (1.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OMC8nzNZqWly","colab_type":"text"},"source":["## Import  libraries"]},{"cell_type":"code","metadata":{"id":"JvyPJPLSNHk1","colab_type":"code","outputId":"18f6b28a-d63c-4414-f870-4c5805b97fde","executionInfo":{"status":"ok","timestamp":1590554172783,"user_tz":-480,"elapsed":5606,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# import modules\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","import warnings\n","\n","warnings.simplefilter('ignore')\n","\n","print(\"Tensorflow Version:\", tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 1.12.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lEOQWbUkqb9j","colab_type":"text"},"source":["## Auth the coud"]},{"cell_type":"code","metadata":{"id":"Fxhb2R1MNZJn","colab_type":"code","colab":{}},"source":["# auth the could\n","from google.colab import auth\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FzHN8j91qkn8","colab_type":"text"},"source":["## Get training data\n","\n","This notebook is try to use DNN to the predition with stack overflow question dataset with tags. \n","\n","One thing to notice is that we just train the model in server side, but in AI-platform training service."]},{"cell_type":"code","metadata":{"id":"xlPd_vNuNr1i","colab_type":"code","outputId":"a685ad74-a6ac-47ba-ff22-ab24094c8d79","executionInfo":{"status":"ok","timestamp":1590554181428,"user_tz":-480,"elapsed":14233,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Download data with Stack overflow from google storage\n","!gsutil cp 'gs://cloudml-demo-lcm/SO_ml_tags_avocado_188k_v2.csv' ./\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Copying gs://cloudml-demo-lcm/SO_ml_tags_avocado_188k_v2.csv...\n","- [1 files][276.7 MiB/276.7 MiB]                                                \n","Operation completed over 1 objects/276.7 MiB.                                    \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A2HJsJdRN5hl","colab_type":"code","outputId":"0a38069e-3a2f-4248-d696-89a836857ccf","executionInfo":{"status":"ok","timestamp":1590554181429,"user_tz":-480,"elapsed":14225,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# show what we have in current server.\n","import os\n","\n","print(os.listdir('.'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['.config', 'preprocess.py', 'model_prediction.py', 'tutorial_pred.egg-info', 'setup.py', 'dist', 'adc.json', 'processor.pkl', 'SO_ml_tags_avocado_188k_v2.csv', '__pycache__', 'keras_model.h5', 'sample_data']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jq-tj2y2q-ro","colab_type":"text"},"source":["## Get data ready"]},{"cell_type":"code","metadata":{"id":"vF96FkrSOBcN","colab_type":"code","outputId":"c819a2b7-263d-4ce2-c5a2-a491aba44864","executionInfo":{"status":"ok","timestamp":1590554186078,"user_tz":-480,"elapsed":18859,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["# then we could use pandas to read data and remove the empty rows\n","df = pd.read_csv(\"SO_ml_tags_avocado_188k_v2.csv\", names=['tags', 'original_tags', 'text'], header=0)  # without header\n","df = df.drop(columns=['original_tags'])\n","df = df.dropna()\n","\n","df = shuffle(df, random_state=1234)\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tags</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>63842</th>\n","      <td>pandas</td>\n","      <td>avocado does not read values from excel cells ...</td>\n","    </tr>\n","    <tr>\n","      <th>63486</th>\n","      <td>pandas</td>\n","      <td>optimizing iteration over avocado dataframe i ...</td>\n","    </tr>\n","    <tr>\n","      <th>105751</th>\n","      <td>matplotlib</td>\n","      <td>erase previously drawn content from a pyplot d...</td>\n","    </tr>\n","    <tr>\n","      <th>70621</th>\n","      <td>pandas</td>\n","      <td>sort/alphabetzing columns in dataframes (avoca...</td>\n","    </tr>\n","    <tr>\n","      <th>161865</th>\n","      <td>tensorflow</td>\n","      <td>error in importing niftynet with avocado 1.9 i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              tags                                               text\n","63842       pandas  avocado does not read values from excel cells ...\n","63486       pandas  optimizing iteration over avocado dataframe i ...\n","105751  matplotlib  erase previously drawn content from a pyplot d...\n","70621       pandas  sort/alphabetzing columns in dataframes (avoca...\n","161865  tensorflow  error in importing niftynet with avocado 1.9 i..."]},"metadata":{"tags":[]},"execution_count":129}]},{"cell_type":"markdown","metadata":{"id":"5h_szjmTrCp8","colab_type":"text"},"source":["## Explore data"]},{"cell_type":"code","metadata":{"id":"viqSnRjIOcPa","colab_type":"code","outputId":"c0e42f53-f28c-488b-9a10-888e668660a4","executionInfo":{"status":"ok","timestamp":1590554186080,"user_tz":-480,"elapsed":18849,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":276}},"source":["# how many unique lables that we have\n","print(\"how many:\", len(df))\n","print(\"unique label numbers:\", len(np.unique(df['tags'])))\n","print(\"unique label:\", np.unique(df['tags']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["how many: 188199\n","unique label numbers: 33\n","unique label: ['keras' 'keras,scikitlearn' 'keras,tensorflow' 'matplotlib'\n"," 'matplotlib,keras' 'matplotlib,keras,scikitlearn' 'matplotlib,pandas'\n"," 'matplotlib,pandas,scikitlearn' 'matplotlib,scikitlearn'\n"," 'matplotlib,tensorflow' 'matplotlib,tensorflow,keras' 'pandas'\n"," 'pandas,keras' 'pandas,matplotlib' 'pandas,matplotlib,keras'\n"," 'pandas,matplotlib,scikitlearn' 'pandas,scikitlearn'\n"," 'pandas,scikitlearn,keras' 'pandas,tensorflow' 'pandas,tensorflow,keras'\n"," 'pandas,tensorflow,scikitlearn' 'scikitlearn' 'scikitlearn,keras'\n"," 'scikitlearn,pandas' 'scikitlearn,tensorflow'\n"," 'scikitlearn,tensorflow,keras' 'tensorflow' 'tensorflow,keras'\n"," 'tensorflow,keras,scikitlearn' 'tensorflow,matplotlib'\n"," 'tensorflow,matplotlib,keras' 'tensorflow,scikitlearn'\n"," 'tensorflow,scikitlearn,keras']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-qCh-qfLOrez","colab_type":"code","outputId":"e23ccb5e-a215-41bf-8446-b125c74f055d","executionInfo":{"status":"ok","timestamp":1590554186080,"user_tz":-480,"elapsed":18839,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# but we do find that some tags are subset of high level code, as there are the same in each domain\n","tags_split = [tags.split(',')[0] for tags in df['tags'].values]\n","print(tags_split[:10])\n","print(\"unique labels number:\", len(np.unique(tags_split)))\n","print(\"unique labels :\", np.unique(tags_split))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['pandas', 'pandas', 'matplotlib', 'pandas', 'tensorflow', 'matplotlib', 'pandas', 'pandas', 'pandas', 'pandas']\n","unique labels number: 5\n","unique labels : ['keras' 'matplotlib' 'pandas' 'scikitlearn' 'tensorflow']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OcYkaIj2ROVN","colab_type":"text"},"source":["## Transform label data\n","\n","So that we just need to make a 5 classes model would be fine. In fact, we couldn't just put the whole data into the model as there are strings, what the model accepted is number! So we need to process string into numbers...\n","\n","Before we do that, we also need to conver the string label into numbers, but with sklearn is really easy."]},{"cell_type":"code","metadata":{"id":"be-gUpQUPJlc","colab_type":"code","outputId":"c8009b3e-d0cd-4c7e-b958-96b17cae36c2","executionInfo":{"status":"ok","timestamp":1590554186807,"user_tz":-480,"elapsed":19554,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["encoder = LabelEncoder()\n","tags_encoded = encoder.fit_transform(tags_split)\n","\n","num_tags = len(encoder.classes_)\n","\n","print(df['text'].values[0])\n","print(encoder.classes_)\n","print(\"Class name: `{}` with label: `{}`\".format(df['tags'].values[0], tags_encoded[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["avocado does not read values from excel cells with such simple formula as =10+10 i've got a few excel sheets with columns with different data types. some of them consist of formulas as well. avocado does not read values from excel cells with such simple formula as =10+10 or =250+30+40  code like this   truck_work = avocado.read_excel(hauls_monthly_data, sheetname=truck)   returns dataframe where column filled from those excel cells consists data, which type is float and value is nan. but i'm waiting for float with value 10 and 320  the only way i've worked out yet to solve this issue is by manually saving each time an excel-file before processing data from it. which is not much pythonic way of dealing with problems.  if i'm using such code as   wb = load_workbook(filename = hauls_monthly_data) sheet_names = wb.get_sheet_names() name = sheet_names[1] sheet_ranges = wb[name] truck_work = avocado.dataframe(sheet_ranges.values)   then it returns   8          =16+109+108        =6+40+29                none      any help be very appreciated.\n","['keras' 'matplotlib' 'pandas' 'scikitlearn' 'tensorflow']\n","Class name: `pandas` with label: `2`\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bE7rVbfkTtmJ","colab_type":"code","outputId":"d69dcd79-2589-4052-961c-7cefdda7934e","executionInfo":{"status":"ok","timestamp":1590554186808,"user_tz":-480,"elapsed":19542,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# As we already shuffle our data, so we could just get the training size\n","train_size = int(len(df) * .8)\n","print('Train size: ', train_size)\n","print(\"test size:\", len(df) - train_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train size:  150559\n","test size: 37640\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kabk4bkqTC0X","colab_type":"code","colab":{}},"source":["# get train and validation label\n","train_tags = tags_encoded[:train_size]\n","test_tags = tags_encoded[train_size:]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jK5MbhprrLUC","colab_type":"text"},"source":["## Processor class\n","\n","we have to process the text into numbers, but here we should do one thing that shoudld be noticed is that when we do prediction, we should follow with same process, so the best way is to create a class to do that. One more thing, as currently we do the training in notebook, but when do predition\n","\n","we would do the in python code, so we could write the logic into a python file."]},{"cell_type":"code","metadata":{"id":"2q8CAdKkURm1","colab_type":"code","outputId":"fb8598bb-7f01-456b-9fc0-0474023b2fcd","executionInfo":{"status":"ok","timestamp":1590554186810,"user_tz":-480,"elapsed":19518,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","%%writefile preprocess.py\n","\n","from tensorflow.keras.preprocessing import text\n","\n","class TextProcessor:\n","  def __init__(self, vocab_size):\n","    self._vocab_size = vocab_size\n","    self._tokenizer = None\n","\n","  def create_tokens(self, text_list):\n","    # this is to conver text into vector, but how to convert text into vector? with Tokenizer\n","    # will try to only keep most frequent `num_words` words with lowercase based on\n","    # binary, or word count, or tf-id. Currently with binarizer.\n","    # could be found: https://keras.io/api/preprocessing/text/\n","    tokenizer = text.Tokenizer(num_words=self._vocab_size)\n","    # then we could update internal vocabulary: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts\n","    tokenizer.fit_on_texts(text_list)\n","    self._tokenizer = tokenizer\n","\n","  def transform_text(self, text_list):\n","    # then we could convert the text into numpy matrix.\n","    text_matrix = self._tokenizer.texts_to_matrix(text_list)\n","    return text_matrix"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting preprocess.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kCUfDjmyWiNu","colab_type":"code","colab":{}},"source":["# process whole text data, this does takes some time as we process whole data\n","from preprocess import TextProcessor\n","\n","VOCAB_SIZE = 500\n","\n","train_data = df['text'].values[:train_size]\n","test_data = df['text'].values[train_size:]\n","\n","processor = TextProcessor(VOCAB_SIZE)\n","\n","processor.create_tokens(train_data)\n","\n","train_data = processor.transform_text(train_data)\n","test_data = processor.transform_text(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hb28pngVXBHa","colab_type":"code","outputId":"14c771a8-7c32-45e9-f11c-6e79eda5ffe5","executionInfo":{"status":"ok","timestamp":1590554258591,"user_tz":-480,"elapsed":91276,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":414}},"source":["# get some sample data\n","print(\"How many dimensions: \", len(train_data[0]))\n","print(\"****\")\n","print(train_data[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["How many dimensions:  500\n","****\n","[0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0.\n"," 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n"," 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n"," 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ClT5NxJwXSnd","colab_type":"code","colab":{}},"source":["# we could save the processed object into disk for later usecase\n","import pickle\n","\n","with open('./processor.pkl', 'wb') as f:\n","  pickle.dump(processor, f)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWjZ3KUTrWk1","colab_type":"text"},"source":["## Define model and do traning logic"]},{"cell_type":"code","metadata":{"id":"_79Ucp5PXhnw","colab_type":"code","colab":{}},"source":["# then we could start our model build logic\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","\n","def create_model(vocab_size, num_tags):\n","  model = Sequential()\n","  model.add(Dense(128, input_shape=(vocab_size, ), activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(.5))\n","  model.add(Dense(64, activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(.5))\n","  model.add(Dense(num_tags, activation='softmax'))\n","  \n","  model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHKxtbrKYOpp","colab_type":"code","outputId":"07fe22cd-91ac-453b-db51-f685eac9b057","executionInfo":{"status":"ok","timestamp":1590554260232,"user_tz":-480,"elapsed":92881,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":380}},"source":["model = create_model(VOCAB_SIZE, num_tags)\n","\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_6 (Dense)              (None, 128)               64128     \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 128)               512       \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 64)                8256      \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 64)                256       \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 5)                 325       \n","=================================================================\n","Total params: 73,477\n","Trainable params: 73,093\n","Non-trainable params: 384\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cJgQNkO7Y2WG","colab_type":"code","outputId":"703da655-4c4e-48df-cb15-c9ba0505abef","executionInfo":{"status":"ok","timestamp":1590554296879,"user_tz":-480,"elapsed":129517,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":155}},"source":["# Now that we could start our training logic\n","his = model.fit(train_data, train_tags, epochs=3, batch_size=128)\n","eval_res = model.evaluate(test_data, test_tags)\n","print('Evaluation loss: {}, accuracy:{}'.format(eval_res[0], eval_res[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","150559/150559 [==============================] - 13s 89us/step - loss: 0.4983 - acc: 0.8283\n","Epoch 2/3\n","150559/150559 [==============================] - 10s 66us/step - loss: 0.3252 - acc: 0.8818\n","Epoch 3/3\n","150559/150559 [==============================] - 10s 64us/step - loss: 0.3024 - acc: 0.8882\n","37640/37640 [==============================] - 3s 82us/step\n","Evaluation loss: 0.26055393286294815, accuracy:0.8972635494155154\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-JAd5mSnxmnl","colab_type":"code","colab":{}},"source":["# After we have trained our model, we could save the trained model into disk\n","model.save(\"keras_model.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKudPN771T0i","colab_type":"code","colab":{}},"source":["os.remove('model_prediction.py')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5cTlPv6Frbue","colab_type":"text"},"source":["## Write prediction logic class"]},{"cell_type":"code","metadata":{"id":"Dx18qQt0Zjcs","colab_type":"code","outputId":"6db99755-3950-4f07-c315-8b928442075f","executionInfo":{"status":"ok","timestamp":1590557125496,"user_tz":-480,"elapsed":905,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Here we should write another class to load the processor object and trained model object\n","%%writefile model_prediction.py\n","import pickle\n","import os\n","import numpy as np\n","\n","class ModelPredictor:\n","  def __init__(self, model, processor):\n","    self._model = model\n","    self._processor = processor\n","\n","  def predict(self, instances, **kwargs):\n","    processed_data = self._processor.transform_text(instances)\n","    predictions = self._model.predict(processed_data)\n","    return predictions.tolist()    # we have to make the array data into a list, as array is not json serializable\n","\n","  @classmethod\n","  def from_path(cls, model_dir):\n","    import tensorflow as tf\n","    model = tf.keras.models.load_model(os.path.join(model_dir, 'keras_model.h5'))\n","    \n","    with open(os.path.join(model_dir, 'processor.pkl'), 'rb') as f:\n","      processor = pickle.load(f)\n","    \n","    return cls(model, processor)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting model_prediction.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KB6-qxq0bMvy","colab_type":"code","colab":{}},"source":["test_requests = [\n","  \"How to preprocess strings in Keras models Lambda layer? I have the problem that the value passed on to the Lambda layer (at compile time) is a placeholder generated by keras (without values). When the model is compiled, the .eval () method throws the error: You must feed a value for placeholder tensor 'input_1' with dtype string and shape [?, 1] def text_preprocess(x): strings = tf.keras.backend.eval(x) vectors = [] for string in strings: vector = string_to_one_hot(string.decode('utf-8')) vectors.append(vector) vectorTensor = tf.constant(np.array(vectors),dtype=tf.float32) return vectorTensor input_text = Input(shape=(1,), dtype=tf.string) embedding = Lambda(text_preprocess)(input_text) dense = Dense(256, activation='relu')(embedding) outputs = Dense(2, activation='softmax')(dense) model = Model(inputs=[input_text], outputs=outputs) model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) model.summary() model.save('test.h5') If I pass a string array into the input layer statically, I can compile the model, but I get the same error if I want to convert the model to tflite. #I replaced this line: input_text = Input(shape=(1,), dtype=tf.string) #by this lines: test = tf.constant(['Hello', 'World']) input_text = Input(shape=(1,), dtype=tf.string, tensor=test) #but calling this ... converter = TFLiteConverter.from_keras_model_file('string_test.h5') tfmodel = converter.convert() #... still leads to this error: InvalidArgumentError: You must feed a value for placeholder tensor 'input_3' with dtype string and shape [2] [[{{node input_3}}]] \",\n","  \"Change the bar item name in Pandas I have a test excel file like: df = pd.DataFrame({'name':list('abcdefg'), 'age':[10,20,5,23,58,4,6]}) print (df) name  age 0    a   10 1    b   20 2    c    5 3    d   23 4    e   58 5    f    4 6    g    6 I use Pandas and matplotlib to read and plot it: import pandas as pd import numpy as np import matplotlib.pyplot as plt import os excel_file = 'test.xlsx' df = pd.read_excel(excel_file, sheet_name=0) df.plot(kind='bar') plt.show() the result shows: enter image description here it use index number as item name, how can I change it to the name, which stored in column name?\"\n","]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OmD6XhWhrhoe","colab_type":"text"},"source":["## Make sample prediction in local server"]},{"cell_type":"code","metadata":{"id":"VK9rpf7fxIFb","colab_type":"code","outputId":"a2e863cf-152f-4398-d850-a20a83044388","executionInfo":{"status":"ok","timestamp":1590557140217,"user_tz":-480,"elapsed":11964,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from model_prediction import ModelPredictor\n","\n","classifier = ModelPredictor.from_path('.')\n","pred = classifier.predict(test_requests)\n","\n","for i in range(len(test_requests)):\n","  pred_labels = encoder.classes_[np.argmax(pred[i])]\n","  print(\"Sample: {} as {}... with prediction label: {}\".format(i, test_requests[i][:20], pred_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sample: 0 as How to preprocess st... with prediction label: keras\n","Sample: 1 as Change the bar item ... with prediction label: pandas\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DplyroKL61jw","colab_type":"text"},"source":["## Deploy our trained model into AI platform\n","\n","As we have already trained our model, then we could server our model into the cloud server. Currently with Google cloud, we would use container as service, then most commonly way is to wrap our source code into a package."]},{"cell_type":"code","metadata":{"id":"x5B-oDT1yMPc","colab_type":"code","outputId":"0878d490-efbd-4e1b-920f-0bd954b1eab7","executionInfo":{"status":"ok","timestamp":1590557145153,"user_tz":-480,"elapsed":874,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile setup.py\n","\n","from setuptools import setup\n","\n","setup(name='tutorial_pred', \n","      version='0.1', \n","      include_package_data=True, \n","      scripts=['preprocess.py', 'model_prediction.py'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting setup.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xil689v59sv7","colab_type":"code","outputId":"5b3d8a3d-5dcd-47a7-f877-0e2e5f53f404","executionInfo":{"status":"ok","timestamp":1590554313343,"user_tz":-480,"elapsed":145894,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":365}},"source":["! pip install google-cloud-storage"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (1.18.1)\n","Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage) (0.4.1)\n","Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage) (1.0.3)\n","Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage) (1.7.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from google-resumable-media<0.5.0dev,>=0.3.1->google-cloud-storage) (1.12.0)\n","Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (1.16.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage) (0.2.8)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage) (46.3.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage) (3.1.1)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage) (4.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (1.51.0)\n","Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (3.10.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2018.9)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2.23.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hu_VMe9grp4r","colab_type":"text"},"source":["## Upload files into bucket"]},{"cell_type":"code","metadata":{"id":"_EU_RZQr7glU","colab_type":"code","outputId":"58e972d9-0960-4896-e065-73d738810b5a","executionInfo":{"status":"ok","timestamp":1590557151877,"user_tz":-480,"elapsed":2151,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from google.cloud import storage\n","\n","# then we should our trained model and processor object into our bucket for later use case\n","bucket_name = \"first_bucket_lugq\"\n","\n","os.environ['GCLOUD_PROJECT'] = 'cloudtutorial-278306'\n","\n","from google.cloud import storage\n","client = storage.Client()\n","\n","bucket = client.bucket(bucket_name)\n","\n","def upload_file(source_file, des_file):\n","  blob = bucket.blob(des_file)\n","  try:\n","    blob.upload_from_filename(source_file)\n","    print(\"File :{} has been uploaded\".format(source_file))\n","  except Exception as e:\n","    raise Exception(\"Upload file with error:\", e)\n","\n","source_file_list = ['keras_model.h5', 'processor.pkl']\n","for file in source_file_list:\n","  upload_file(file, os.path.join('model', file))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["File :keras_model.h5 has been uploaded\n","File :processor.pkl has been uploaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kgciSJ917o4N","colab_type":"code","outputId":"979f9769-7e17-490b-828b-e330fe291da6","executionInfo":{"status":"ok","timestamp":1590554314916,"user_tz":-480,"elapsed":147239,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["# just to ensure the file has been uploaded\n","list(bucket.list_blobs())"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<Blob: first_bucket_lugq, keras_model.h5, 1590554313328791>,\n"," <Blob: first_bucket_lugq, models/, 1590549398786260>,\n"," <Blob: first_bucket_lugq, models/keras_model.h5, 1590549490950859>,\n"," <Blob: first_bucket_lugq, package/tutorial_pred-0.1.tar.gz, 1590547407658021>,\n"," <Blob: first_bucket_lugq, processor.pkl, 1590554314224308>,\n"," <Blob: first_bucket_lugq, test.zip, 1590405307598367>,\n"," <Blob: first_bucket_lugq, tmp, 1590404269440879>]"]},"metadata":{"tags":[]},"execution_count":151}]},{"cell_type":"code","metadata":{"id":"AC_mlz_k_UOY","colab_type":"code","outputId":"806ee74f-cfac-41da-ccc0-383e326b9bf7","executionInfo":{"status":"ok","timestamp":1590557156508,"user_tz":-480,"elapsed":2661,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":483}},"source":["# wrap the project into a zip file\n","!python setup.py sdist"],"execution_count":0,"outputs":[{"output_type":"stream","text":["running sdist\n","running egg_info\n","writing tutorial_pred.egg-info/PKG-INFO\n","writing dependency_links to tutorial_pred.egg-info/dependency_links.txt\n","writing top-level names to tutorial_pred.egg-info/top_level.txt\n","reading manifest file 'tutorial_pred.egg-info/SOURCES.txt'\n","writing manifest file 'tutorial_pred.egg-info/SOURCES.txt'\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","running check\n","warning: check: missing required meta-data: url\n","\n","warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n","\n","creating tutorial_pred-0.1\n","creating tutorial_pred-0.1/tutorial_pred.egg-info\n","copying files to tutorial_pred-0.1...\n","copying model_prediction.py -> tutorial_pred-0.1\n","copying preprocess.py -> tutorial_pred-0.1\n","copying setup.py -> tutorial_pred-0.1\n","copying tutorial_pred.egg-info/PKG-INFO -> tutorial_pred-0.1/tutorial_pred.egg-info\n","copying tutorial_pred.egg-info/SOURCES.txt -> tutorial_pred-0.1/tutorial_pred.egg-info\n","copying tutorial_pred.egg-info/dependency_links.txt -> tutorial_pred-0.1/tutorial_pred.egg-info\n","copying tutorial_pred.egg-info/top_level.txt -> tutorial_pred-0.1/tutorial_pred.egg-info\n","Writing tutorial_pred-0.1/setup.cfg\n","Creating tar archive\n","removing 'tutorial_pred-0.1' (and everything under it)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xs0pfqai-qvh","colab_type":"code","outputId":"147ad889-06ee-4bd5-d357-b17f6b698274","executionInfo":{"status":"ok","timestamp":1590557164411,"user_tz":-480,"elapsed":850,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# upload the target source file into bucket\n","package_file_name = os.listdir('dist')[0]\n","print(package_file_name)\n","upload_file(os.path.join('dist', package_file_name), os.path.join('package', package_file_name))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tutorial_pred-0.1.tar.gz\n","File :dist/tutorial_pred-0.1.tar.gz has been uploaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"beOxKfHxAIEr","colab_type":"code","outputId":"e5fc73ec-087a-470e-ac57-6ad5450d9e4c","executionInfo":{"status":"ok","timestamp":1590554319783,"user_tz":-480,"elapsed":152050,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# config current script with our project\n","!gcloud config set project cloudtutorial-278306"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tPf1tSrTrySK","colab_type":"text"},"source":["## Create model version\n","\n","Why we need with model version in AI-Platform? As we should deploy our model into the server when we have already trained our model which as a serialized obejct, a `version` is a instance represented in cloud, after we have deployed our model, then we could even write our own prediction class to do the prediction as we have already done before. If you are curious about the other term when deploy the model could get info [here](https://cloud.google.com/ai-platform/training/docs/projects-models-versions-jobs)."]},{"cell_type":"code","metadata":{"id":"dp7KMHKtGwJQ","colab_type":"code","colab":{}},"source":["MODEL_VERSION = 'v12'\n","MODEL_NAME = \"keras_model_tutorial\"\n","BUCKET_NAME = \"first_bucket_lugq\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0nX0xiYeAaL","colab_type":"code","outputId":"d4aff557-949c-4831-e720-efd6df3a74e4","executionInfo":{"status":"ok","timestamp":1590555275379,"user_tz":-480,"elapsed":3125,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":192}},"source":["# then create the model on the platform\n","!gcloud ai-platform models create $MODEL_NAME "],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n","\u001b[1;33mWARNING:\u001b[0m Please explicitly specify a region. Using [us-central1] by default on https://ml.googleapis.com. Please note that your model will be inaccessible from https://us-central1-ml.googelapis.com\n","\n","Learn more about regional endpoints and see a list of available regions: https://cloud.google.com/ai-platform/prediction/docs/regional-endpoints\n","\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.models.create) Resource in project [cloudtutorial-278306] is the subject of a conflict: Field: model.name Error: A model with the same name already exists.\n","- '@type': type.googleapis.com/google.rpc.BadRequest\n","  fieldViolations:\n","  - description: A model with the same name already exists.\n","    field: model.name\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pOIE6NQieXOC","colab_type":"code","outputId":"b5b2efdb-0151-4af5-b9c8-80a7a6689722","executionInfo":{"status":"ok","timestamp":1590557237323,"user_tz":-480,"elapsed":60851,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!gcloud beta ai-platform versions create $MODEL_VERSION \\\n","--model $MODEL_NAME \\\n","--runtime-version 1.12 \\\n","--python-version 3.5 \\\n","--origin gs://$BUCKET_NAME/model/ \\\n","--package-uris gs://$BUCKET_NAME/package/tutorial_pred-0.1.tar.gz \\\n","--prediction-class model_prediction.ModelPredictor"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L8DXlDPCjX3U","colab_type":"text"},"source":["## Make predictons based on trained model\n","\n","As we have already deployed our model into the server, then we could do some tests to send the request data to the server to confirm that we already put our model into server."]},{"cell_type":"code","metadata":{"id":"Nk2-t5PQdDaj","colab_type":"code","outputId":"9d57515b-6206-41b1-de37-8d31b51e7e44","executionInfo":{"status":"ok","timestamp":1590556598505,"user_tz":-480,"elapsed":5397,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":503}},"source":["# we could use google python client to do that with easier way\n","! pip install --upgrade google-api-python-client"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting google-api-python-client\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/dc/1207147686a770a867c918fac580e73fd5c8dcac1ec918ebd868b3f62c8b/google_api_python_client-1.8.4-py3-none-any.whl (58kB)\n","\r\u001b[K     |█████▋                          | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.12.0)\n","Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.1)\n","Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.17.3)\n","Requirement already satisfied, skipping upgrade: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.16.0)\n","Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.3)\n","Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.7.2)\n","Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client) (46.3.0)\n","Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client) (2018.9)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client) (3.10.0)\n","Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client) (1.51.0)\n","Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client) (2.23.0)\n","Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.1.1)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.8)\n","Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client) (2.9)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client) (2020.4.5.1)\n","Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.8)\n","Installing collected packages: google-api-python-client\n","  Found existing installation: google-api-python-client 1.7.12\n","    Uninstalling google-api-python-client-1.7.12:\n","      Successfully uninstalled google-api-python-client-1.7.12\n","Successfully installed google-api-python-client-1.8.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kUX11zwvuVlx","colab_type":"text"},"source":["## Get prediction online\n","\n","After we have already deployed our model into server, we could get prediction with HTTP post request. There are many ways that we could use to get prediction like `curl` or `python request`. \n","\n","The logic here is trying to serialize the raw data within JSON data for remote server, then remote server get raw data and do preprocessing logic, and load trained model from bucket in memory, last we could get the prediction with our custom prediction class, returned object is also a JSON object, if we face any error, this JSON is just with `error` key, otherwise we could get `predictions` key, this is based on custom implement logic, currently I just return a probabilities. For detail info could be found [predict](https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects/predict#tensorflow)"]},{"cell_type":"code","metadata":{"id":"G2ZAC6UVmcBn","colab_type":"code","colab":{}},"source":["# Here just make a function to get the prediction result\n","def get_pred_class(predictions):\n","  for i, pred in enumerate(predictions):\n","    pred_class_name = encoder.classes_[np.argmax(pred)]\n","    yield pred_class_name"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kBVhwLAeDiWi","colab_type":"code","outputId":"adf413a4-d444-496a-be04-7af2b10f077e","executionInfo":{"status":"ok","timestamp":1590558099438,"user_tz":-480,"elapsed":1733,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# we could use google api to get the prediction result.\n","import googleapiclient.discovery\n","\n","project_id = \"cloudtutorial-278306\"\n","\n","# find the service and build the object\n","service = googleapiclient.discovery.build('ml', 'v1')\n","name = \"projects/{}/models/{}/versions/{}\".format(project_id, MODEL_NAME, MODEL_VERSION)\n","\n","# get prediction repsponse\n","response = service.projects().predict(name=name, body={'instances': request_data}).execute()\n","\n","if 'error' in response:\n","  raise RuntimeError(response['error'])\n","else:\n","  # by default will return the prediction probabilities\n","  pred = get_pred_class(response['predictions'])\n","  for i, p_class in enumerate(pred):\n","    print(\"Sample text: {}.. With prediction class: `{}`\".format(request_data[i][:20], p_class))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sample text: How to preprocess st.. With prediction class: `keras`\n","Sample text: Change the bar item .. With prediction class: `pandas`\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4gaTyFyrpoCQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3AByx5sTyPMK","colab_type":"text"},"source":["## Final words\n","\n","This script is mainly for model prediction logic here, there are many other functionalities that should be covered, code more and learn more."]},{"cell_type":"code","metadata":{"id":"2R7eCG89y3qW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}