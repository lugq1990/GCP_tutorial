{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCP_Composer_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE3vkJw5a1Ob",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "When we need to deploy our code into production for batch processing, maybe we need to automate the job to run during some periods or at some time, that's **Composer** comes in, which builds on [**apache airflow**](https://airflow.apache.org/): a open-source author, schedule and monitor workflows\n",
        "\n",
        "Key features:\n",
        "\n",
        "\n",
        "> Fully managed workflow orchestration\n",
        "\n",
        "\n",
        "> Integrates with other Google Cloud products\n",
        "\n",
        "This is pure python supported, let's get started with **Composer**.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtyt4jdAr6ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first auth\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qrcSOJNsCan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72bf9d22-97b5-44ed-ea64-1bf2ef1a9074"
      },
      "source": [
        "# set project\n",
        "! gcloud config set project cloudtutorial-279003"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwsxZT_0sxW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for composer, the main entry is environment that the airflow runs in\n",
        "# first let's try to create environment\n",
        "# this do take much time to finish, as this will also need to create a GKE cluster\n",
        "# as Composer is container based, that's why.\n",
        "! gcloud composer environments create first-composer --location us-central1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKVVQ_nStA2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f681279-37e6-428c-9c5a-36b87ac2c028"
      },
      "source": [
        "# let's create a python file to do ML training with sklearn, \n",
        "# this is just a example, we could do more than this.\n",
        "%%writefile training.py\n",
        "import datetime\n",
        "import logging\n",
        "import airflow\n",
        "from airflow.operators import bash_operator\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# When I deploy code into Composer always no_status, \n",
        "# according to: https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/\n",
        "# shouldn't provide start_date with now timestamps\n",
        "now = datetime.datetime.now() - datetime.timedelta(days=1)\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'lugq',\n",
        "    'depends_on_past': False,\n",
        "    'email': [''],\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': datetime.timedelta(minutes=5),\n",
        "    'start_date': now,\n",
        "}\n",
        "\n",
        "\n",
        "# training interval is just 1 day a time.\n",
        "dag = airflow.DAG(\"training_sklearn\", \"catchup=False\", default_args=default_args, schedule_interval=datetime.timedelta(days=1))\n",
        "\n",
        "x, y = load_iris(return_X_y=True)\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# this is our training function, we could write our logic here to trigger our function.\n",
        "def train_model():\n",
        "    print(\"Start to train model\")\n",
        "    lr.fit(x, y)\n",
        "\n",
        "    score = lr.score(x, y)\n",
        "    print(\"Model test score: {}\".format(score))\n",
        "\n",
        "PythonOperator(dag=dag,\n",
        "               task_id='Task_with_python',\n",
        "               provide_context=False,\n",
        "               python_callable=train_model)\n",
        "\n",
        "print(\"Whole training finished.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting training.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcEvG-aqw9Cl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f3f86e2-0624-4c1a-f7eb-5b3be3587e98"
      },
      "source": [
        "# before we do anything, I have to say that currently with GKE for python, sklearn hasn't been installed.\n",
        "# so we have to provide a requirments.txt file to define the libraries we use\n",
        "%%writefile requirements.txt\n",
        "scikit-learn == 0.23.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJUY6rYYxt6I",
        "colab_type": "text"
      },
      "source": [
        "#### Install dependencies\n",
        "\n",
        "We could install packages we used in code with composer [here](https://cloud.google.com/composer/docs/how-to/using/installing-python-dependencies#install-package)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUBrUKKkxW_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then we have to update current env with gcloud\n",
        "# if we don't want to wait, could just add: --async\n",
        "# I have to say this do take too much time!!!\n",
        "! gcloud composer environments update first-composer --update-pypi-packages-from-file requirements.txt --location us-central1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYNkHoacxnQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "fa3445e7-bc5f-461b-a3b6-b559e0ec2967"
      },
      "source": [
        "# then we have to update our code into GCS, into the bucket and folder name: dag\n",
        "# then Composer will help us to create schudule automatically\n",
        "\n",
        "# but first let's check which folder we upload our files\n",
        "! gsutil ls gs://"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://artifacts.cloudtutorial-279003.appspot.com/\n",
            "gs://asia-northeast1-first-compo-a1a2973a-bucket/\n",
            "gs://asia.artifacts.cloudtutorial-279003.appspot.com/\n",
            "gs://cloudtutorial-279003.appspot.com/\n",
            "gs://cloudtutorial-279003_cloudbuild/\n",
            "gs://staging.cloudtutorial-279003.appspot.com/\n",
            "gs://us-central1-first-composer-ca40dafb-bucket/\n",
            "gs://us.artifacts.cloudtutorial-279003.appspot.com/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRGgCM4UzTF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a37d1d9a-bc3a-471a-ea1d-310705b5e3c1"
      },
      "source": [
        "# the bucket should be : gs://us-central1-first-composer-ca40dafb-bucket/\n",
        "# so let's upload our file into the bucket folder: dags\n",
        "! gsutil copy training.py gs://us-central1-first-composer-ca40dafb-bucket/dags/training.py"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://training.py [Content-Type=text/x-python]...\n",
            "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
            "Operation completed over 1 objects/1.4 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HouIcw2R0Vl6",
        "colab_type": "text"
      },
      "source": [
        "#### View our application in Airflow\n",
        "\n",
        "After the whole thing finished, we could get the Airflow link in our project **Composer** module, click the **environment**: first-composer, with link **environment variables**, we could get the link with **Airflow web UI**. \n",
        "\n",
        "Then we could get our application info we created, to get some info about Airflow, could just walk through the website we created, we could moniter, re-run and get the running info in the website, this is beyond of this tutorial.\n",
        "\n",
        "But you could find my result with airflow.\n",
        "![My airflow](https://docs.google.com/uc?export=download&id=1GjmW4arrii7a3wx8NYzDrpaKRXCIP7YD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dsxJ98A-CF1",
        "colab_type": "text"
      },
      "source": [
        "#### DAG with Airflow\n",
        "\n",
        "Let's try **Composer** with more advance with DAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRyLtrv7zlWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbdd2eeb-1ad2-4a64-9ee0-eef562be8edd"
      },
      "source": [
        "%%writefile training.py\n",
        "import datetime\n",
        "import logging\n",
        "\n",
        "import airflow\n",
        "from airflow.operators import bash_operator\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# When I deploy code into Composer always no_status, \n",
        "# according to: https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/\n",
        "# shouldn't provide start_date with now timestamps\n",
        "now = datetime.datetime.now() - datetime.timedelta(days=1)\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'lugq',\n",
        "    'depends_on_past': False,\n",
        "    'email': [''],\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': datetime.timedelta(minutes=5),\n",
        "    'start_date': now,\n",
        "}\n",
        "\n",
        "\n",
        "# training interval is just 1 day a time.\n",
        "dag = airflow.DAG(\"training_sklearn\", \"catchup=False\", default_args=default_args, schedule_interval=datetime.timedelta(days=1))\n",
        "\n",
        "x, y = load_iris(return_X_y=True)\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# this is our training function, we could write our logic here to trigger our function.\n",
        "def train_model():\n",
        "    print(\"Start to train model\")\n",
        "    lr.fit(x, y)\n",
        "\n",
        "    score = lr.score(x, y)\n",
        "    print(\"Model test score: {}\".format(score))\n",
        "\n",
        "\n",
        "# let's make airflow with DAG manager\n",
        "with airflow.models.DAG(\"Combine_bash_python\", default_args=default_args, \n",
        "                        schedule_interval=datetime.timedelta(days=1)) as dag:\n",
        "  echo_command =  BashOperator(task_id='start_task', bash_command=\"echo Start training with echo\")                    \n",
        "  python_training = PythonOperator(task_id='training_task', python_callable=train_model)\n",
        "  same_command = BashOperator(task_id='simulate_task', bash_command=\"echo This simuluately with training\")                    \n",
        "\n",
        "  # then we could define our DAG, with >> will define the steps.\n",
        "  echo_command >> [python_training, same_command]\n",
        "\n",
        "print(\"Whole training finished.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting training.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga8xcl5Q_mVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8028ca6a-efe6-4dc1-82df-71db60098d52"
      },
      "source": [
        "# so let's upload our file into the bucket folder: dags\n",
        "! gsutil copy training.py gs://us-central1-first-composer-ca40dafb-bucket/dags/training.py"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://training.py [Content-Type=text/x-python]...\n",
            "/ [1 files][  1.9 KiB/  1.9 KiB]                                                \n",
            "Operation completed over 1 objects/1.9 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPqXy8J8DmgR",
        "colab_type": "text"
      },
      "source": [
        "#### Result\n",
        "\n",
        "You could get result from from my web with DAG.\n",
        "![DAG result](https://docs.google.com/uc?export=download&id=1GH7HRDHyTA_2AEXVIUhcfAbpoUBoJNgU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bACAeuwgFnvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFWv6yVwEeho",
        "colab_type": "text"
      },
      "source": [
        "### Last word\n",
        "\n",
        "We do get the schedule with **Composer**, last step is to delete our resoures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrKFNpzx_ndZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "f2b88813-0b1d-4f2f-c5b8-d2b9eae579ab"
      },
      "source": [
        "! gcloud composer environments delete first-composer --location us-central1"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting the following environments: \n",
            " - [first-composer] in [us-central1]\n",
            "\n",
            "Do you want to continue (Y/n)?  y\n",
            "\n",
            "Delete in progress for environment [projects/cloudtutorial-279003/locations/us-central1/environments/first-composer] with operation [projects/cloudtutorial-279003/locations/us-central1/operations/03512721-fe13-43f5-8e30-a97b0b26d653].\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.composer.environments.delete) Aborting wait for operation projects/cloudtutorial-279003/locations/us-central1/operations/03512721-fe13-43f5-8e30-a97b0b26d653.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}