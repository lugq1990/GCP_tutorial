{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dataprocess_storage_bigqurey_dataproc_composer_terraform_combine.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMV8xvqGySSSMrN+f5FRGV+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"C7oq9O3128xU","colab_type":"text"},"source":["### Introduction\n","\n","This notebook is trying to combine whole GCP big data components into one notebook contains GCS, Bigquery, Dataproc, Composer and Terraform.\n","\n","If we face in real product, we need to use CI/CD pipeline to combine whole of them, but that's just a process not the fundamental part. \n","\n","Let's just start."]},{"cell_type":"code","metadata":{"id":"4xpQOr0N9qjZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595210770071,"user_tz":-480,"elapsed":747,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"01aa0955-2e58-4a52-baf5-f240a0797f15"},"source":["# first we need to create a **.tf file to build cloud environment that we need\n","# so here just write the file into a file called init.tf(any name should be fine)\n","\n","# as in the notebook to init the terraform isn't that easy, so I just start it in my own env.\n","%%writefile init.tf\n","// this will create some common usecase with AIA, so here just to create resources\n","\n","// first with composer\n","resource \"google_composer_environment\" \"test\" {\n","  name = \" proc-cluster-lu\"\n","  region = \"us-central1\"\n","  config {\n","    node_count = 3\n","    node_config {\n","      zone = \"us-central1-a\"\n","      machine_type = \"n1-standard-1\"\n","    }\n","  }\n","}\n","\n","// create bucket\n","resource \"google_storage_bucket\" \"default\" {\n","  name = \"aia-terraform-lugq\"\n","  location = \"US\"\n","\n","}\n","\n","// create dataproc cluster\n","resource \"google_dataproc_cluster\" \"my-cluster\" {\n","  name = \"my-cluster\"\n","  region = \"us-central1\"\n","\n","  cluster_config {\n","    master_config {\n","      num_instances = 1\n","      machine_type = \"n1-standard-1\"\n","      disk_config {\n","        boot_disk_size_gb = 15\n","      }\n","    }\n","    worker_config {\n","      num_instances = 2\n","      machine_type = \"n1-standard-1\"\n","      disk_config {\n","        boot_disk_size_gb = 15\n","      }\n","    }\n","  }\n","}"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Writing init.tf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ktr0euXJ3lWf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595209234112,"user_tz":-480,"elapsed":16878,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["#auth the notebook\n","from google.colab import auth\n","\n","auth.authenticate_user()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUu6krB4TSNA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595209243469,"user_tz":-480,"elapsed":6555,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"a0979b06-611f-4aec-8931-51c4f1dd6dd5"},"source":["! gcloud config set project \tcloudtutorial-283609"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kQNeYFe5UCzz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595210007535,"user_tz":-480,"elapsed":1197,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["project_id = \"cloudtutorial-283609\""],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"9i6S6dnyTZgJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595209246388,"user_tz":-480,"elapsed":2903,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["from sklearn.datasets import load_iris\n","import numpy as np\n","import pandas as pd\n","\n","x, y = load_iris(return_X_y=True)\n","\n","data = np.concatenate([x, y[:, np.newaxis]], axis=1)\n","\n","df = pd.DataFrame(data, columns=['a', 'b','c', 'd', 'label'])\n","\n","df.to_csv('data.csv', index=False)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AfCPyr0VI3M","colab_type":"text"},"source":["### upload file into bucket"]},{"cell_type":"code","metadata":{"id":"uUg-aYPvT0Pk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595209247774,"user_tz":-480,"elapsed":2751,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["from google.cloud import storage\n","\n","client = storage.Client(project_id)\n","\n","bucket_name = \"full_step_bucket_lugq\"\n","\n","bucket = client.create_bucket(bucket_name)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ac0C8LlnT6I4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595209255603,"user_tz":-480,"elapsed":6125,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"b40b44ba-4af2-48d6-89cc-46c9aa72eb83"},"source":["! gsutil ls gs://"],"execution_count":6,"outputs":[{"output_type":"stream","text":["gs://full_step_bucket_lugq/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hopXiJxWUdkk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595209257579,"user_tz":-480,"elapsed":1665,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"24c8ed8c-12c1-402f-83a6-3d8f55143dea"},"source":["# let's upload the file into bucket\n","blob = bucket.blob('data.csv')\n","try:\n","  blob.upload_from_filename('data.csv')\n","  print(\"file has been uploaded\")\n","except Exception as e:\n","  print(\"When upload file with error: {}\".format(e))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["file has been uploaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wYTRtTZCUwVt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595209261417,"user_tz":-480,"elapsed":1306,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"0a676d20-0c98-4ae1-9bcd-e762e145299b"},"source":["# list bucket\n","bucket = client.get_bucket(bucket_name)\n","blobs = bucket.list_blobs()\n","print(list(blobs))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[<Blob: full_step_bucket_lugq, data.csv, 1595209256655044>]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2OH77p7WVKny","colab_type":"text"},"source":["### Read file from bucket with DataProc"]},{"cell_type":"code","metadata":{"id":"7_1fDRSXYS5O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595209276369,"user_tz":-480,"elapsed":669,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"ad2e3213-8cc6-4596-a77f-3e996d9c8949"},"source":["### create a file to read data from storage\n","# and do a feature engineering step with Binarizer\n","%%writefile process_data_with_spark.py\n","\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import Binarizer\n","import os\n","import logging\n","\n","logger = logging.getLogger(__name__)\n","\n","project_id = \"cloudtutorial-283609\"\n","bucket_name = \"full_step_bucket_lugq\"\n","file_name = \"data.csv\"\n","\n","file_path = \"gs://{}/{}\".format(bucket_name, file_name)\n","\n","# init spark\n","spark = SparkSession.builder.getOrCreate()\n","\n","logger.info(\"read data.\")\n","# we should set `inferSchema `, so that with data type\n","df = spark.read.format('csv').option('header', 'true').option(\"inferSchema\", 'true').load(file_path)\n","\n","logger.info(\"Get data:\", df.show(5))\n","\n","# binarizer columns: `a`\n","binarizer = Binarizer(threshold=3.0, inputCol=\"a\", outputCol=\"a_binary\")\n","\n","df_new = binarizer.transform(df)\n","logger.info(\"get binarizer result:\", df_new.show(5))\n","\n","# write the result into bucket\n","logger.info(\"save result into bucket\")\n","# in case the file already exists.\n","df_new.write.format('csv').mode(\"overwrite\").save(\"gs://{}/{}\".format(bucket_name, 'data_new.csv'))\n","\n","logger.info(\"whole step finsished.\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Overwriting process_data_with_spark.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yu3C-T69aTnx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595209283540,"user_tz":-480,"elapsed":5530,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"ec7dd53b-083c-4d55-c325-61955b53659a"},"source":["# let's upload our pyspark file into bucket\n","! gsutil cp process_data_with_spark.py gs://full_step_bucket_lugq/process_data_with_spark.py"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Copying file://process_data_with_spark.py [Content-Type=text/x-python]...\n","/ [1 files][   1004 B/   1004 B]                                                \n","Operation completed over 1 objects/1004.0 B.                                     \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zyB2zWn_iXYa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595209285538,"user_tz":-480,"elapsed":1994,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# Here we have to create a Dataproc cluster with name: proc-cluster-lu\n","CLUSTER = \"proc-cluster-lu\""],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJUeur2RihcU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595209292573,"user_tz":-480,"elapsed":4489,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"90e066c5-c0f1-4715-d56d-bd0f04e6fd94"},"source":["# set region for DataProc\n","! gcloud config set dataproc/region us-central1"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Updated property [dataproc/region].\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9BSMOslxai9I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595209296019,"user_tz":-480,"elapsed":6447,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"6a3cbdd1-f868-48a6-fbbe-d72cf8f77d4c"},"source":["# check\n","! gsutil ls gs://full_step_bucket_lugq"],"execution_count":14,"outputs":[{"output_type":"stream","text":["gs://full_step_bucket_lugq/data.csv\n","gs://full_step_bucket_lugq/process_data_with_spark.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lg98NHEEa9xO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595209301390,"user_tz":-480,"elapsed":10309,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"1dfd4ca4-dfbc-443e-9601-efeaf78a0f1e"},"source":["# install dataproc\n","! pip install google-cloud-dataproc --quiet"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |█▏                              | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 296kB 2.8MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pGCWDxMgbNXh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595209402437,"user_tz":-480,"elapsed":109619,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"414d5393-7e5c-4e7d-8319-b131e2025add"},"source":["# with error: AttributeError: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'\n","# reinstall protobuf\n","! pip uninstall protobuf --quiet\n","\n","! pip install protobuf --quiet"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Proceed (y/n)? y\n","\u001b[K     |████████████████████████████████| 1.3MB 2.8MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y0_G7bJGcFTm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595209402439,"user_tz":-480,"elapsed":6917,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}}},"source":["# after re-start should make credential for service account\n","import os\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = [x for x in os.listdir('.') if x.lower().startswith('cloud')][0]"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSCsAlzecri7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595209961128,"user_tz":-480,"elapsed":65635,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"d625b303-1216-4195-ef31-0ff63b571467"},"source":["# let's just use gcloud to submit job to our cluster\n","# if we haven't enable with API will ask you to enable\n","! gcloud dataproc jobs submit pyspark process_data_with_spark.py --cluster=$CLUSTER"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Job [7cab330510b6490d9918a755d806a3e3] submitted.\n","Waiting for job output...\n","20/07/20 01:51:45 INFO org.spark_project.jetty.util.log: Logging initialized @4094ms\n","20/07/20 01:51:45 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n","20/07/20 01:51:45 INFO org.spark_project.jetty.server.Server: Started @4275ms\n","20/07/20 01:51:45 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@735566d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","20/07/20 01:51:46 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n","20/07/20 01:51:48 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at proc-cluster-lu-m/10.128.0.2:8032\n","20/07/20 01:51:48 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at proc-cluster-lu-m/10.128.0.2:10200\n","20/07/20 01:51:51 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\n","java.lang.InterruptedException\n","\tat java.lang.Object.wait(Native Method)\n","\tat java.lang.Thread.join(Thread.java:1252)\n","\tat java.lang.Thread.join(Thread.java:1326)\n","\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:980)\n","\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:630)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:807)\n","20/07/20 01:51:52 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1595209762313_0001\n","+---+---+---+---+-----+\n","|  a|  b|  c|  d|label|\n","+---+---+---+---+-----+\n","|5.1|3.5|1.4|0.2|  0.0|\n","|4.9|3.0|1.4|0.2|  0.0|\n","|4.7|3.2|1.3|0.2|  0.0|\n","|4.6|3.1|1.5|0.2|  0.0|\n","|5.0|3.6|1.4|0.2|  0.0|\n","+---+---+---+---+-----+\n","only showing top 5 rows\n","\n","+---+---+---+---+-----+--------+\n","|  a|  b|  c|  d|label|a_binary|\n","+---+---+---+---+-----+--------+\n","|5.1|3.5|1.4|0.2|  0.0|     1.0|\n","|4.9|3.0|1.4|0.2|  0.0|     1.0|\n","|4.7|3.2|1.3|0.2|  0.0|     1.0|\n","|4.6|3.1|1.5|0.2|  0.0|     1.0|\n","|5.0|3.6|1.4|0.2|  0.0|     1.0|\n","+---+---+---+---+-----+--------+\n","only showing top 5 rows\n","\n","20/07/20 01:52:35 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@735566d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","Job [7cab330510b6490d9918a755d806a3e3] finished successfully.\n","done: true\n","driverControlFilesUri: gs://dataproc-staging-us-central1-544826698357-zpklxhfw/google-cloud-dataproc-metainfo/c677af79-9d1a-43ba-b150-15cae758bca5/jobs/7cab330510b6490d9918a755d806a3e3/\n","driverOutputResourceUri: gs://dataproc-staging-us-central1-544826698357-zpklxhfw/google-cloud-dataproc-metainfo/c677af79-9d1a-43ba-b150-15cae758bca5/jobs/7cab330510b6490d9918a755d806a3e3/driveroutput\n","jobUuid: 3783aadf-5471-391b-9590-701e560b30c0\n","placement:\n","  clusterName: proc-cluster-lu\n","  clusterUuid: c677af79-9d1a-43ba-b150-15cae758bca5\n","pysparkJob:\n","  mainPythonFileUri: gs://dataproc-staging-us-central1-544826698357-zpklxhfw/google-cloud-dataproc-metainfo/c677af79-9d1a-43ba-b150-15cae758bca5/jobs/7cab330510b6490d9918a755d806a3e3/staging/process_data_with_spark.py\n","reference:\n","  jobId: 7cab330510b6490d9918a755d806a3e3\n","  projectId: cloudtutorial-283609\n","status:\n","  state: DONE\n","  stateStartTime: '2020-07-20T01:52:36.457Z'\n","statusHistory:\n","- state: PENDING\n","  stateStartTime: '2020-07-20T01:51:39.381Z'\n","- state: SETUP_DONE\n","  stateStartTime: '2020-07-20T01:51:39.415Z'\n","- details: Agent reported job success\n","  state: RUNNING\n","  stateStartTime: '2020-07-20T01:51:39.662Z'\n","yarnApplications:\n","- name: process_data_with_spark.py\n","  progress: 1.0\n","  state: FINISHED\n","  trackingUrl: http://proc-cluster-lu-m:8088/proxy/application_1595209762313_0001/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sZt9lQG884Rj","colab_type":"text"},"source":["### Load storage file into bigquery"]},{"cell_type":"code","metadata":{"id":"wR-Fh1-c8D34","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595210246565,"user_tz":-480,"elapsed":5344,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"94f5bd16-725f-4d23-9735-bc90749dab43"},"source":["! gsutil ls gs://"],"execution_count":27,"outputs":[{"output_type":"stream","text":["gs://dataproc-staging-us-central1-544826698357-zpklxhfw/\n","gs://dataproc-temp-us-central1-544826698357-vyt5vdvo/\n","gs://full_step_bucket_lugq/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HkSrWM7y9-f3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595210252319,"user_tz":-480,"elapsed":1270,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"ff7718be-2b1a-4f5e-d81f-9f975a5ef248"},"source":["# when we use spark to save file to storage, won't save just a file but a partition file\n","# so here just to get the file name in the bucket\n","from google.cloud import storage\n","\n","storage_client = storage.Client(\"cloudtutorial-283609\")\n","\n","bucket_name = \"full_step_bucket_lugq\"\n","folder_name = \"data_new.csv\"\n","\n","bucket = storage_client.get_bucket(bucket_name)\n","\n","file_list =  list(bucket.list_blobs(prefix=folder_name))\n","\n","try:\n","  file_name = [x.name for x in file_list if x.name.lower().endswith('csv')][0]\n","  print(\"get file: {}\".format(file_name))\n","except Exception as e:\n","  print(\"There isn't that file in the bucket.\")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["get file: data_new.csv/part-00000-c7d40999-3e58-42b6-836c-1ec4043af258-c000.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_yA_nCNUQ7Zb","colab_type":"text"},"source":["Create table with API, but we have to create the dataset with bigquery, `dataset` could represent as `database`, so let's create it first."]},{"cell_type":"code","metadata":{"id":"QEHYxnwoR8ss","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":414},"executionInfo":{"status":"ok","timestamp":1595210266034,"user_tz":-480,"elapsed":9332,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"687645ca-7fbf-48ae-cd62-0f5b8defcf77"},"source":["# create dataset with command\n","! bq mk iris_dataset"],"execution_count":29,"outputs":[{"output_type":"stream","text":["\n","Welcome to BigQuery! This script will walk you through the \n","process of initializing your .bigqueryrc configuration file.\n","\n","First, we need to set up your credentials if they do not \n","already exist.\n","\n","Credential creation complete. Now we will select a default project.\n","\n","List of projects:\n","  #        projectId           friendlyName    \n"," --- ---------------------- ------------------ \n","  1   cloudtutorial-283609   CloudTutorial     \n","  2   my-project-34336       My Project 34336  \n","Found multiple projects. Please enter a selection for \n","which should be the default, or leave blank to not \n","set a default.\n","\n","Enter a selection (1 - 2): 1\n","\n","BigQuery configuration complete! Type \"bq\" to get started.\n","\n","Dataset 'cloudtutorial-283609:iris_dataset' successfully created.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lx0NcHHL8pGc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595210275390,"user_tz":-480,"elapsed":6382,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"ce7662ab-4280-4e48-993f-4aae39bfc838"},"source":["from google.cloud import bigquery\n","\n","# we need to create the dataset in console first\n","dataset_id = \"iris_dataset\"\n","\n","# init bigquery client\n","client = bigquery.Client(project_id)\n","\n","# create dataset inference\n","dataset_ref = client.dataset(dataset_id)\n","\n","# define schema\n","job_config = bigquery.LoadJobConfig()\n","job_config.schema = [bigquery.SchemaField(\"a\", \"float\"),\n","                     bigquery.SchemaField(\"b\", \"float\"),\n","                     bigquery.SchemaField(\"c\", \"float\"),\n","                     bigquery.SchemaField(\"d\", \"float\"),\n","                     bigquery.SchemaField(\"label\", \"float\"),\n","                     bigquery.SchemaField(\"a_bina\", \"float\")]\n","\n","# skip the header, as I skip first row just get 149 records. not correct\n","# job_config.skip_leading_rows = 1\n","# set to load csv\n","job_config.source_format = bigquery.SourceFormat.CSV\n","\n","# data uri\n","data_uri = \"gs://{}/{}\".format(bucket_name, file_name)\n","\n","# create a load job\n","load_job = client.load_table_from_uri(data_uri, dataset_ref.table('iris'), job_config =job_config)\n","print(\"submitted job: {}\".format(load_job.job_id))\n","\n","# wait result to finish\n","load_job.result()\n","\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["submitted job: fc4ecc80-28b3-44c8-a10f-2aed61412fe4\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<google.cloud.bigquery.job.LoadJob at 0x7f25f128ae48>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"W5ftKxMt_zmZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595210277315,"user_tz":-480,"elapsed":1909,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"b47811d4-b535-46a0-8227-9c17e63dfedd"},"source":["# let's check\n","check_table = client.get_table(dataset_ref.table('iris'))\n","\n","print(\"there are {} records in bigquery\".format(check_table.num_rows))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["there are 150 records in bigquery\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uf0hR59cNiqe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595210280427,"user_tz":-480,"elapsed":1122,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"b5f849bf-4acb-49bc-cce0-8bd8e098826e"},"source":["# let's make a into a file for Composer test\n","%%writefile load_data_into_bigquery.py\n","from google.cloud import storage\n","\n","storage_client = storage.Client(project_id)\n","\n","bucket_name = \"full_step_bucket\"\n","folder_name = \"data_new.csv\"\n","\n","bucket = storage_client.get_bucket(bucket_name)\n","\n","file_list =  list(bucket.list_blobs(prefix=folder_name))\n","\n","file_name = [x.name for x in file_list if x.name.lower().endswith('csv')][0]\n","\n","print(\"get file: {}\".format(file_name))\n","\n","from google.cloud import bigquery\n","\n","# we need to create the dataset in console first\n","dataset_id = \"iris_dataset\"\n","\n","# init bigquery client\n","client = bigquery.Client(project_id)\n","\n","# create dataset inference\n","dataset_ref = client.dataset(dataset_id)\n","\n","# define schema\n","job_config = bigquery.LoadJobConfig()\n","job_config.schema = [bigquery.SchemaField(\"a\", \"float\"),\n","                     bigquery.SchemaField(\"b\", \"float\"),\n","                     bigquery.SchemaField(\"c\", \"float\"),\n","                     bigquery.SchemaField(\"d\", \"float\"),\n","                     bigquery.SchemaField(\"label\", \"float\"),\n","                     bigquery.SchemaField(\"a_bina\", \"float\")]\n","\n","# skip the header, as I skip first row just get 149 records. not correct\n","# job_config.skip_leading_rows = 1\n","# set to load csv\n","job_config.source_format = bigquery.SourceFormat.CSV\n","\n","# data uri\n","data_uri = \"gs://{}/{}\".format(bucket_name, file_name)\n","\n","# create a load job\n","load_job = client.load_table_from_uri(data_uri, dataset_ref.table('iris'), job_config =job_config)\n","print(\"submitted job: {}\".format(load_job.job_id))\n","\n","# wait result to finish\n","load_job.result()\n","# let's check\n","check_table = client.get_table(dataset_ref.table('iris'))\n","\n","print(\"there are {} records in bigquery\".format(check_table.num_rows))\n","|"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Writing load_data_into_bigquery.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pHSVfaeXDoKy","colab_type":"text"},"source":["#### DataProc with bigquery"]},{"cell_type":"code","metadata":{"id":"55EKGjqgDnbc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595210289642,"user_tz":-480,"elapsed":1393,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"c2c7e77b-bfc4-462d-f8cb-6f06468e1f6e"},"source":["# this file is to read data from bigquery and train a AI model\n","%%writefile spark_train_bigquery.py\n","\n","from pyspark.sql import SparkSession\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.classification import LogisticRegression\n","import logging\n","\n","logger = logging.getLogger(__name__)\n","\n","# combine features into vector and get lable\n","def inputs_to_vector(row):\n","  return (row['label'], Vectors.dense(float(row['a']), \n","                                      float(row['b']), \n","                                      float(row['c']), \n","                                      float(row['d']), \n","                                      float(row['a_bina']) ))\n","\n","# create sparksession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# read bigquery return into d DataFrame\n","logger.info(\"Read data from bigquery\")\n","df = spark.read.format('bigquery').option('table', 'iris_dataset.iris').load()\n","\n","# logger.info(\"get dataframe:\", df.show(5))\n","df.createOrReplaceTempView('iris')\n","\n","df_new = spark.sql(\"select * from iris\")\n","\n","# map dataframe with vector function\n","data_df = df_new.rdd.map(inputs_to_vector).toDF([\"label\", \"features\"])\n","\n","# split into train and test\n","(train_df, test_df) = data_df.randomSplit([0.7, 0.3])\n","\n","# cache dataframe\n","train_df.cache()\n","\n","lr = LogisticRegression(maxIter=100, regParam=0.1,elasticNetParam=0.8)\n","\n","logger.info(\"start to train model\")\n","model = lr.fit(train_df)\n","\n","pred = model.transform(test_df)\n","\n","print(\"get prediction:\", pred.show(5))\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Writing spark_train_bigquery.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pftUlvFv_0hs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595210340831,"user_tz":-480,"elapsed":4505,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"658bb8b3-7791-4226-c428-1168d7b4f4a8"},"source":["# upload python file into bucket\n","! gsutil cp spark_train_bigquery.py gs://full_step_bucket_lugq"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Copying file://spark_train_bigquery.py [Content-Type=text/x-python]...\n","/ [1 files][  1.3 KiB/  1.3 KiB]                                                \n","Operation completed over 1 objects/1.3 KiB.                                      \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NwLyRwA1G2qc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1595210357236,"user_tz":-480,"elapsed":4624,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"0457f97a-97f9-4600-f861-41e7467388c1"},"source":["# check bucket\n","! gsutil ls gs://full_step_bucket_lugq"],"execution_count":38,"outputs":[{"output_type":"stream","text":["gs://full_step_bucket_lugq/data.csv\n","gs://full_step_bucket_lugq/process_data_with_spark.py\n","gs://full_step_bucket_lugq/spark_train_bigquery.py\n","gs://full_step_bucket_lugq/data_new.csv/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RXfvzzNfH9ZH","colab_type":"text"},"source":["##### Tips:\n","\n","When we connect with bigquery, we have to provide with the connection jar: **gs://spark-lib/bigquery/spark-bigquery-latest.jar**."]},{"cell_type":"code","metadata":{"id":"kstZhaC7G53m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595210550557,"user_tz":-480,"elapsed":97893,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"100f3610-7cf9-4592-b1b2-ec5aba62588d"},"source":["# submit the dataproc job with command\n","! gcloud dataproc jobs submit pyspark gs://full_step_bucket_lugq/spark_train_bigquery.py \\\n","--cluster=$CLUSTER --region=us-central1 \\\n","--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Job [e7d9c005583e4a8195b9b528bad2ec1f] submitted.\n","Waiting for job output...\n","20/07/20 02:01:09 INFO org.spark_project.jetty.util.log: Logging initialized @4728ms\n","20/07/20 02:01:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n","20/07/20 02:01:09 INFO org.spark_project.jetty.server.Server: Started @4919ms\n","20/07/20 02:01:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@2a07dfeb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","20/07/20 02:01:10 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n","20/07/20 02:01:11 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at proc-cluster-lu-m/10.128.0.2:8032\n","20/07/20 02:01:11 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at proc-cluster-lu-m/10.128.0.2:10200\n","20/07/20 02:01:14 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\n","java.lang.InterruptedException\n","\tat java.lang.Object.wait(Native Method)\n","\tat java.lang.Thread.join(Thread.java:1252)\n","\tat java.lang.Thread.join(Thread.java:1326)\n","\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:980)\n","\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:630)\n","\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:807)\n","20/07/20 02:01:15 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1595209762313_0002\n","20/07/20 02:01:35 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table cloudtutorial-283609.iris_dataset.iris, parameters sent from Spark: requiredColumns=[a,b,c,d,label,a_bina], filters=[]\n","20/07/20 02:01:35 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from cloudtutorial-283609.iris_dataset.iris columns=[a, b, c, d, label, a_bina], filter=''\n","20/07/20 02:01:38 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'cloudtutorial-283609.iris_dataset.iris': projects/cloudtutorial-283609/locations/us/sessions/CAISDExTRDd1cGhuVG8ydBoCanIaAml3GgJuYRoCb3MaAmluGgJqcRoCaXIaAmpjGgJqZBoCb2o\n","20/07/20 02:01:55 INFO com.github.fommil.jni.JniLoader: successfully loaded /tmp/jniloader7867540601188456864netlib-native_system-linux-x86_64.so\n","20/07/20 02:01:56 INFO breeze.optimize.OWLQN: Step Size: 0.05118\n","20/07/20 02:01:56 INFO breeze.optimize.OWLQN: Val and Grad Norm: 1.07523 (rel: 0.0210) 0.668710\n","20/07/20 02:01:56 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:56 INFO breeze.optimize.OWLQN: Val and Grad Norm: 1.00216 (rel: 0.0680) 0.776060\n","20/07/20 02:01:56 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:56 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.850449 (rel: 0.151) 1.36818\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.830620 (rel: 0.0233) 1.32716\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.788586 (rel: 0.0506) 0.161374\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.786985 (rel: 0.00203) 0.195787\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.785646 (rel: 0.00170) 0.126793\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.783871 (rel: 0.00226) 0.105855\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:57 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.779254 (rel: 0.00589) 0.213694\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.777933 (rel: 0.00169) 0.132314\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.776690 (rel: 0.00160) 0.160126\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.775263 (rel: 0.00184) 0.0952772\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.774368 (rel: 0.00115) 0.183248\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:01:58 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.771862 (rel: 0.00324) 0.123789\n","20/07/20 02:01:59 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:01:59 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.769779 (rel: 0.00270) 0.128011\n","20/07/20 02:01:59 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:01:59 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.767692 (rel: 0.00271) 0.122258\n","20/07/20 02:01:59 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:01:59 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.767119 (rel: 0.000746) 0.220735\n","20/07/20 02:02:00 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:00 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.764812 (rel: 0.00301) 0.0965628\n","20/07/20 02:02:00 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:00 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.764087 (rel: 0.000949) 0.0731989\n","20/07/20 02:02:01 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:01 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.763554 (rel: 0.000696) 0.0682308\n","20/07/20 02:02:01 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:01 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.763108 (rel: 0.000584) 0.102016\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.762574 (rel: 0.000700) 0.149340\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.761605 (rel: 0.00127) 0.235563\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.759816 (rel: 0.00235) 0.0817402\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.759006 (rel: 0.00107) 0.103923\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.755492 (rel: 0.00463) 0.252068\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.753495 (rel: 0.00264) 0.286505\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:02 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.746702 (rel: 0.00901) 0.366192\n","20/07/20 02:02:03 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:03 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.744875 (rel: 0.00245) 0.145294\n","20/07/20 02:02:03 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:03 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.744116 (rel: 0.00102) 0.381589\n","20/07/20 02:02:03 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:03 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.741387 (rel: 0.00367) 0.0577992\n","20/07/20 02:02:04 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:04 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.740309 (rel: 0.00145) 0.131337\n","20/07/20 02:02:04 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:04 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.739605 (rel: 0.000951) 0.0982980\n","20/07/20 02:02:04 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:04 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.739411 (rel: 0.000263) 0.334441\n","20/07/20 02:02:04 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:04 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.737056 (rel: 0.00318) 0.201066\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.736265 (rel: 0.00107) 0.121940\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.735869 (rel: 0.000538) 0.171570\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.735397 (rel: 0.000641) 0.142871\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.734691 (rel: 0.000960) 0.130932\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:05 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.734610 (rel: 0.000111) 0.166964\n","20/07/20 02:02:06 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:06 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.733769 (rel: 0.00114) 0.117978\n","20/07/20 02:02:06 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:06 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.733455 (rel: 0.000428) 0.0647262\n","20/07/20 02:02:06 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:06 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.733317 (rel: 0.000188) 0.100667\n","20/07/20 02:02:07 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:07 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.733115 (rel: 0.000276) 0.0740386\n","20/07/20 02:02:07 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:07 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.732913 (rel: 0.000275) 0.0838627\n","20/07/20 02:02:07 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:07 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.732813 (rel: 0.000136) 0.0787516\n","20/07/20 02:02:07 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:07 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.732591 (rel: 0.000303) 0.0807659\n","20/07/20 02:02:08 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:08 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.732508 (rel: 0.000113) 0.0804000\n","20/07/20 02:02:08 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:08 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.732271 (rel: 0.000324) 0.0768845\n","20/07/20 02:02:08 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:08 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.732175 (rel: 0.000130) 0.0770674\n","20/07/20 02:02:08 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:08 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.732132 (rel: 5.86e-05) 0.154248\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.731698 (rel: 0.000594) 0.0983097\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.731433 (rel: 0.000362) 0.102234\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.731298 (rel: 0.000185) 0.108116\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.731090 (rel: 0.000284) 0.0984856\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:09 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730951 (rel: 0.000190) 0.0928390\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730717 (rel: 0.000320) 0.0696339\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730599 (rel: 0.000162) 0.0906580\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730404 (rel: 0.000266) 0.0339081\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730280 (rel: 0.000170) 0.0534742\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730234 (rel: 6.27e-05) 0.0222088\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:10 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730113 (rel: 0.000167) 0.0545575\n","20/07/20 02:02:11 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:11 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730053 (rel: 8.19e-05) 0.0271540\n","20/07/20 02:02:11 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:11 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.730036 (rel: 2.36e-05) 0.0756434\n","20/07/20 02:02:11 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:11 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.729898 (rel: 0.000188) 0.0291130\n","20/07/20 02:02:11 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:11 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.729830 (rel: 9.40e-05) 0.0435772\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.729816 (rel: 1.94e-05) 0.0427776\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.729688 (rel: 0.000175) 0.0363648\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.729537 (rel: 0.000207) 0.101964\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.729292 (rel: 0.000336) 0.0971719\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:12 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.729076 (rel: 0.000296) 0.0724519\n","20/07/20 02:02:13 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:13 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.728803 (rel: 0.000375) 0.0474123\n","20/07/20 02:02:13 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:13 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.728723 (rel: 0.000109) 0.0267653\n","20/07/20 02:02:13 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:13 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.728433 (rel: 0.000397) 0.0943594\n","20/07/20 02:02:14 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:14 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.728126 (rel: 0.000423) 0.0555135\n","20/07/20 02:02:14 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:14 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.727937 (rel: 0.000260) 0.0472222\n","20/07/20 02:02:14 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:14 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.727906 (rel: 4.15e-05) 0.0949110\n","20/07/20 02:02:15 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:15 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.727686 (rel: 0.000303) 0.124621\n","20/07/20 02:02:15 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:15 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.727525 (rel: 0.000220) 0.105057\n","20/07/20 02:02:15 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:15 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.727070 (rel: 0.000626) 0.0739599\n","20/07/20 02:02:15 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:15 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726902 (rel: 0.000232) 0.0294370\n","20/07/20 02:02:16 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:16 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726870 (rel: 4.43e-05) 0.0679994\n","20/07/20 02:02:16 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:16 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726727 (rel: 0.000196) 0.0296590\n","20/07/20 02:02:16 INFO breeze.optimize.OWLQN: Step Size: 0.1250\n","20/07/20 02:02:16 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726672 (rel: 7.63e-05) 0.0614970\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726585 (rel: 0.000119) 0.0458462\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726485 (rel: 0.000139) 0.0671251\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726361 (rel: 0.000171) 0.0433424\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726255 (rel: 0.000145) 0.0506826\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726183 (rel: 0.000100) 0.0756063\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:17 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.726040 (rel: 0.000197) 0.104172\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.725932 (rel: 0.000149) 0.0970697\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.725747 (rel: 0.000254) 0.117708\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Step Size: 0.5000\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.725461 (rel: 0.000394) 0.0779694\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Step Size: 0.2500\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.725169 (rel: 0.000402) 0.0215801\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Step Size: 1.000\n","20/07/20 02:02:18 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.724652 (rel: 0.000714) 0.0338240\n","20/07/20 02:02:19 INFO breeze.optimize.OWLQN: Step Size: 0.06250\n","20/07/20 02:02:19 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.724561 (rel: 0.000125) 0.0468774\n","20/07/20 02:02:19 INFO breeze.optimize.OWLQN: Step Size: 0.06250\n","20/07/20 02:02:19 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.724518 (rel: 5.99e-05) 0.0544983\n","20/07/20 02:02:19 INFO breeze.optimize.OWLQN: Step Size: 0.06250\n","20/07/20 02:02:19 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.724372 (rel: 0.000202) 0.0457426\n","20/07/20 02:02:19 INFO breeze.optimize.OWLQN: Step Size: 0.06250\n","20/07/20 02:02:19 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.724326 (rel: 6.26e-05) 0.0521191\n","20/07/20 02:02:20 INFO breeze.optimize.OWLQN: Step Size: 0.06250\n","20/07/20 02:02:20 INFO breeze.optimize.OWLQN: Val and Grad Norm: 0.724196 (rel: 0.000181) 0.0447165\n","20/07/20 02:02:20 INFO breeze.optimize.OWLQN: Converged because max iterations reached\n","+-----+--------------------+--------------------+--------------------+----------+\n","|label|            features|       rawPrediction|         probability|prediction|\n","+-----+--------------------+--------------------+--------------------+----------+\n","|  0.0|[4.6,3.2,1.4,0.2,...|[1.83334704999830...|[0.79776474121306...|       0.0|\n","|  0.0|[4.6,3.4,1.4,0.3,...|[1.84798400382203...|[0.81082691666089...|       0.0|\n","|  0.0|[4.6,3.6,1.0,0.2,...|[2.22770198015119...|[0.87369147494938...|       0.0|\n","|  0.0|[4.8,3.4,1.9,0.2,...|[1.70597774606196...|[0.78789844507779...|       0.0|\n","|  0.0|[4.9,3.1,1.5,0.2,...|[1.73475831746008...|[0.77402503653184...|       0.0|\n","+-----+--------------------+--------------------+--------------------+----------+\n","only showing top 5 rows\n","\n","('get prediction:', None)\n","20/07/20 02:02:22 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@2a07dfeb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","Job [e7d9c005583e4a8195b9b528bad2ec1f] finished successfully.\n","done: true\n","driverControlFilesUri: gs://dataproc-staging-us-central1-544826698357-zpklxhfw/google-cloud-dataproc-metainfo/c677af79-9d1a-43ba-b150-15cae758bca5/jobs/e7d9c005583e4a8195b9b528bad2ec1f/\n","driverOutputResourceUri: gs://dataproc-staging-us-central1-544826698357-zpklxhfw/google-cloud-dataproc-metainfo/c677af79-9d1a-43ba-b150-15cae758bca5/jobs/e7d9c005583e4a8195b9b528bad2ec1f/driveroutput\n","jobUuid: 75514890-fa2d-383d-ad06-6754c12dede2\n","placement:\n","  clusterName: proc-cluster-lu\n","  clusterUuid: c677af79-9d1a-43ba-b150-15cae758bca5\n","pysparkJob:\n","  jarFileUris:\n","  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n","  mainPythonFileUri: gs://full_step_bucket_lugq/spark_train_bigquery.py\n","reference:\n","  jobId: e7d9c005583e4a8195b9b528bad2ec1f\n","  projectId: cloudtutorial-283609\n","status:\n","  state: DONE\n","  stateStartTime: '2020-07-20T02:02:27.028Z'\n","statusHistory:\n","- state: PENDING\n","  stateStartTime: '2020-07-20T02:01:02.658Z'\n","- state: SETUP_DONE\n","  stateStartTime: '2020-07-20T02:01:02.710Z'\n","- details: Agent reported job success\n","  state: RUNNING\n","  stateStartTime: '2020-07-20T02:01:02.931Z'\n","yarnApplications:\n","- name: spark_train_bigquery.py\n","  progress: 1.0\n","  state: FINISHED\n","  trackingUrl: http://proc-cluster-lu-m:8088/proxy/application_1595209762313_0002/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bc2rVc7KM7ej","colab_type":"text"},"source":["### Composer with Dataproc and bigquery"]},{"cell_type":"code","metadata":{"id":"5oSFjtagOG8P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"status":"ok","timestamp":1595210460078,"user_tz":-480,"elapsed":25699,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"3cf0f472-0b5e-4a10-a9d2-d1ced3cd8afc"},"source":["# first we need to create composer environments\n","# one thing to notice if we create a composer, we will create a Kubernetes cluster!\n","# By default this will create a K8S cluster with 3 nodes: 1 master and 2 slaves\n","! gcloud composer environments create dataproc --location us-central1 --async "],"execution_count":41,"outputs":[{"output_type":"stream","text":["API [composer.googleapis.com] not enabled on project [544826698357]. \n","Would you like to enable and retry (this will take a few minutes)? \n","(y/N)?  y\n","\n","Enabling service [composer.googleapis.com] on project [544826698357]...\n","Operation \"operations/acf.bd914bc3-ec92-4b47-ae75-15b721c924d0\" finished successfully.\n","Create in progress for environment [projects/cloudtutorial-283609/locations/us-central1/environments/dataproc] with operation [projects/cloudtutorial-283609/locations/us-central1/operations/6abd9347-d18e-4ca9-a69c-f4f7d9143ce8].\n","metadata:\n","  '@type': type.googleapis.com/google.cloud.orchestration.airflow.service.v1.OperationMetadata\n","  createTime: '2020-07-20T02:00:57.605Z'\n","  operationType: CREATE\n","  resource: projects/cloudtutorial-283609/locations/us-central1/environments/dataproc\n","  resourceUuid: 58cf2723-e86a-43be-a87b-a644199075af\n","  state: PENDING\n","name: projects/cloudtutorial-283609/locations/us-central1/operations/6abd9347-d18e-4ca9-a69c-f4f7d9143ce8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I8g_Ol28pPgL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595210554391,"user_tz":-480,"elapsed":88962,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"94be7472-e65c-419c-a146-a3b725ec2297"},"source":["# for sdk not found\n","!gcloud config set container/use_application_default_credentials true"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Updated property [container/use_application_default_credentials].\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"STaGOsnNqF-E","colab_type":"text"},"source":["#### install Cloud SDK in colab"]},{"cell_type":"code","metadata":{"id":"xKf_pfbdpqtb","colab_type":"code","colab":{}},"source":["# before we install kubectl, we have to install Cloud SDK\n","# then let's install the kubectl to interact with k8s\n","! echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n","! sudo apt-get install apt-transport-https ca-certificates gnupg\n","\n","! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n","\n","# install gcloud-sdk\n","! sudo apt-get update && sudo apt-get install google-cloud-sdk\n","\n","# install kubectl\n","! sudo apt-get install kubectl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FfD774NO9_R1","colab_type":"text"},"source":["##### Set parameters for Composer"]},{"cell_type":"code","metadata":{"id":"RJweuLZtHMH4","colab_type":"code","colab":{}},"source":["# first we need to set up the variables that we would need in composer\n","\n","# if sometime we need to set some avariables could be use for composer \n","# we could just set the parameters as bellow\n","\n","\n","# ! gcloud composer environments run dataproc --location us-central1 \\\n","# variables -- \\\n","# --set project_id  cloudtutorial-282102"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oq-oSm6tpJFn","colab_type":"code","colab":{}},"source":["# set gce region into airflow\n","# ! gcloud composer environments run dataproc --location us-central1 \\\n","# variables -- --set gce_region us-central1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZ50OUyqrcdG","colab_type":"code","colab":{}},"source":["# set gce zone\n","# ! gcloud composer  environments run dataproc --location us-central1 \\\n","# variables -- --set gce_zone us-central1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7o-mu9Y-rpzE","colab_type":"code","colab":{}},"source":["# set bucket\n","# ! gcloud composer environments run dataproc --location us-central1 \\\n","# variables -- --set bucket_path gs://full_step_bucket"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pl1bx9sj-ED3","colab_type":"text"},"source":["##### Creat DAG"]},{"cell_type":"code","metadata":{"id":"o1F8OyotsOfC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595211847399,"user_tz":-480,"elapsed":1804,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"d632b307-a330-4530-f9fa-fa5a6eb333bb"},"source":["# last step is to create airflow python DAG\n","%%writefile dataproc_dag.py\n","\"\"\"\n","To run the code we need 3 variables:\n","* gcp_project - Google Cloud Project to use for the Cloud Dataproc cluster.\n","* gce_zone - Google Compute Engine zone where Cloud Dataproc cluster should be\n","  created.\n","* gcs_bucket - Google Cloud Storage bucket to use for result of Hadoop job.\n","  See https://cloud.google.com/storage/docs/creating-buckets for creating a\n","  bucket.\n","\"\"\"\n","import datetime\n","\n","from airflow import models\n","from airflow.contrib.operators import  dataproc_operator\n","from airflow.utils import trigger_rule\n","\n","### This is to get the paramaters configured in running processing \n","# bucket_path = models.Variable.get(\"bucket_path\")\n","# project_id = models.Variable.get('project_id')\n","# gce_zone = models.Variable.get(\"gce_zone\")\n","# gce_region = models.Variable.get('gce_region')\n","\n","\n","# default args, we have to set the now before current timestamp\n","now = datetime.datetime.now() - datetime.timedelta(days=1)\n","\n","default_args = {\n","    'owner': 'lugq',\n","    'depends_on_past': False,\n","    'email': [''],\n","    'email_on_failure': False,\n","    'email_on_retry': False,\n","    'retries': 1,\n","    'retry_delay': datetime.timedelta(minutes=5),\n","    'start_date': now,\n","    'project_id': \"cloudtutorial-283609\",    # one thing to notice here: when I change a new project, do keep in mind to change this, otherwise will get error: permission.\n","    'region': 'us-central1',\n","    'zone': 'us-central1',\n","   #'cluster_name': \" proc-cluster-lu\",  # according to: https://stackoverflow.com/questions/50134110/dataprocpysparkoperator-cluster-region-and-zone-issue\n","}\n","\n","# spark training file url\n","training_file = \"gs://full_step_bucket_lugq/process_data_with_spark.py\"\n","\n","# create DAG operations, Maybe we should use composer to create the dataproc cluster\n","# run the job and delete the cluster. Let's try to do this here.\n","with models.DAG(\"dataproc_dag\", default_args=default_args,\n","                schedule_interval=datetime.timedelta(days=1)) as dag:\n","  # HERE will create a DAG : create cluster -> submit job -> delete cluster\n","  \n","  # 1. create cluster, as current with IP usecase problem, we have to delete original cluster first.\n","  create_cluster = dataproc_operator.DataprocClusterCreateOperator(\n","      task_id = \"create_cluster\",\n","      cluster_name = \"composer-cluster\",\n","      num_workers=0,\n","      zone = 'us-central1-a',\n","      master_machine_type='n1-standard-1',\n","  )\n","  \n","  # 2. submit jobs\n","  # One thing to notice here: if we want to submit our job with Dataproc, we have to provide\n","  # with cluster name and region etc. as by default will use 'cluster-1', I have try many times.\n","  data_job = dataproc_operator.DataProcPySparkOperator(task_id=\"training_with_airflow\", \n","                                                       main=training_file,\n","                                                       cluster_name=\"composer-cluster\")\n","  \n","  # 3. delete cluster\n","  delete_cluster = dataproc_operator.DataprocClusterDeleteOperator(\n","      task_id=\"delete_cluster\",\n","      cluster_name = \"composer-cluster\",\n","      # this means will delete cluster no matter done or fail.\n","      trigger_rule = trigger_rule.TriggerRule.ALL_DONE\n","  )\n","\n","  # this is DAG\n","  create_cluster >> data_job >> delete_cluster\n","\n","  print(\"airflow with dataproc finished!\")"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Overwriting dataproc_dag.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZxbBqcv9Ahdr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":587},"executionInfo":{"status":"ok","timestamp":1595211626669,"user_tz":-480,"elapsed":5321,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"1062fb7a-3f32-4982-e19c-b0dbb19032b0"},"source":["# let's get the DAGs bucket path\n","! gcloud composer environments describe  --location us-central1 --format=json dataproc"],"execution_count":55,"outputs":[{"output_type":"stream","text":["{\n","  \"config\": {\n","    \"airflowUri\": \"https://qd0d300c3c3857e79p-tp.appspot.com\",\n","    \"dagGcsPrefix\": \"gs://us-central1-dataproc-6abd9347-bucket/dags\",\n","    \"gkeCluster\": \"projects/cloudtutorial-283609/zones/us-central1-a/clusters/us-central1-dataproc-6abd9347-gke\",\n","    \"nodeConfig\": {\n","      \"diskSizeGb\": 100,\n","      \"ipAllocationPolicy\": {},\n","      \"location\": \"projects/cloudtutorial-283609/zones/us-central1-a\",\n","      \"machineType\": \"projects/cloudtutorial-283609/zones/us-central1-a/machineTypes/n1-standard-1\",\n","      \"network\": \"projects/cloudtutorial-283609/global/networks/default\",\n","      \"oauthScopes\": [\n","        \"https://www.googleapis.com/auth/cloud-platform\"\n","      ],\n","      \"serviceAccount\": \"544826698357-compute@developer.gserviceaccount.com\"\n","    },\n","    \"nodeCount\": 3,\n","    \"privateEnvironmentConfig\": {\n","      \"cloudSqlIpv4CidrBlock\": \"10.0.0.0/12\",\n","      \"privateClusterConfig\": {},\n","      \"webServerIpv4CidrBlock\": \"172.31.245.0/24\"\n","    },\n","    \"softwareConfig\": {\n","      \"imageVersion\": \"composer-1.10.6-airflow-1.10.3\",\n","      \"pythonVersion\": \"3\"\n","    }\n","  },\n","  \"createTime\": \"2020-07-20T02:00:57.605Z\",\n","  \"name\": \"projects/cloudtutorial-283609/locations/us-central1/environments/dataproc\",\n","  \"state\": \"RUNNING\",\n","  \"updateTime\": \"2020-07-20T02:15:59.649Z\",\n","  \"uuid\": \"58cf2723-e86a-43be-a87b-a644199075af\"\n","}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vxiq8Y0TvgQR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595211855002,"user_tz":-480,"elapsed":5286,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"outputId":"aff419c7-9743-4f33-e7cc-d37d371efa54"},"source":["# upload the training file into bucket\n","# this is from Composer website: DAG folder: gs://us-central1-dataproc-d189ca6a-bucket/dags\n","\n","# Everytime to rerun use another project we have to change it.\n","! gsutil cp dataproc_dag.py gs://us-central1-dataproc-6abd9347-bucket/dags/"],"execution_count":58,"outputs":[{"output_type":"stream","text":["Copying file://dataproc_dag.py [Content-Type=text/x-python]...\n","/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n","Operation completed over 1 objects/3.0 KiB.                                      \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6T-5D-mmLH0J","colab_type":"text"},"source":["#### Conclusion\n","\n","Good news we do could use **Composer** to create, submit and delete our cluster with job success, you could find the result that we do finished the whole thing sucessfully:\n","![Composer with Dataproc result](https://docs.google.com/uc?export=download&id=1u998GAmTc6ohrC_RHMYjMCFDNnc82Hhc)"]},{"cell_type":"markdown","metadata":{"id":"xvPoq4zjEu97","colab_type":"text"},"source":["After the whole step finished, we could just use Terraform command: destropy to cancel whole resources are used for this project. "]},{"cell_type":"code","metadata":{"id":"o5IQkljdE8pv","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}