{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataProc with bigquery.ipynb","provenance":[],"authorship_tag":"ABX9TyPNWzhb/VxJCrAWgPfPjhKh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CEGf8dYO_PNO","colab_type":"text"},"source":["## Introduction\n","\n","According official website: Dataproc is a managed Apache Spark and Apache Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data [official](https://cloud.google.com/dataproc/docs).\n","\n","When we need to use Spark to do data processing, then we could use DataProc as a tool to process data."]},{"cell_type":"code","metadata":{"id":"leM-0VVX_who","colab_type":"code","colab":{}},"source":["# first auth the lab\n","from google.colab import auth\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_aB2J7QgFJU6","colab_type":"code","colab":{}},"source":["# install dataproc\n","! pip install google-cloud-dataproc --quiet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gep4mAlE5To","colab_type":"code","colab":{}},"source":["import argparse\n","import time\n","\n","from google.cloud import dataproc_v1 as dataproc\n","from google.cloud import storage\n","\n","# define the parameters\n","project_id = \"cloudtutorial-279003\"\n","cluster_name = 'python-cluster'\n","region = 'us-central1'\n","\n","\n","def create_cluster():\n","  # first create cluster client\n","  cluster_client = dataproc.ClusterControllerClient(client_options={\n","        'api_endpoint': '{}-dataproc.googleapis.com:443'.format(region)\n","    })\n","\n","\n","  cluster = {\n","          'project_id': project_id,\n","          'cluster_name': cluster_name,\n","          'config': {\n","              'master_config': {\n","                  'num_instances': 1,\n","                  'machine_type_uri': 'n1-standard-1'\n","              },\n","              'worker_config': {\n","                  'num_instances': 2,\n","                  'machine_type_uri': 'n1-standard-1'\n","              }\n","          }\n","      }\n","\n","\n","  # then create cluster\n","  cluster_client = cluster_client.create_cluster(project_id, region, cluster)\n","  result = cluster_client.result()\n","\n","  print(\"Cluster has been created: {}\".format(result.cluster_name))\n","\n","  return cluster_client\n","\n","\n","def submit_job(job_path):\n","  # create job client, so that we could submit our job into cluster\n","  job_client = dataproc.JobControllerClient(client_options={\n","      'api_endpoint': \"{}-dataproc.googleapi.com:443\".format(region)\n","  })\n","\n","  # create job config\n","  job = {\n","      'placement': {\n","          'cluster_name': cluster_name\n","      },\n","      'pyspark_job':{\n","          'main_python_file_uri': job_path\n","      }\n","  }\n","\n","  job_response = job_client.submit_job(project_id, region, job)\n","  job_id = job_response.reference.job_id\n","\n","  print('Submit job {}'.format(job_id))\n","\n","  # get terminate states of jobs\n","  terminal_states = {\n","      dataproc.types.JobStatus.ERROR,\n","      dataproc.types.JobStatus.CANCELLED,\n","      dataproc.types.JobStatus.DONE\n","  }\n","\n","  # we could also config a timeline that if a job fun too long, we could terminate the job\n","  timeout_seconds = 600\n","  time_start = time.time()\n","\n","  # we have to wait the job to complete\n","  while job_response.status.state not in terminal_states:\n","    if time.time() > time_start + timeout_seconds:\n","      job_client.cancel_job(project_id, region, job_id)\n","      print(\"Cancel job after {} seconds\".format(job_id, timeout_seconds))\n","\n","    # we could check every 1 second.\n","    time.sleep(1)\n","    job_response = job_client.get_job(project_id, region, job_id)\n","\n","    return job_response\n","\n","\n","def save_result_into_bucket(cluster_client, job_response):\n","  # this is to save the final result into bucket so that we could get some information about jobs.\n","  storage_client = storage.Client()\n","\n","  cluser_info = cluster_client.get_cluster(project_id, region, cluster_name)\n","\n","  bucket = storage_client.get_bucket(cluster_info.config.config_bucket)\n","\n","  output_blob = 'google-cloud-dataproc-metainfo/{}/jobs/{}/driveroutput.000000000'.format(cluster_info.cluster_uuid, job_id)\n","  output = bucket.blob(output_blob).download_as_string()\n","\n","  print('job {} finished with status: {}:\\n{}'.format(job_id, job_response.status.state, output))\n","\n","\n","def delete_cluster(cluster_client):\n","  # after whole step finished, we could delete our cluster\n","  delete_cluster = cluster_client.detele_cluster(project_id, region, cluster_name)\n","  delete_cluster.result()\n","\n","  print('cluster has been deleted')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MzAzsq-Sob8J","colab_type":"code","outputId":"feec3429-f7fd-4761-8d1b-585ea5d809c7","executionInfo":{"status":"ok","timestamp":1591161797047,"user_tz":-480,"elapsed":5500,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["! gcloud config set project $project_id"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IMu-gZwriZO6","colab_type":"code","outputId":"dbf92f88-2255-414d-ddd1-3a772505c56c","executionInfo":{"status":"ok","timestamp":1591162956185,"user_tz":-480,"elapsed":190307,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# first let's get the cluster obj\n","cluster = create_cluster()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cluster has been created: python-cluster\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_c0AwrKTGQOJ","colab_type":"text"},"source":["## Noted\n","\n","As with the `sandbox` problem, I haven't make the job run successfully, as the reason is for DataProc, it will start a cluster in compute engine, this is really expensive for `sandbox`, so the logic should do be similiar, we could sand our jars into the server and start the job in the remote server, but do keep in mind, we have to delete the cluster in the dataproc, so that we won't cost so much. In fact, this is just a solution that we could start our Spark job in cloud but with cluster created manually by us."]},{"cell_type":"code","metadata":{"id":"F7CvQXwMtBck","colab_type":"code","outputId":"41da63dd-e8ea-46c0-ffc9-8d45275bc400","executionInfo":{"status":"error","timestamp":1591163847175,"user_tz":-480,"elapsed":675737,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":341}},"source":["# then we could submit our job to the cluster\n","job_path = 'gs://{}'.format(storage_path)\n","job_response = submit_job(job_path)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-686668db7a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# then we could submit our job to the cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjob_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gs://{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjob_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmit_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-61-b7c7a3938afa>\u001b[0m in \u001b[0;36msubmit_job\u001b[0;34m(job_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m   }\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mjob_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/dataproc_v1/gapic/job_controller_client.py\u001b[0m in \u001b[0;36msubmit_job\u001b[0;34m(self, project_id, region, job, request_id, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[1;32m    276\u001b[0m         return self._inner_api_calls[\"submit_job\"](\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         )\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             )\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;34m\"Retrying due to {}, sleeping {:.1f}s ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep generator stopped yielding sleep values.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"yc-Z44WRhzTF","colab_type":"text"},"source":["## Training file\n","\n","Now we could start our training or processing logic with PySpark, here I just create a sample file to use Spark to do feature extraction. After we  finish the logic, then we should upload the training file into storage."]},{"cell_type":"code","metadata":{"id":"ao2_m3EzKVqn","colab_type":"code","outputId":"9839bf13-3d6e-46be-fb7b-bc691d59ceae","executionInfo":{"status":"ok","timestamp":1591160158171,"user_tz":-480,"elapsed":609,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# here I write the job logic\n","%%writefile training_spark.py\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import IntegerType\n","\n","import logging\n","\n","logger = logging.getLogger(__name__)\n","\n","logger.info('init spark')\n","spark = SparkSession.builder.getOrCreate()\n","\n","sentenceDataFrame = spark.createDataFrame([\n","    (0, \"Hi I heard about Spark\"),\n","    (1, \"I wish Java could use case classes\"),\n","    (2, \"Logistic,regression,models,are,neat\")\n","], [\"id\", \"sentence\"])\n","\n","# split sentence into words.\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","\n","countTokens = udf(lambda words: len(words), IntegerType())\n","\n","# with udf function to get each sentence length.\n","tokenized = tokenizer.transform(sentenceDataFrame)\n","tokenized.select(\"sentence\", \"words\")\\\n","    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n","\n","logger.info('whole Spark logic finished.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting training_spark.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FHCKVVhXNVsk","colab_type":"code","outputId":"d0220240-ea9f-4e64-cc6f-8c8b86188a7f","executionInfo":{"status":"ok","timestamp":1591160170132,"user_tz":-480,"elapsed":4679,"user":{"displayName":"guangqiang lu","photoUrl":"","userId":"05333431788560706430"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# upload the file into bucket\n","bucket_name = 'dataflow_tutorial_bucket'\n","folder_name = 'spark_code'\n","file_name = 'training_spark.py'\n","\n","storage_path = os.path.join(bucket_name, folder_name, file_name)\n","\n","! gsutil cp ./training_spark.py gs://$storage_path"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Copying file://./training_spark.py [Content-Type=text/x-python]...\n","/ [1 files][  909.0 B/  909.0 B]                                                \n","Operation completed over 1 objects/909.0 B.                                      \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"69xyS_WXNwzV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}